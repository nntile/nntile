{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 (Text-to-Text Transfer Transformer): A Mathematical Overview\n",
    "\n",
    "**Introduction**:\n",
    "The T5 model, which stands for \"Text-to-Text Transfer Transformer,\" represents a unified framework for tackling a wide variety of Natural Language Processing (NLP) tasks. Developed by Google AI, T5 reframes every NLP problem into a text-to-text format, where the model takes text as input and produces text as output. This approach leverages the power of the Transformer architecture and large-scale pre-training on a diverse dataset.\n",
    "\n",
    "**1. Architectural Framework**:\n",
    "T5 is built upon the standard Transformer Decoder-Encoder architecture. Key components include:\n",
    "\n",
    "- **Encoder-Decoder Stack**: Unlike decoder-only models like GPT-2, T5 employs both an encoder (to process the input sequence) and a decoder (to generate the output sequence). Each consists of multiple layers.\n",
    "- **Multi-Head Self-Attention with relational embeddings**: Both the encoder and decoder use multi-head self-attention mechanisms to weigh the importance of different parts of the sequence. The mathematical formulation is similar to the standard Transformer:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "where each head is computed as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)\n",
    "$$\n",
    "\n",
    "and the attention function is:\n",
    "# \n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( QK^T + R \\right)V\n",
    "$$\n",
    "# \n",
    "Here, \\( X \\) represents the input embeddings, \\( $W_i^Q, W_i^K, W_i^V$ \\) are learnable projection matrices.\n",
    "\n",
    "The decoder has two attention mechanisms:\n",
    "1. **Masked Self-Attention**: Similar to the encoder's self-attention but with a causal mask to prevent attending to future tokens.\n",
    "2. **Cross-Attention**: This mechanism allows the decoder to attend to the encoder's output:\n",
    "\n",
    "$$\n",
    "\\text{CrossAttention}(X, EO) = \\text{softmax}\\left( XW^Q(E W^K)^T + R \\right) E W^V\n",
    "$$\n",
    "\n",
    "where \\( **X** \\) is the decoder's representation and \\( **E** \\) is the encoder's output. This cross-attention enables the decoder to focus on relevant parts of the input sequence when generating each output token.\n",
    "\n",
    "The relational embeddings $R$ in the attention mechanism are computed as:\n",
    "\n",
    "$$\n",
    "R_{i,j} = emb_r(f(i-j))\n",
    "$$\n",
    "\n",
    "where $emb_r$ is a learned embedding layer that maps the relative distance between positions $i$ and $j$ to a vector of head dimension.   \n",
    "$f$ is a bucketing function that maps relative distances to a fixed number of buckets, limiting embedding layer size (e.g., clamping large distances to a maximum value). $f$ is shared across all layers in decoder and encoder stacks.\n",
    "\n",
    "\n",
    "\n",
    "**2. The Text-to-Text Framework**:\n",
    "Alfought T5 represents classic encoder-decoder model with relation embeddings addition, the core innovation of T5 is its unified approach. Every task is converted into a text-to-text problem by adding a task-specific prefix to the input sequence. The model is trained to generate the target text based on this combined input.\n",
    "\n",
    "Examples:\n",
    "- **Translation (English to German)**: `translate English to German: That is good.` -> `Das ist gut.`\n",
    "- **Summarization**: `summarize: [article text...]` -> `[summary text...]`\n",
    "- **Question Answering**: `question: Who invented the lightbulb? context: Thomas Edison invented the lightbulb in 1879.` -> `Thomas Edison`\n",
    "- **Sentiment Analysis**: `sst2 sentence: This movie was fantastic!` -> `positive`\n",
    "\n",
    "**3. Pre-training Objective**:\n",
    "T5 is pre-trained on a massive and diverse text corpus called C4 (Colossal Clean Crawled Corpus) using a self-supervised denoising objective inspired by Masked Language Modeling (MLM). Specifically, T5 uses **span corruption**:\n",
    "\n",
    "- Randomly sample spans (contiguous sequences of tokens) from the input text.\n",
    "- Replace each chosen span with a single unique sentinel token (e.g., `<X>`, `<Y>`, etc.).\n",
    "- The model is trained to predict the original text of the corrupted spans, using the corresponding sentinel tokens as delimiters in the target sequence.\n",
    "\n",
    "Example:\n",
    "- Original: `Thank you for inviting me to your party last week.`\n",
    "- Input: `Thank you <X> me to your party <Y> week.`\n",
    "- Target: `<X> for inviting <Y> last <EOS>`\n",
    "\n",
    "This pre-training task encourages the model to learn general language understanding and generation capabilities.\n",
    "\n",
    "**4. Fine-tuning**:\n",
    "After pre-training, the *same* T5 model is fine-tuned on various downstream tasks. The fine-tuning process also uses the text-to-text format, simply by providing task-specific examples with the appropriate prefixes (like `translate English to German:`, `summarize:`, etc.). The model learns to associate the prefix with the desired task and output format. The loss function during both pre-training and fine-tuning is typically the standard cross-entropy loss computed over the target sequence tokens:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{n} \\log P(y_t | y_1, \\ldots, y_{t-1}, \\text{input})\n",
    "$$\n",
    "\n",
    "where \\( P(y_t | \\ldots) \\) is the probability of the target token \\( y_t \\) given the input and previously generated target tokens.\n",
    "\n",
    "**In Summary**:\n",
    "T5 provides a powerful and flexible text-to-text framework that simplifies the approach to diverse NLP tasks. By leveraging the Transformer architecture, a large-scale denoising pre-training objective (span corruption), and a unified input/output format, T5 achieves state-of-the-art performance on many benchmarks with a single model architecture. Its versatility makes it a foundational model in modern NLP research and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment variable setting block:\n",
    "\n",
    "The following block is required to set environment variables that are read during the execution of the program code. \n",
    "\n",
    "User can change these environment variables between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of experimental environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"1\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"0\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"T5_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation Block: \n",
    "\n",
    "This block uses the interpreted file \"causal_lm_data_preparation.py\". This Python script supports the following arguments when run:\n",
    "- hf-dataset, (default=`\"roneneldan/TinyStories\"`): The name of the dataset to be processed and prepared for use in the training process. By default, the \"TinyStories\" dataset from the Huggingface infrastructure is specified,\n",
    "- dataset-path, (default=`\".data\"`): path to the directory where previously downloaded datasets from remote sources are saved, making it easy to access for the future use,\n",
    "- dataset-select, (`int`, default=`100`): specifies the number of records from the original dataset that fall into the training set,\n",
    "- hf-tokenizer, (`str`, default=`\"kimihailv/llama-1.3b\"`): specifies the repository from the Huggingface infrastructure used as a tokenizer,\n",
    "- tokenizer-path, (`str`, default=`\".model\"`): path to the directory where previously downloaded tokenizers are saved,\n",
    "- seq-len, (`int`, default=`1024`): length of the input token sequence for the training process,\n",
    "- batch-size, (`int`, default=`1`): batch size for the training process, then is the number of input data sentences between which the loss function optimizer step is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../wrappers/python/examples/causal_lm_data_preparation.py \\\n",
    "--hf-tokenizer=\"google/flan-t5-small\" --seq-len=512 \\\n",
    "--batch-size=1 --dataset-select=16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show an example of utilizing the T5 model, implemented using the NNTile framework. We explore the following scenarios:\n",
    "\n",
    "- **Training the model from a random initial state and saving it to a checkpoint.**\n",
    "- **Loading the model weights from a checkpoint and continuing training with a different data type.**\n",
    "- **Training the remote model downloaded from the Hugging Face infrastructure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training and continuing retraining scenarios, the interpreted file \"t5_lmhead_training.py\" is used. This Python script supports the following arguments when running:\n",
    "\n",
    "- remote_model_name, (str, default=\"google/flan-t5-small\"): Specifies the name of the T5-based model on HuggingFace hub used to initialize the configuration and initial state of the NNTile model.  \n",
    "\n",
    "- pretrained, (choices=[\"local\", \"remote\"], default=\"remote\"): Indicates the source of the pretrained model. \"local\" requires config-path (and optionally checkpoint-path), while \"remote\" downloads from the hub specified by remote_model_name.\n",
    "\n",
    "- checkpoint-path, (str, default=\"\"): Path to a saved checkpoint file to resume training or initialize a local model.\n",
    "\n",
    "- config-path, (str, default=\"\"): Path to the configuration JSON file, required if pretrained is \"local\" and no checkpoint-path is provided.\n",
    "\n",
    "- save-checkpoint-path, (str, default=\".model\"): Path where the trained model checkpoint will be saved.\n",
    "\n",
    "- optimizer, (choices=[\"sgd\", \"adam\", \"adamw\"], default=\"adam\"): Specifies the optimization algorithm to use during training.\n",
    "\n",
    "- model-path, (str, default=\".model\"): Directory path used to cache models downloaded from HuggingFace hub.\n",
    "\n",
    "- seq-len, (int, default=512): Length of the input token sequences for training.\n",
    "seq-len-tile, (int, default=-1): Tile size for the sequence length dimension. If -1, defaults to seq-len.\n",
    "\n",
    "- batch-size, (int, default=1): Number of sequences processed between optimizer steps.\n",
    "\n",
    "- minibatch-size, (int, default=-1): The size of smaller batches the full batch-size is divided into for gradient accumulation. If -1, defaults to batch-size.\n",
    "\n",
    "- minibatch-size-tile, (int, default=-1): The tile size for the minibatch dimension that is processed by individual hardware units (CPU/GPU). If -1, defaults to minibatch-size.\n",
    "\n",
    "- d-model-tile, (int, default=-1): Tile size for the model's hidden dimension (d_model). If -1, it's inferred from the loaded model configuration.\n",
    "\n",
    "- d-ff-tile, (int, default=-1): Tile size for the feed-forward intermediate dimension (d_ff). If -1, it's inferred from the loaded model configuration.\n",
    "\n",
    "- num-heads-tile, (int, default=-1): Tile size for the number of attention heads dimension. If -1, it's inferred from the loaded model configuration.\n",
    "\n",
    "- num-labels, (int, default=2): Number of output classes for the sequence classification task.\n",
    "\n",
    "- dtype, (choices=[\"fp32\", \"fp64\", \"tf32\", \"bf16\", \"fp32_fast_fp16\", \"fp32_fast_bf16\"], default=\"fp32\"): Data type used for model computations and storage.\n",
    "\n",
    "- restrict, (choices=[\"cpu\", \"cuda\", None], default=None): Restricts computations to specific hardware: \"cpu\" for CPU only, \"cuda\" for GPU only, or None to use all available resources.\n",
    "\n",
    "- use-redux, (action=\"store_true\"): Enables the use of reduction operations for potentially faster computation on certain hardware configurations.\n",
    "\n",
    "- dataset-path, (str, default=\".data\"): Directory path where the dataset file is located.\n",
    "\n",
    "- dataset-file, (str, default=\"\"): Path to the dataset file (relative to dataset-path), expected in .npz format with 'input_ids' and 'labels' keys. If empty, dummy data is generated.\n",
    "\n",
    "- lr, (float, default=1e-4): Learning rate for the optimizer.\n",
    "nepochs, (int, default=1): Number of times to iterate over the entire training dataset.\n",
    "\n",
    "- logger, (action=\"store_true\"): Enables NNTile's internal logger for debugging and performance monitoring.\n",
    "\n",
    "- logger-server-addr, (str, default=\"localhost\"): Network address of the NNTile logger server.\n",
    "\n",
    "- logger-server-port, (int, default=5001): Network port of the NNTile logger server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training from the random initial state and saving into checkpoint.\n",
    "\n",
    "This requires option `pretrained` set to `local` and `config-path` to point on previously created `.json` configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../wrappers/python/examples/t5_lmhead_training.py \\\n",
    "--restrict=\"cuda\" --pretrained=local \\\n",
    "--config-path=\"../wrappers/python/examples/t5_config.json\" \\\n",
    "--save-checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "--optimizer=\"adam\" --lr=1e-4 --dtype=bf16 --nepochs=1 \\\n",
    "--dataset-file=\"tinystories/train.bin\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the weights of the model from the control point and continuing training with a different data type\n",
    "\n",
    "To do this, you need to set the `pretrained` parameter to `local` again. The `config-path` parameter must point to a previously created configuration file in the format. The config, as well as the `checkpoint-path`, must point to an existing checkpoint file in the PyTorch format. After that, the training can be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../wrappers/python/examples/t5_lmhead_training.py \\\n",
    "--restrict=\"cuda\" --pretrained=local \\\n",
    "--checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "--config-path=\"../wrappers/python/examples/t5_config.json\" \\\n",
    "--save-checkpoint-path=\".model/nntile_checkpoint_v1.pt\" \\\n",
    "--optimizer=\"adam\" --lr=1e-4 --dtype=bf16 --nepochs=1 \\\n",
    "--dataset-file=\"tinystories/train.bin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Continuing to train the model loaded from the Hugging Face library\n",
    "\n",
    "The NNTile framework currently supports continued training of a model loaded from a remote source, \n",
    "in our example, from the Hugging Face infrastructure library. \n",
    "\n",
    "\n",
    "The weights of the loaded model are passed to the model implemented in NNTile.To run such a scenario, set the `pretrained` parameter to the `remote` value.\n",
    "The `config-path` and `checkpoint-path` parameters are no longer required, \n",
    "as the model configuration and layer weights will be obtained from the loaded model. \n",
    "After that, the training can be continued.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../wrappers/python/examples/t5_lmhead_training.py \\\n",
    "--restrict=\"cuda\" --pretrained=remote \\\n",
    "--save-checkpoint-path=\".model/nntile_checkpoint_v2.pt\" \\\n",
    "--optimizer=\"adam\" --lr=1e-4 --dtype=bf16 --nepochs=1 \\\n",
    "--dataset-file=\"tinystories/train.bin\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
