{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817cdfc7-ccec-4671-9013-24b42563e12a",
   "metadata": {},
   "source": [
    "# How to use the Bert model inside the NNTile framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a729f1-df4c-4b02-aba1-d2d6aaf47846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of experimental environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"2\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"Bert_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6e3f2-e834-40c4-8678-f09b5ddcc8d3",
   "metadata": {},
   "source": [
    "## Prepare dataset for the Masked Language Model with the Bert model \n",
    "\n",
    "- ```hf-dataset``` (str, default=\"roneneldan/TinyStories\"): the name of the dataset aligned with name in ```datasets``` library used to download it\n",
    "- ```dataset-path``` (str, default=\".data\"): path to the directory where previously prepared data sets are saved.\n",
    "- ```dataset-select``` (int, default=100): number of the first pieces of texts from the dataset used for training model\n",
    "- ```hf-tokenizer``` (str, default=\"bert-base-uncased\"): tokenizer used to train masked language model\n",
    "- ```tokenizer-path``` (str, default=\".model\"): path to the folder where the tokenizer data is stored\n",
    "- ```seq-len``` (int, deault=1024): length of the input token sequence for training\n",
    "- ```batch-size``` (int, default=1): batch size for the training process, which specifies the number of sentences processed by ```seq-len``` tokens between steps of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676ca1dd-8686-4ea9-a7b7-a93708412cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md: 100%|███████████████████████████| 1.06k/1.06k [00:00<00:00, 4.16MB/s]\n",
      "(…)-00000-of-00004-2d5a1467fff1081b.parquet: 100%|█| 249M/249M [00:05<00:00, 46.\n",
      "(…)-00001-of-00004-5852b56a2bd28fd9.parquet: 100%|█| 248M/248M [00:05<00:00, 48.\n",
      "(…)-00002-of-00004-a26307300439e943.parquet: 100%|█| 246M/246M [00:05<00:00, 49.\n",
      "(…)-00003-of-00004-d243063613e5a057.parquet: 100%|█| 248M/248M [00:04<00:00, 50.\n",
      "(…)-00000-of-00001-869c898b519ad725.parquet: 100%|█| 9.99M/9.99M [00:00<00:00, 2\n",
      "Generating train split: 100%|█| 2119719/2119719 [00:15<00:00, 135857.16 examples\n",
      "Generating validation split: 100%|█| 21990/21990 [00:00<00:00, 128413.15 example\n",
      "tokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 148kB/s]\n",
      "config.json: 100%|█████████████████████████████| 570/570 [00:00<00:00, 1.82MB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 2.19MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 1.56MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "# Describe parameters and arguments\n",
    "!python ../wrappers/python/examples/mlm_data_preparation.py --seq-len=512 --batch-size=32 --dataset-select=500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d0ae4-42d8-4764-8052-0a6006aed1a2",
   "metadata": {},
   "source": [
    "## Arguments of the ```bert_training.py``` script, which is used to run all the scenarios below\n",
    "\n",
    "- ```remote-model-name```, (str, default=\"bert-base-uncased\"): the name of the Bert architecture-based model that resides in the HuggingFace framework and will be used to initialize the configuration and initial state of the NNTile model.\n",
    "- ```pretrained```, (choices=[\"local\", \"remote\"], default=\"local\"): the source type of the pre-trained model. The remote option loads the model ```remote-model-name``` from the Huggingface infrastructure. The ```local``` option requires a configuration file path (```config-path```) to start training from a randomly initialized state, or to continue training if a checkpoint file path (```checkpoint-path```) is also provided.\n",
    "- ```checkpoint-path```, (str, default=\"\"): path to the saved state of the pre-trained model weights. If the file is available, training will continue from this state.\n",
    "- ```config-path```, (str, default=\"\"): path to a .json configuration file that must be provided in the current version if the pretrained parameter is set to ```local```.  \n",
    "- ```save-checkpoint-path```, (str, default=\".model\"): the path in which the state of the model will be saved at the end of the current training cycle.\n",
    "- ```optimizer```, (choices=[\"sgd\", \"adam\", \"adamw\"], default=\"adam\"): the parameter determines the type of optimizer that will be used during the training process; the current version of NNTile supports three different optimization methods.\n",
    "- ```model-path```, (str, default=\".model\"): path where previously downloaded models from a remote HuggingFace source are saved, making it easy to access for future use.  \n",
    "- ```seq-len```, (int, default=1024): length of the input token sequence for training.\n",
    "- ```batch-size```, (int, default=1): batch size for the training process, which specifies the number of sentences processed by ```seq-len``` tokens between steps of the optimizer.\n",
    "- ```minibatch-size```, (int, default=-1): размер батча, под который выделяется память при обучении. Весь батч разбивается на целые минибатчи. Все минибатчи из одного батча один за другим «прогоняются» через модель для накапливания градиентов параметров.\n",
    "- ```minibatch-size-tile```, (type=int, default=-1): batch size for which memory is allocated during training. The entire batch is divided into entire minibatches. All minibatches from one batch are passed through the model one after another to accumulate parameter gradients.\n",
    "- ```hidden-size-tile```, (type=int, default=-1): size of pieces (tiles) into which the dimension ```hidden size``` (also known as ```embedding size```) is divided - the size of the multidimensional space into which incoming tokens are embedded. Only \"tiled\" tensors with the ```hidden-size-tile``` size along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```intermediate-size-tile```, (type=int, default=-1): size of pieces (tiles) into which the ```intermediate size``` dimension is divided. Only \"tiled\" tensors with the size ```intermediate-size-tile``` along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```n-head-tile```, (type=int, default=-1): size of pieces (tiles) into which the number of heads of the Transformer layer is divided. Only \"tiled\" tensors with a size of ```n-head-tile``` along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```dtype```, (choices=[\"fp32\", \"fp64\", \"fp32_fast_tf32\", \"bf16\", \"fp32_fast_fp16\", \"fp32_fast_bf16\"], default=\"fp32\"): set the data type from those supported by the NNTile framework in the current state. It allows users to select the appropriate option depending on their requirements.\n",
    "- ```restrict```, (choices=[\"cpu\", \"cuda\", None], default=None): the option allows users to set limits on the computing resources used during training. Selecting ```cpu``` limits training to CPU cores only, ```cuda``` limits training to GPU cores only, while setting it to ```None``` allows all available computing cores to be used.\n",
    "- ```flash-attention```, (action=\"store_true\"): a logical flag that, when used in the argument string, enables the current implementation of the FlashAttention algorithm (low-level Flash Attention kernels are currently not available) for processing data in the \"attention mechanism\" of the Transformers-type neural networks.\n",
    "- ```use-redux```, (action=\"store_true\"): a logical flag that, when used in the argument string, allows dependent tasks to be evaluated simultaneously, with the results then reduced to a single tensor.\n",
    "- ```dataset-path```, (default=\".data\"): path to the directory where previously prepared data sets are saved.\n",
    "- ```dataset-file```, (default=\"\"): path (relative to ```dataset-path```) to the .bin file that is created in the data preparation script for training.\n",
    "- ```lr```, (type=float, default=1e-4): step size for the optimization algorithm.\n",
    "- ```nepochs```, (type=int, default=1): number of complete passes through the training set\n",
    "- ```label-mask-token``` (type=int, default=3): index of the token that is responsible for masking the elements of the sequence. It must be consistent with the tokenizer used to avoid intersections of the indices of masked and normal tokens\n",
    "- ```n-masked-tokens-per-seq``` (type=int, default=1): the number of tokens in each sequence that will be randomly masked\n",
    "- ```n-masks-per-seq``` (type=int, default=1): number of masks that are applied to each sequence. These masks are applied before training begins and the result of their application is used for all epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00378fe0-670c-46c2-90dc-fcdd6677ec0a",
   "metadata": {},
   "source": [
    "## 1. Training from a random initial state and saving the weights of the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9daf74-1efe-4736-b157-8538c1693763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 15:17:50.975963: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-13 15:17:51.005963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/bert_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=32, minibatch_size=16, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-05, nepochs=1, n_masks_per_seq=2, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:568: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n",
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.8810820579528809 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=3, redux=False, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.32332372665405273 seconds\n",
      "From PyTorch loader to NNTile batches in 0.0026116371154785156 seconds\n",
      "Params+grads (GB): 0.515\n",
      "Activations  (GB): 4.306\n",
      "Optimizer    (GB): 0.515\n",
      "Persistent   (GB): 5.337\n",
      "Temporaries  (GB): 2.489\n",
      "Batch=1/10 Epoch=1/1 Loss=10.576749801635742\n",
      "Batch=2/10 Epoch=1/1 Loss=10.44841194152832\n",
      "Batch=3/10 Epoch=1/1 Loss=10.34447956085205\n",
      "Batch=4/10 Epoch=1/1 Loss=10.372608184814453\n",
      "Batch=5/10 Epoch=1/1 Loss=10.347963333129883\n",
      "Batch=6/10 Epoch=1/1 Loss=10.27706527709961\n",
      "Batch=7/10 Epoch=1/1 Loss=10.218955039978027\n",
      "Batch=8/10 Epoch=1/1 Loss=10.218851089477539\n",
      "Batch=9/10 Epoch=1/1 Loss=10.100818634033203\n",
      "Batch=10/10 Epoch=1/1 Loss=10.015487670898438\n",
      "NNTile training time: 3.074160575866699 seconds\n",
      "NNTile training throughput tokens/sec: 26647.924849177492\n",
      "NNTile loss on the last batch: 10.015487670898438\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/bert_training.py --pretrained=local \\\n",
    "                                                     --config-path=\"../wrappers/python/examples/bert_config.json\" \\\n",
    "                                                     --save-checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "                                                     --optimizer=\"adam\" \\\n",
    "                                                     --lr=1e-5 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "                                                     --batch-size=32 --minibatch-size=16 --n-masks-per-seq=2 \\\n",
    "                                                     --seq-len=512 --dataset-file=\"tinystories/train.bin\" \\\n",
    "                                                     --restrict=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e562a-b3e6-4a54-9ca4-63f5555b1c03",
   "metadata": {},
   "source": [
    "## 2. Load the model weights from the checkpoint and continue training with a different data type.\n",
    "\n",
    "This again requires setting the ```pretrained``` parameter to ```local```, the ```config-path``` parameter should point to the previously created ```.json``` configuration file, and the ```checkpoint-path``` should point to an existing PyTorch checkpoint file. \n",
    "Training can be continued using a different data type and on a different set of compute nodes.\n",
    "For example, here we switch to the ```fp32_fast_tf32``` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bbc4a2-79b8-4893-8437-294701e0de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 15:18:05.610730: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-13 15:18:05.640422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='local', checkpoint_path='.model/nntile_checkpoint.pt', config_path='../wrappers/python/examples/bert_config.json', save_checkpoint_path='.model/nntile_further_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=32, minibatch_size=16, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_tf32', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-05, nepochs=1, n_masks_per_seq=2, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:568: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n",
      "/workspace/nntile/notebooks/../wrappers/python/examples/bert_training.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.checkpoint_path)\n",
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.1739180088043213 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_tf32', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=3, redux=False, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.3007369041442871 seconds\n",
      "From PyTorch loader to NNTile batches in 0.002382040023803711 seconds\n",
      "Params+grads (GB): 0.515\n",
      "Activations  (GB): 4.306\n",
      "Optimizer    (GB): 0.515\n",
      "Persistent   (GB): 5.337\n",
      "Temporaries  (GB): 2.489\n",
      "Batch=1/10 Epoch=1/1 Loss=9.930547714233398\n",
      "Batch=2/10 Epoch=1/1 Loss=10.019750595092773\n",
      "Batch=3/10 Epoch=1/1 Loss=9.90141487121582\n",
      "Batch=4/10 Epoch=1/1 Loss=9.834307670593262\n",
      "Batch=5/10 Epoch=1/1 Loss=9.808883666992188\n",
      "Batch=6/10 Epoch=1/1 Loss=9.713675498962402\n",
      "Batch=7/10 Epoch=1/1 Loss=9.653298377990723\n",
      "Batch=8/10 Epoch=1/1 Loss=9.640944480895996\n",
      "Batch=9/10 Epoch=1/1 Loss=9.66589641571045\n",
      "Batch=10/10 Epoch=1/1 Loss=9.64930248260498\n",
      "NNTile training time: 3.1969919204711914 seconds\n",
      "NNTile training throughput tokens/sec: 25624.087278871244\n",
      "NNTile loss on the last batch: 9.64930248260498\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/bert_training.py --pretrained=local \\\n",
    "                                                     --checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "                                                     --config-path=\"../wrappers/python/examples/bert_config.json\" \\\n",
    "                                                     --save-checkpoint-path=\".model/nntile_further_checkpoint.pt\" \\\n",
    "                                                     --optimizer=\"adam\" --lr=1e-5 --dtype=fp32_fast_tf32 \\\n",
    "                                                     --nepochs=1 --batch-size=32 --minibatch-size=16 \\\n",
    "                                                     --dataset-file=\"tinystories/train.bin\" --n-masks-per-seq=2 \\\n",
    "                                                     --restrict=\"cuda\" --seq-len=512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a1c25-cc29-4ff0-bbb6-d84b0f414ed6",
   "metadata": {},
   "source": [
    "## 3. Continue training of a model loaded from the Hugging Face framework.\n",
    "\n",
    "The NNTile framework currently supports continued training of a model loaded from a remote source, as shown in our example from the Hugging Face framework library.\n",
    "The weights of the loaded model are passed to the model implemented in NNTile.\n",
    "To run such a scenario, the ```pretrained``` parameter must be set to ```remote```.\n",
    "The ```config-path``` and ```checkpoint-path``` parameters are no longer required, as the model configuration and layer weights will be obtained from the loaded model.\n",
    "Training can be continued using any data type and on any compute nodes that support the selected data type.\n",
    "In the example below, we switch to the ```bf16``` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3239307d-d4f1-47b4-9436-f8b03bebffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 15:18:19.582559: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-13 15:18:19.612462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='remote', checkpoint_path='', config_path='', save_checkpoint_path='.model/nntile_remote_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=32, minibatch_size=16, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-10, nepochs=1, n_masks_per_seq=2, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "model.safetensors: 100%|█████████████████████| 440M/440M [00:10<00:00, 42.3MB/s]\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.2169356346130371 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='bf16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=12, redux=False, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.7899689674377441 seconds\n",
      "From PyTorch loader to NNTile batches in 0.004277706146240234 seconds\n",
      "Params+grads (GB): 0.495\n",
      "Activations  (GB): 5.317\n",
      "Optimizer    (GB): 0.495\n",
      "Persistent   (GB): 6.308\n",
      "Temporaries  (GB): 4.838\n",
      "Batch=1/10 Epoch=1/1 Loss=1.518920660018921\n",
      "Batch=2/10 Epoch=1/1 Loss=3.0525412559509277\n",
      "Batch=3/10 Epoch=1/1 Loss=1.3193359375\n",
      "Batch=4/10 Epoch=1/1 Loss=1.7438147068023682\n",
      "Batch=5/10 Epoch=1/1 Loss=0.8870441913604736\n",
      "Batch=6/10 Epoch=1/1 Loss=1.0066733360290527\n",
      "Batch=7/10 Epoch=1/1 Loss=1.3167319297790527\n",
      "Batch=8/10 Epoch=1/1 Loss=1.5603841543197632\n",
      "Batch=9/10 Epoch=1/1 Loss=1.2307943105697632\n",
      "Batch=10/10 Epoch=1/1 Loss=1.4381510019302368\n",
      "NNTile training time: 4.428769826889038 seconds\n",
      "NNTile training throughput tokens/sec: 18497.235846990086\n",
      "NNTile loss on the last batch: 1.4381510019302368\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/bert_training.py --restrict=\"cuda\" --pretrained=remote \\\n",
    "                                                     --save-checkpoint-path=\".model/nntile_remote_checkpoint.pt\" \\\n",
    "                                                     --optimizer=\"adam\" --lr=1e-10 --dtype=bf16 --nepochs=1 \\\n",
    "                                                     --batch-size=32 --minibatch-size=16 --n-masks-per-seq=2 --seq-len=512 \\\n",
    "                                                     --dataset-file=\"tinystories/train.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd591de6-d7e3-4b35-aab5-1cd2698d0624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
