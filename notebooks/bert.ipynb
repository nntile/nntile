{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817cdfc7-ccec-4671-9013-24b42563e12a",
   "metadata": {},
   "source": [
    "# How to use the Bert model inside the NNTile framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a729f1-df4c-4b02-aba1-d2d6aaf47846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of experimental environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"2\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"Bert_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6e3f2-e834-40c4-8678-f09b5ddcc8d3",
   "metadata": {},
   "source": [
    "## Prepare dataset for the Masked Language Model with the Bert model \n",
    "\n",
    "- ```hf-dataset``` (str, default=\"roneneldan/TinyStories\"): the name of the dataset aligned with name in ```datasets``` library used to download it\n",
    "- ```dataset-path``` (str, default=\".data\"): path to the directory where previously prepared data sets are saved.\n",
    "- ```dataset-select``` (int, default=100): number of the first pieces of texts from the dataset used for training model\n",
    "- ```hf-tokenizer``` (str, default=\"bert-base-uncased\"): tokenizer used to train masked language model\n",
    "- ```tokenizer-path``` (str, default=\".model\"): path to the folder where the tokenizer data is stored\n",
    "- ```seq-len``` (int, deault=1024): length of the input token sequence for training\n",
    "- ```batch-size``` (int, default=1): batch size for the training process, which specifies the number of sentences processed by ```seq-len``` tokens between steps of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676ca1dd-8686-4ea9-a7b7-a93708412cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/nntile/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "# Describe parameters and arguments\n",
    "!python ../wrappers/python/examples/mlm_data_preparation.py --seq-len=512 --batch-size=8 --dataset-select=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d0ae4-42d8-4764-8052-0a6006aed1a2",
   "metadata": {},
   "source": [
    "## Arguments of the ```bert_training.py``` script, which is used to run all the scenarios below\n",
    "\n",
    "- ```remote-model-name```, (str, default=\"bert-base-uncased\"): the name of the Bert architecture-based model that resides in the HuggingFace framework and will be used to initialize the configuration and initial state of the NNTile model.\n",
    "- ```pretrained```, (choices=[\"local\", \"remote\"], default=\"local\"): the source type of the pre-trained model. The remote option loads the model ```remote-model-name``` from the Huggingface infrastructure. The ```local``` option requires a configuration file path (```config-path```) to start training from a randomly initialized state, or to continue training if a checkpoint file path (```checkpoint-path```) is also provided.\n",
    "- ```checkpoint-path```, (str, default=\"\"): path to the saved state of the pre-trained model weights. If the file is available, training will continue from this state.\n",
    "- ```config-path```, (str, default=\"\"): path to a .json configuration file that must be provided in the current version if the pretrained parameter is set to ```local```.  \n",
    "- ```save-checkpoint-path```, (str, default=\".model\"): the path in which the state of the model will be saved at the end of the current training cycle.\n",
    "- ```optimizer```, (choices=[\"sgd\", \"adam\", \"adamw\"], default=\"adam\"): the parameter determines the type of optimizer that will be used during the training process; the current version of NNTile supports three different optimization methods.\n",
    "- ```model-path```, (str, default=\".model\"): path where previously downloaded models from a remote HuggingFace source are saved, making it easy to access for future use.  \n",
    "- ```seq-len```, (int, default=1024): length of the input token sequence for training.\n",
    "- ```batch-size```, (int, default=1): batch size for the training process, which specifies the number of sentences processed by ```seq-len``` tokens between steps of the optimizer.\n",
    "- ```minibatch-size```, (int, default=-1): размер батча, под который выделяется память при обучении. Весь батч разбивается на целые минибатчи. Все минибатчи из одного батча один за другим «прогоняются» через модель для накапливания градиентов параметров.\n",
    "- ```minibatch-size-tile```, (type=int, default=-1): batch size for which memory is allocated during training. The entire batch is divided into entire minibatches. All minibatches from one batch are passed through the model one after another to accumulate parameter gradients.\n",
    "- ```hidden-size-tile```, (type=int, default=-1): size of pieces (tiles) into which the dimension ```hidden size``` (also known as ```embedding size```) is divided - the size of the multidimensional space into which incoming tokens are embedded. Only \"tiled\" tensors with the ```hidden-size-tile``` size along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```intermediate-size-tile```, (type=int, default=-1): size of pieces (tiles) into which the ```intermediate size``` dimension is divided. Only \"tiled\" tensors with the size ```intermediate-size-tile``` along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```n-head-tile```, (type=int, default=-1): size of pieces (tiles) into which the number of heads of the Transformer layer is divided. Only \"tiled\" tensors with a size of ```n-head-tile``` along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```dtype```, (choices=[\"fp32\", \"fp64\", \"fp32_fast_tf32\", \"bf16\", \"fp32_fast_fp16\", \"fp32_fast_bf16\"], default=\"fp32\"): set the data type from those supported by the NNTile framework in the current state. It allows users to select the appropriate option depending on their requirements.\n",
    "- ```restrict```, (choices=[\"cpu\", \"cuda\", None], default=None): the option allows users to set limits on the computing resources used during training. Selecting ```cpu``` limits training to CPU cores only, ```cuda``` limits training to GPU cores only, while setting it to ```None``` allows all available computing cores to be used.\n",
    "- ```flash-attention```, (action=\"store_true\"): a logical flag that, when used in the argument string, enables the current implementation of the FlashAttention algorithm (low-level Flash Attention kernels are currently not available) for processing data in the \"attention mechanism\" of the Transformers-type neural networks.\n",
    "- ```use-redux```, (action=\"store_true\"): a logical flag that, when used in the argument string, allows dependent tasks to be evaluated simultaneously, with the results then reduced to a single tensor.\n",
    "- ```dataset-path```, (default=\".data\"): path to the directory where previously prepared data sets are saved.\n",
    "- ```dataset-file```, (default=\"\"): path (relative to ```dataset-path```) to the .bin file that is created in the data preparation script for training.\n",
    "- ```lr```, (type=float, default=1e-4): step size for the optimization algorithm.\n",
    "- ```nepochs```, (type=int, default=1): number of complete passes through the training set\n",
    "- ```label-mask-token``` (type=int, default=3): index of the token that is responsible for masking the elements of the sequence. It must be consistent with the tokenizer used to avoid intersections of the indices of masked and normal tokens\n",
    "- ```n-masked-tokens-per-seq``` (type=int, default=1): the number of tokens in each sequence that will be randomly masked\n",
    "- ```n-masks-per-seq``` (type=int, default=1): number of masks that are applied to each sequence. These masks are applied before training begins and the result of their application is used for all epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00378fe0-670c-46c2-90dc-fcdd6677ec0a",
   "metadata": {},
   "source": [
    "## 1. Training from a random initial state and saving the weights of the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9daf74-1efe-4736-b157-8538c1693763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/bert_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-05, nepochs=1, n_masks_per_seq=3, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "/home/jovyan/.mlspace/envs/nntile/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:546: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
      "  warnings.warn(\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.27330589294433594 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=3, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.38168883323669434 seconds\n",
      "From PyTorch loader to NNTile batches in 0.0019202232360839844 seconds\n",
      "Params+grads (GB): 0.515\n",
      "Activations  (GB): 2.162\n",
      "Optimizer    (GB): 0.515\n",
      "Persistent   (GB): 3.193\n",
      "Temporaries  (GB): 1.810\n",
      "Batch=1/6 Epoch=1/1 Loss=14.015607833862305\n",
      "Batch=2/6 Epoch=1/1 Loss=13.239965438842773\n",
      "Batch=3/6 Epoch=1/1 Loss=13.971638679504395\n",
      "Batch=4/6 Epoch=1/1 Loss=13.628914833068848\n",
      "Batch=5/6 Epoch=1/1 Loss=13.649360656738281\n",
      "Batch=6/6 Epoch=1/1 Loss=13.54525375366211\n",
      "NNTile training time: 1.0899133682250977 seconds\n",
      "NNTile training throughput tokens/sec: 15032.387415048426\n",
      "NNTile loss on the last batch: 13.54525375366211\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/bert_training.py --pretrained=local \\\n",
    "                                                     --config-path=\"../wrappers/python/examples/bert_config.json\" \\\n",
    "                                                     --save-checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "                                                     --optimizer=\"adam\" \\\n",
    "                                                     --lr=1e-5 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "                                                     --batch-size=8 --minibatch-size=4 --n-masks-per-seq=3 \\\n",
    "                                                     --seq-len=1024 --dataset-file=\"tinystories/train.bin\" \\\n",
    "                                                     --restrict=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e562a-b3e6-4a54-9ca4-63f5555b1c03",
   "metadata": {},
   "source": [
    "## 2. Load the model weights from the checkpoint and continue training with a different data type.\n",
    "\n",
    "This again requires setting the ```pretrained``` parameter to ```local```, the ```config-path``` parameter should point to the previously created ```.json``` configuration file, and the ```checkpoint-path``` should point to an existing PyTorch checkpoint file. \n",
    "Training can be continued using a different data type and on a different set of compute nodes.\n",
    "For example, here we switch to the ```fp32_fast_tf32``` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bbc4a2-79b8-4893-8437-294701e0de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='local', checkpoint_path='.model/nntile_checkpoint.pt', config_path='../wrappers/python/examples/bert_config.json', save_checkpoint_path='.model/nntile_further_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_tf32', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-05, nepochs=1, n_masks_per_seq=3, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "/home/jovyan/.mlspace/envs/nntile/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:546: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
      "  warnings.warn(\n",
      "/home/jovyan/katrutsa/nntile/notebooks/../wrappers/python/examples/bert_training.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.checkpoint_path)\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.2807948589324951 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_tf32', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=3, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.3763244152069092 seconds\n",
      "From PyTorch loader to NNTile batches in 0.0017652511596679688 seconds\n",
      "Params+grads (GB): 0.515\n",
      "Activations  (GB): 2.162\n",
      "Optimizer    (GB): 0.515\n",
      "Persistent   (GB): 3.193\n",
      "Temporaries  (GB): 1.810\n",
      "Batch=1/6 Epoch=1/1 Loss=13.383455276489258\n",
      "Batch=2/6 Epoch=1/1 Loss=13.411012649536133\n",
      "Batch=3/6 Epoch=1/1 Loss=13.327157974243164\n",
      "Batch=4/6 Epoch=1/1 Loss=13.063363075256348\n",
      "Batch=5/6 Epoch=1/1 Loss=13.12226390838623\n",
      "Batch=6/6 Epoch=1/1 Loss=13.231039047241211\n",
      "NNTile training time: 1.0806474685668945 seconds\n",
      "NNTile training throughput tokens/sec: 15161.281062109658\n",
      "NNTile loss on the last batch: 13.231039047241211\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/bert_training.py --pretrained=local \\\n",
    "                                                     --checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "                                                     --config-path=\"../wrappers/python/examples/bert_config.json\" \\\n",
    "                                                     --save-checkpoint-path=\".model/nntile_further_checkpoint.pt\" \\\n",
    "                                                     --optimizer=\"adam\" --lr=1e-5 --dtype=fp32_fast_tf32 \\\n",
    "                                                     --nepochs=1 --batch-size=8 --minibatch-size=4 \\\n",
    "                                                     --dataset-file=\"tinystories/train.bin\" --n-masks-per-seq=3 \\\n",
    "                                                     --restrict=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a1c25-cc29-4ff0-bbb6-d84b0f414ed6",
   "metadata": {},
   "source": [
    "## 3. Continue training of a model loaded from the Hugging Face framework.\n",
    "\n",
    "The NNTile framework currently supports continued training of a model loaded from a remote source, as shown in our example from the Hugging Face framework library.\n",
    "The weights of the loaded model are passed to the model implemented in NNTile.\n",
    "To run such a scenario, the ```pretrained``` parameter must be set to ```remote```.\n",
    "The ```config-path``` and ```checkpoint-path``` parameters are no longer required, as the model configuration and layer weights will be obtained from the loaded model.\n",
    "Training can be continued using any data type and on any compute nodes that support the selected data type.\n",
    "In the example below, we switch to the ```bf16``` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3239307d-d4f1-47b4-9436-f8b03bebffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='remote', checkpoint_path='', config_path='', save_checkpoint_path='.model/nntile_remote_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-13, nepochs=1, n_masks_per_seq=1, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "model.safetensors: 100%|██████████████████████| 440M/440M [00:02<00:00, 178MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.25333189964294434 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='bf16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=12, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 2.7334659099578857 seconds\n",
      "From PyTorch loader to NNTile batches in 0.0012080669403076172 seconds\n",
      "Params+grads (GB): 0.495\n",
      "Activations  (GB): 2.663\n",
      "Optimizer    (GB): 0.495\n",
      "Persistent   (GB): 3.654\n",
      "Temporaries  (GB): 3.554\n",
      "Batch=1/2 Epoch=1/1 Loss=24.864215850830078\n",
      "Batch=2/2 Epoch=1/1 Loss=26.211429595947266\n",
      "NNTile training time: 0.6205909252166748 seconds\n",
      "NNTile training throughput tokens/sec: 26400.643860977576\n",
      "NNTile loss on the last batch: 26.211429595947266\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/bert_training.py --restrict=\"cuda\" --pretrained=remote \\\n",
    "                                                     --save-checkpoint-path=\".model/nntile_remote_checkpoint.pt\" \\\n",
    "                                                     --optimizer=\"adam\" --lr=1e-13 --dtype=bf16 --nepochs=1 \\\n",
    "                                                     --batch-size=8 --minibatch-size=4 --n-masks-per-seq=1 --seq-len=1024 \\\n",
    "                                                     --dataset-file=\"tinystories/train.bin\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-nntile]",
   "language": "python",
   "name": "conda-env-.mlspace-nntile-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
