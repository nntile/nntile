{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817cdfc7-ccec-4671-9013-24b42563e12a",
   "metadata": {},
   "source": [
    "# How to use the RoBerta model inside the NNTile framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a729f1-df4c-4b02-aba1-d2d6aaf47846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of experimental environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"2\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"Bert_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91bbaa7-6de5-48c5-80fe-bd84c42fb474",
   "metadata": {},
   "source": [
    "## Prepare dataset for the Masked Language Model with the RoBerta model \n",
    "\n",
    "- ```hf-dataset``` (str, default=\"roneneldan/TinyStories\"): the name of the dataset aligned with name in ```datasets``` library used to download it\n",
    "- ```dataset-path``` (str, default=\".data\"): path to the directory where previously prepared data sets are saved.\n",
    "- ```dataset-select``` (int, default=100): number of the first pieces of texts from the dataset used for training model\n",
    "- ```hf-tokenizer``` (str, default=\"bert-base-uncased\"): tokenizer used to train masked language model\n",
    "- ```tokenizer-path``` (str, default=\".model\"): path to the folder where the tokenizer data is stored\n",
    "- ```seq-len``` (int, deault=1024): length of the input token sequence for training\n",
    "- ```batch-size``` (int, default=1): batch size for the training process, which specifies the number of sentences processed by ```seq-len``` tokens between steps of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ca1dd-8686-4ea9-a7b7-a93708412cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "# Describe parameters and arguments\n",
    "!python ../wrappers/python/examples/mlm_data_preparation.py --seq-len=512 --batch-size=8 --dataset-select=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d0ae4-42d8-4764-8052-0a6006aed1a2",
   "metadata": {},
   "source": [
    "## Arguments of the ```roberta_training.py``` script, which is used to run all the scenarios below\n",
    "\n",
    "- ```remote-model-name```, (str, default=\"FacebookAI/roberta-base\"): the name of the Bert architecture-based model that resides in the HuggingFace framework and will be used to initialize the configuration and initial state of the NNTile model.\n",
    "- ```pretrained```, (choices=[\"local\", \"remote\"], default=\"local\"): the source type of the pre-trained model. The remote option loads the model ```remote-model-name``` from the Huggingface infrastructure. The ```local``` option requires a configuration file path (```config-path```) to start training from a randomly initialized state, or to continue training if a checkpoint file path (```checkpoint-path```) is also provided.\n",
    "- ```checkpoint-path```, (str, default=\"\"): path to the saved state of the pre-trained model weights. If the file is available, training will continue from this state.\n",
    "- ```config-path```, (str, default=\"\"): path to a .json configuration file that must be provided in the current version if the pretrained parameter is set to ```local```.  \n",
    "- ```save-checkpoint-path```, (str, default=\".model\"): the path in which the state of the model will be saved at the end of the current training cycle.\n",
    "- ```optimizer```, (choices=[\"sgd\", \"adam\", \"adamw\"], default=\"adam\"): the parameter determines the type of optimizer that will be used during the training process; the current version of NNTile supports three different optimization methods.\n",
    "- ```model-path```, (str, default=\".model\"): path where previously downloaded models from a remote HuggingFace source are saved, making it easy to access for future use.  \n",
    "- ```seq-len```, (int, default=1024): length of the input token sequence for training.\n",
    "- ```batch-size```, (int, default=1): batch size for the training process, which specifies the number of sentences processed by ```seq-len``` tokens between steps of the optimizer.\n",
    "- ```minibatch-size```, (int, default=-1): размер батча, под который выделяется память при обучении. Весь батч разбивается на целые минибатчи. Все минибатчи из одного батча один за другим «прогоняются» через модель для накапливания градиентов параметров.\n",
    "- ```minibatch-size-tile```, (type=int, default=-1): batch size for which memory is allocated during training. The entire batch is divided into entire minibatches. All minibatches from one batch are passed through the model one after another to accumulate parameter gradients.\n",
    "- ```hidden-size-tile```, (type=int, default=-1): size of pieces (tiles) into which the dimension ```hidden size``` (also known as ```embedding size```) is divided - the size of the multidimensional space into which incoming tokens are embedded. Only \"tiled\" tensors with the ```hidden-size-tile``` size along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```intermediate-size-tile```, (type=int, default=-1): size of pieces (tiles) into which the ```intermediate size``` dimension is divided. Only \"tiled\" tensors with the size ```intermediate-size-tile``` along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```n-head-tile```, (type=int, default=-1): size of pieces (tiles) into which the number of heads of the Transformer layer is divided. Only \"tiled\" tensors with a size of ```n-head-tile``` along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```dtype```, (choices=[\"fp32\", \"fp64\", \"fp32_fast_tf32\", \"bf16\", \"fp32_fast_fp16\", \"fp32_fast_bf16\"], default=\"fp32\"): set the data type from those supported by the NNTile framework in the current state. It allows users to select the appropriate option depending on their requirements.\n",
    "- ```restrict```, (choices=[\"cpu\", \"cuda\", None], default=None): the option allows users to set limits on the computing resources used during training. Selecting ```cpu``` limits training to CPU cores only, ```cuda``` limits training to GPU cores only, while setting it to ```None``` allows all available computing cores to be used.\n",
    "- ```flash-attention```, (action=\"store_true\"): a logical flag that, when used in the argument string, enables the current implementation of the FlashAttention algorithm (low-level Flash Attention kernels are currently not available) for processing data in the \"attention mechanism\" of the Transformers-type neural networks.\n",
    "- ```use-redux```, (action=\"store_true\"): a logical flag that, when used in the argument string, allows dependent tasks to be evaluated simultaneously, with the results then reduced to a single tensor.\n",
    "- ```dataset-path```, (default=\".data\"): path to the directory where previously prepared data sets are saved.\n",
    "- ```dataset-file```, (default=\"\"): path (relative to ```dataset-path```) to the .bin file that is created in the data preparation script for training.\n",
    "- ```lr```, (type=float, default=1e-4): step size for the optimization algorithm.\n",
    "- ```nepochs```, (type=int, default=1): number of complete passes through the training set\n",
    "- ```label-mask-token``` (type=int, default=3): index of the token that is responsible for masking the elements of the sequence. It must be consistent with the tokenizer used to avoid intersections of the indices of masked and normal tokens\n",
    "- ```n-masked-tokens-per-seq``` (type=int, default=1): the number of tokens in each sequence that will be randomly masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00378fe0-670c-46c2-90dc-fcdd6677ec0a",
   "metadata": {},
   "source": [
    "## 1. Training from a random initial state and saving the weights of the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9daf74-1efe-4736-b157-8538c1693763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/bert_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-05, nepochs=3, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "/home/jovyan/.mlspace/envs/nntile/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:546: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
      "  warnings.warn(\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.2667415142059326 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=3, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.42931151390075684 seconds\n",
      "From PyTorch loader to NNTile batches in 0.0020961761474609375 seconds\n",
      "Params+grads (GB): 0.515\n",
      "Activations  (GB): 1.081\n",
      "Optimizer    (GB): 0.515\n",
      "Persistent   (GB): 2.112\n",
      "Temporaries  (GB): 0.623\n",
      "Batch=1/3 Epoch=1/1 Loss=42.43129348754883\n",
      "Batch=2/3 Epoch=1/1 Loss=42.30378723144531\n",
      "Batch=3/3 Epoch=1/1 Loss=42.08125305175781\n",
      "Batch=1/3 Epoch=1/1 Loss=40.18657302856445\n",
      "Batch=2/3 Epoch=1/1 Loss=41.488033294677734\n",
      "Batch=3/3 Epoch=1/1 Loss=41.537235260009766\n",
      "Batch=1/3 Epoch=1/1 Loss=41.00205993652344\n",
      "Batch=2/3 Epoch=1/1 Loss=38.76338195800781\n",
      "Batch=3/3 Epoch=1/1 Loss=39.26043701171875\n",
      "NNTile training time: 1.2874605655670166 seconds\n",
      "NNTile training throughput tokens/sec: 28633.11000423889\n",
      "NNTile loss on the last batch: 39.26043701171875\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/roberta_training.py --pretrained=local \\\n",
    "                                                        --config-path=\"../wrappers/python/examples/bert_config.json\" \\\n",
    "                                                        --save-checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "                                                        --optimizer=\"adam\" --lr=1e-5 --dtype=fp32_fast_fp16 \\\n",
    "                                                        --nepochs=3  --batch-size=8 --minibatch-size=4 --seq-len=512 \\\n",
    "                                                        --dataset-file=\"tinystories/train.bin\" --restrict=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e562a-b3e6-4a54-9ca4-63f5555b1c03",
   "metadata": {},
   "source": [
    "## 2. Load the model weights from the checkpoint and continue training with a different data type.\n",
    "\n",
    "This again requires setting the ```pretrained``` parameter to ```local```, the ```config-path``` parameter should point to the previously created ```.json``` configuration file, and the ```checkpoint-path``` should point to an existing PyTorch checkpoint file. \n",
    "Training can be continued using a different data type and on a different set of compute nodes.\n",
    "For example, here we switch to the ```fp32_fast_tf32``` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bbc4a2-79b8-4893-8437-294701e0de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='local', checkpoint_path='.model/nntile_checkpoint.pt', config_path='../wrappers/python/examples/bert_config.json', save_checkpoint_path='.model/nntile_further_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict=None, flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-05, nepochs=3, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "/home/jovyan/.mlspace/envs/nntile/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:546: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
      "  warnings.warn(\n",
      "/home/jovyan/katrutsa/nntile/notebooks/../wrappers/python/examples/roberta_training.py:126: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.checkpoint_path)\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.24788260459899902 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=3, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.43167829513549805 seconds\n",
      "From PyTorch loader to NNTile batches in 0.0013222694396972656 seconds\n",
      "Params+grads (GB): 0.515\n",
      "Activations  (GB): 2.162\n",
      "Optimizer    (GB): 0.515\n",
      "Persistent   (GB): 3.193\n",
      "Temporaries  (GB): 1.810\n",
      "Batch=1/1 Epoch=1/1 Loss=40.17149353027344\n",
      "Batch=1/1 Epoch=1/1 Loss=39.701385498046875\n",
      "Batch=1/1 Epoch=1/1 Loss=37.803062438964844\n",
      "NNTile training time: 1.0575482845306396 seconds\n",
      "NNTile training throughput tokens/sec: 23238.65525526081\n",
      "NNTile loss on the last batch: 37.803062438964844\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/roberta_training.py --pretrained=local --checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "                                                        --config-path=\"../wrappers/python/examples/bert_config.json\" \\\n",
    "                                                        --save-checkpoint-path=\".model/nntile_further_checkpoint.pt\" \\\n",
    "                                                        --optimizer=\"adam\" --lr=1e-5 --dtype=fp32_fast_fp16 \\\n",
    "                                                        --nepochs=3 --batch-size=8 --minibatch-size=4 \\\n",
    "                                                        --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a1c25-cc29-4ff0-bbb6-d84b0f414ed6",
   "metadata": {},
   "source": [
    "## 3. Continue training of a model loaded from the Hugging Face framework.\n",
    "\n",
    "The NNTile framework currently supports continued training of a model loaded from a remote source, as shown in our example from the Hugging Face framework library.\n",
    "The weights of the loaded model are passed to the model implemented in NNTile.\n",
    "To run such a scenario, the ```pretrained``` parameter must be set to ```remote```.\n",
    "The ```config-path``` and ```checkpoint-path``` parameters are no longer required, as the model configuration and layer weights will be obtained from the loaded model.\n",
    "Training can be continued using any data type and on any compute nodes that support the selected data type.\n",
    "In the example below, we switch to the ```bf16``` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3239307d-d4f1-47b4-9436-f8b03bebffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='bert-base-uncased', pretrained='remote', checkpoint_path='', config_path='', save_checkpoint_path='.model/nntile_remote_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-13, nepochs=3, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.2492074966430664 seconds\n",
      "BertConfigNNTile(vocab_size=30522, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='bf16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=512, num_hidden_layers=12, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=2, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.8016042709350586 seconds\n",
      "From PyTorch loader to NNTile batches in 0.003301858901977539 seconds\n",
      "Params+grads (GB): 0.495\n",
      "Activations  (GB): 1.332\n",
      "Optimizer    (GB): 0.495\n",
      "Persistent   (GB): 2.322\n",
      "Temporaries  (GB): 1.212\n",
      "Batch=1/3 Epoch=1/1 Loss=2.242187738418579\n",
      "Batch=2/3 Epoch=1/1 Loss=4.6393232345581055\n",
      "Batch=3/3 Epoch=1/1 Loss=4.247396469116211\n",
      "Batch=1/3 Epoch=1/1 Loss=4.4817705154418945\n",
      "Batch=2/3 Epoch=1/1 Loss=4.582030773162842\n",
      "Batch=3/3 Epoch=1/1 Loss=5.774251461029053\n",
      "Batch=1/3 Epoch=1/1 Loss=4.8056640625\n",
      "Batch=2/3 Epoch=1/1 Loss=3.461181879043579\n",
      "Batch=3/3 Epoch=1/1 Loss=5.8463544845581055\n",
      "NNTile training time: 1.636486291885376 seconds\n",
      "NNTile training throughput tokens/sec: 22526.311514366207\n",
      "NNTile loss on the last batch: 5.8463544845581055\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/roberta_training.py --restrict=\"cuda\" --pretrained=remote \\\n",
    "                                                        --save-checkpoint-path=\".model/nntile_remote_checkpoint.pt\" \\\n",
    "                                                        --optimizer=\"adam\" --lr=1e-13 --dtype=bf16 --nepochs=3 \\\n",
    "                                                        --batch-size=8 --minibatch-size=4 --seq-len=512  \\\n",
    "                                                        --dataset-file=\"tinystories/train.bin\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-nntile]",
   "language": "python",
   "name": "conda-env-.mlspace-nntile-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
