{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817cdfc7-ccec-4671-9013-24b42563e12a",
   "metadata": {},
   "source": [
    "# How to use the RoBerta model inside the NNTile framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a729f1-df4c-4b02-aba1-d2d6aaf47846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of experimental environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"2\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"Bert_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91bbaa7-6de5-48c5-80fe-bd84c42fb474",
   "metadata": {},
   "source": [
    "## Prepare dataset for the Masked Language Model with the RoBerta model \n",
    "\n",
    "- ```hf-dataset``` (str, default=\"roneneldan/TinyStories\"): the name of the dataset aligned with name in ```datasets``` library used to download it\n",
    "- ```dataset-path``` (str, default=\".data\"): path to the directory where previously prepared data sets are saved.\n",
    "- ```dataset-select``` (int, default=100): number of the first pieces of texts from the dataset used for training model\n",
    "- ```hf-tokenizer``` (str, default=\"bert-base-uncased\"): tokenizer used to train masked language model\n",
    "- ```tokenizer-path``` (str, default=\".model\"): path to the folder where the tokenizer data is stored\n",
    "- ```seq-len``` (int, deault=1024): length of the input token sequence for training\n",
    "- ```batch-size``` (int, default=1): batch size for the training process, which specifies the number of sentences processed by ```seq-len``` tokens between steps of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676ca1dd-8686-4ea9-a7b7-a93708412cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████| 25.0/25.0 [00:00<00:00, 209kB/s]\n",
      "config.json: 100%|█████████████████████████████| 481/481 [00:00<00:00, 5.01MB/s]\n",
      "vocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 9.90MB/s]\n",
      "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 42.4MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 5.99MB/s]\n",
      "/home/jovyan/.mlspace/envs/nntile/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "# Describe parameters and arguments\n",
    "!python ../wrappers/python/examples/mlm_data_preparation.py --seq-len=512 \\\n",
    "                                                            --batch-size=8 \\\n",
    "                                                            --dataset-select=100 \\\n",
    "                                                            --hf-tokenizer=\"FacebookAI/roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d0ae4-42d8-4764-8052-0a6006aed1a2",
   "metadata": {},
   "source": [
    "## Arguments of the ```roberta_training.py``` script, which is used to run all the scenarios below\n",
    "\n",
    "- ```remote-model-name```, (str, default=\"FacebookAI/roberta-base\"): the name of the Bert architecture-based model that resides in the HuggingFace framework and will be used to initialize the configuration and initial state of the NNTile model.\n",
    "- ```pretrained```, (choices=[\"local\", \"remote\"], default=\"local\"): the source type of the pre-trained model. The remote option loads the model ```remote-model-name``` from the Huggingface infrastructure. The ```local``` option requires a configuration file path (```config-path```) to start training from a randomly initialized state, or to continue training if a checkpoint file path (```checkpoint-path```) is also provided.\n",
    "- ```checkpoint-path```, (str, default=\"\"): path to the saved state of the pre-trained model weights. If the file is available, training will continue from this state.\n",
    "- ```config-path```, (str, default=\"\"): path to a .json configuration file that must be provided in the current version if the pretrained parameter is set to ```local```.  \n",
    "- ```save-checkpoint-path```, (str, default=\".model\"): the path in which the state of the model will be saved at the end of the current training cycle.\n",
    "- ```optimizer```, (choices=[\"sgd\", \"adam\", \"adamw\"], default=\"adam\"): the parameter determines the type of optimizer that will be used during the training process; the current version of NNTile supports three different optimization methods.\n",
    "- ```model-path```, (str, default=\".model\"): path where previously downloaded models from a remote HuggingFace source are saved, making it easy to access for future use.  \n",
    "- ```seq-len```, (int, default=1024): length of the input token sequence for training.\n",
    "- ```batch-size```, (int, default=1): batch size for the training process, which specifies the number of sentences processed by ```seq-len``` tokens between steps of the optimizer.\n",
    "- ```minibatch-size```, (int, default=-1): размер батча, под который выделяется память при обучении. Весь батч разбивается на целые минибатчи. Все минибатчи из одного батча один за другим «прогоняются» через модель для накапливания градиентов параметров.\n",
    "- ```minibatch-size-tile```, (type=int, default=-1): batch size for which memory is allocated during training. The entire batch is divided into entire minibatches. All minibatches from one batch are passed through the model one after another to accumulate parameter gradients.\n",
    "- ```hidden-size-tile```, (type=int, default=-1): size of pieces (tiles) into which the dimension ```hidden size``` (also known as ```embedding size```) is divided - the size of the multidimensional space into which incoming tokens are embedded. Only \"tiled\" tensors with the ```hidden-size-tile``` size along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```intermediate-size-tile```, (type=int, default=-1): size of pieces (tiles) into which the ```intermediate size``` dimension is divided. Only \"tiled\" tensors with the size ```intermediate-size-tile``` along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```n-head-tile```, (type=int, default=-1): size of pieces (tiles) into which the number of heads of the Transformer layer is divided. Only \"tiled\" tensors with a size of ```n-head-tile``` along the corresponding axis are processed on the CPU and GPU.\n",
    "- ```dtype```, (choices=[\"fp32\", \"fp64\", \"fp32_fast_tf32\", \"bf16\", \"fp32_fast_fp16\", \"fp32_fast_bf16\"], default=\"fp32\"): set the data type from those supported by the NNTile framework in the current state. It allows users to select the appropriate option depending on their requirements.\n",
    "- ```restrict```, (choices=[\"cpu\", \"cuda\", None], default=None): the option allows users to set limits on the computing resources used during training. Selecting ```cpu``` limits training to CPU cores only, ```cuda``` limits training to GPU cores only, while setting it to ```None``` allows all available computing cores to be used.\n",
    "- ```flash-attention```, (action=\"store_true\"): a logical flag that, when used in the argument string, enables the current implementation of the FlashAttention algorithm (low-level Flash Attention kernels are currently not available) for processing data in the \"attention mechanism\" of the Transformers-type neural networks.\n",
    "- ```use-redux```, (action=\"store_true\"): a logical flag that, when used in the argument string, allows dependent tasks to be evaluated simultaneously, with the results then reduced to a single tensor.\n",
    "- ```dataset-path```, (default=\".data\"): path to the directory where previously prepared data sets are saved.\n",
    "- ```dataset-file```, (default=\"\"): path (relative to ```dataset-path```) to the .bin file that is created in the data preparation script for training.\n",
    "- ```lr```, (type=float, default=1e-4): step size for the optimization algorithm.\n",
    "- ```nepochs```, (type=int, default=1): number of complete passes through the training set\n",
    "- ```label-mask-token``` (type=int, default=3): index of the token that is responsible for masking the elements of the sequence. It must be consistent with the tokenizer used to avoid intersections of the indices of masked and normal tokens\n",
    "- ```n-masked-tokens-per-seq``` (type=int, default=1): the number of tokens in each sequence that will be randomly masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00378fe0-670c-46c2-90dc-fcdd6677ec0a",
   "metadata": {},
   "source": [
    "## 1. Training from a random initial state and saving the weights of the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b9daf74-1efe-4736-b157-8538c1693763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='FacebookAI/roberta-base', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/roberta_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-05, nepochs=3, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.24262237548828125 seconds\n",
      "BertConfigNNTile(vocab_size=50265, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=514, num_hidden_layers=3, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=1, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.6719303131103516 seconds\n",
      "From PyTorch loader to NNTile batches in 0.004417896270751953 seconds\n",
      "Params+grads (GB): 0.741\n",
      "Activations  (GB): 1.382\n",
      "Optimizer    (GB): 0.741\n",
      "Persistent   (GB): 2.865\n",
      "Temporaries  (GB): 0.623\n",
      "Batch=1/12 Epoch=1/1 Loss=11.017107963562012\n",
      "Batch=2/12 Epoch=1/1 Loss=10.85515308380127\n",
      "Batch=3/12 Epoch=1/1 Loss=11.053153991699219\n",
      "Batch=4/12 Epoch=1/1 Loss=11.116438865661621\n",
      "Batch=5/12 Epoch=1/1 Loss=10.799418449401855\n",
      "Batch=6/12 Epoch=1/1 Loss=10.884441375732422\n",
      "Batch=7/12 Epoch=1/1 Loss=10.75899600982666\n",
      "Batch=8/12 Epoch=1/1 Loss=10.883479118347168\n",
      "Batch=9/12 Epoch=1/1 Loss=10.829633712768555\n",
      "Batch=10/12 Epoch=1/1 Loss=10.688567161560059\n",
      "Batch=11/12 Epoch=1/1 Loss=10.823612213134766\n",
      "Batch=12/12 Epoch=1/1 Loss=10.593282699584961\n",
      "NNTile training time: 2.5417377948760986 seconds\n",
      "NNTile training throughput tokens/sec: 19337.950633257984\n",
      "NNTile loss on the last batch: 10.593282699584961\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/roberta_training.py --pretrained=local \\\n",
    "                                                        --config-path=\"../wrappers/python/examples/roberta_config.json\" \\\n",
    "                                                        --save-checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "                                                        --optimizer=\"adam\" --lr=1e-5 --dtype=fp32_fast_fp16 \\\n",
    "                                                        --nepochs=3  --batch-size=8 --minibatch-size=4 --seq-len=512 \\\n",
    "                                                        --dataset-file=\"tinystories/train.bin\" --restrict=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e562a-b3e6-4a54-9ca4-63f5555b1c03",
   "metadata": {},
   "source": [
    "## 2. Load the model weights from the checkpoint and continue training with a different data type.\n",
    "\n",
    "This again requires setting the ```pretrained``` parameter to ```local```, the ```config-path``` parameter should point to the previously created ```.json``` configuration file, and the ```checkpoint-path``` should point to an existing PyTorch checkpoint file. \n",
    "Training can be continued using a different data type and on a different set of compute nodes.\n",
    "For example, here we switch to the ```fp32_fast_tf32``` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51bbc4a2-79b8-4893-8437-294701e0de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='FacebookAI/roberta-base', pretrained='local', checkpoint_path='.model/nntile_checkpoint.pt', config_path='../wrappers/python/examples/roberta_config.json', save_checkpoint_path='.model/nntile_further_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict=None, flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-05, nepochs=3, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "/home/jovyan/katrutsa/nntile/notebooks/../wrappers/python/examples/roberta_training.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.checkpoint_path)\n",
      "RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.24263310432434082 seconds\n",
      "BertConfigNNTile(vocab_size=50265, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=514, num_hidden_layers=3, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=1, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 0.7627031803131104 seconds\n",
      "From PyTorch loader to NNTile batches in 0.004656314849853516 seconds\n",
      "Params+grads (GB): 0.741\n",
      "Activations  (GB): 1.382\n",
      "Optimizer    (GB): 0.741\n",
      "Persistent   (GB): 2.865\n",
      "Temporaries  (GB): 0.623\n",
      "Batch=1/12 Epoch=1/1 Loss=10.560806274414062\n",
      "Batch=2/12 Epoch=1/1 Loss=10.48877239227295\n",
      "Batch=3/12 Epoch=1/1 Loss=10.6073579788208\n",
      "Batch=4/12 Epoch=1/1 Loss=10.22824478149414\n",
      "Batch=5/12 Epoch=1/1 Loss=10.144061088562012\n",
      "Batch=6/12 Epoch=1/1 Loss=10.688617706298828\n",
      "Batch=7/12 Epoch=1/1 Loss=10.170238494873047\n",
      "Batch=8/12 Epoch=1/1 Loss=10.611958503723145\n",
      "Batch=9/12 Epoch=1/1 Loss=10.39381217956543\n",
      "Batch=10/12 Epoch=1/1 Loss=9.924991607666016\n",
      "Batch=11/12 Epoch=1/1 Loss=10.43026351928711\n",
      "Batch=12/12 Epoch=1/1 Loss=10.256317138671875\n",
      "NNTile training time: 44.41726088523865 seconds\n",
      "NNTile training throughput tokens/sec: 1106.596827908739\n",
      "NNTile loss on the last batch: 10.256317138671875\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/roberta_training.py --pretrained=local --checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "                                                        --config-path=\"../wrappers/python/examples/roberta_config.json\" \\\n",
    "                                                        --save-checkpoint-path=\".model/nntile_further_checkpoint.pt\" \\\n",
    "                                                        --optimizer=\"adam\" --lr=1e-5 --dtype=fp32_fast_fp16 \\\n",
    "                                                        --nepochs=3 --batch-size=8 --minibatch-size=4 \\\n",
    "                                                        --dataset-file=\"tinystories/train.bin\" --seq-len=512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a1c25-cc29-4ff0-bbb6-d84b0f414ed6",
   "metadata": {},
   "source": [
    "## 3. Continue training of a model loaded from the Hugging Face framework.\n",
    "\n",
    "The NNTile framework currently supports continued training of a model loaded from a remote source, as shown in our example from the Hugging Face framework library.\n",
    "The weights of the loaded model are passed to the model implemented in NNTile.\n",
    "To run such a scenario, the ```pretrained``` parameter must be set to ```remote```.\n",
    "The ```config-path``` and ```checkpoint-path``` parameters are no longer required, as the model configuration and layer weights will be obtained from the loaded model.\n",
    "Training can be continued using any data type and on any compute nodes that support the selected data type.\n",
    "In the example below, we switch to the ```bf16``` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3239307d-d4f1-47b4-9436-f8b03bebffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='FacebookAI/roberta-base', pretrained='remote', checkpoint_path='', config_path='', save_checkpoint_path='.model/nntile_remote_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=8, minibatch_size=4, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=1e-13, nepochs=3, label_mask_token=3, n_masked_tokens_per_seq=3, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "model.safetensors: 100%|██████████████████████| 499M/499M [00:03<00:00, 140MB/s]\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.22286415100097656 seconds\n",
      "BertConfigNNTile(vocab_size=50265, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, num_attention_heads=12, n_head_tile=12, activation_function='gelutanh', dtype='bf16', flashattention=False, layer_norm_epsilon=1e-05, max_position_embeddings=514, num_hidden_layers=12, redux=True, hidden_dropout_prob=0, attention_probs_dropout_prob=0, classifier_dropout=0, type_vocab_size=1, position_embedding_type='absolute', name='bert')\n",
      "Converting PyTorch model to NNTile requires 1.9103257656097412 seconds\n",
      "From PyTorch loader to NNTile batches in 0.005426883697509766 seconds\n",
      "Params+grads (GB): 0.608\n",
      "Activations  (GB): 1.482\n",
      "Optimizer    (GB): 0.608\n",
      "Persistent   (GB): 2.699\n",
      "Temporaries  (GB): 1.212\n",
      "Batch=1/12 Epoch=1/1 Loss=3.8658854961395264\n",
      "Batch=2/12 Epoch=1/1 Loss=1.7037353515625\n",
      "Batch=3/12 Epoch=1/1 Loss=1.8027344942092896\n",
      "Batch=4/12 Epoch=1/1 Loss=2.528646230697632\n",
      "Batch=5/12 Epoch=1/1 Loss=1.5638021230697632\n",
      "Batch=6/12 Epoch=1/1 Loss=3.520833730697632\n",
      "Batch=7/12 Epoch=1/1 Loss=2.478515625\n",
      "Batch=8/12 Epoch=1/1 Loss=1.3849284648895264\n",
      "Batch=9/12 Epoch=1/1 Loss=2.06640625\n",
      "Batch=10/12 Epoch=1/1 Loss=1.8046875\n",
      "Batch=11/12 Epoch=1/1 Loss=2.921875\n",
      "Batch=12/12 Epoch=1/1 Loss=2.7220051288604736\n",
      "NNTile training time: 2.4262964725494385 seconds\n",
      "NNTile training throughput tokens/sec: 20258.03546932308\n",
      "NNTile loss on the last batch: 2.7220051288604736\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/roberta_training.py --restrict=\"cuda\" --pretrained=remote \\\n",
    "                                                        --save-checkpoint-path=\".model/nntile_remote_checkpoint.pt\" \\\n",
    "                                                        --optimizer=\"adam\" --lr=1e-13 --dtype=bf16 --nepochs=3 \\\n",
    "                                                        --batch-size=8 --minibatch-size=4 --seq-len=512  \\\n",
    "                                                        --dataset-file=\"tinystories/train.bin\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-nntile]",
   "language": "python",
   "name": "conda-env-.mlspace-nntile-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
