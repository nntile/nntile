{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7c9e3f-1c2f-44e3-a887-025863461efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of execution environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"1\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"GPT2_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled\n",
    "\n",
    "# NNTile-related\n",
    "os.environ[\"NNTILE_LOGGER\"] = \"1\" # Enable logger\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_ADDR\"] = \"127.0.0.1\" # Logger will be running on the localhost\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_PORT\"] = \"5001\" # Port for logger server\n",
    "os.environ[\"NNTILE_LOGGER_CLIENT_PORT\"] = \"6006\" # Port for client web interface of the logger\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_DIR\"] = str(Path.cwd() / \"logs\") # Directory to store logs on the logger server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02eec0e3-bd27-4caa-8081-202cd423edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 07:29:02.760547: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-13 07:29:02.790321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Launch logger if needed\n",
    "if os.getenv(\"NNTILE_LOGGER\", \"0\") != \"0\":\n",
    "    logger_env = os.environ.copy()\n",
    "    logger_env.update({\n",
    "        \"LOG_DIR\": os.getenv(\"NNTILE_LOGGER_SERVER_DIR\"),\n",
    "        \"SPLIT_HOURS\": \"720\",\n",
    "        \"CLEAR_LOGS\": \"0\",\n",
    "        \"SERVER_PORT\": os.getenv(\"NNTILE_LOGGER_SERVER_PORT\")\n",
    "    })\n",
    "    logger_proc = subprocess.Popen([\"python\", nntile_dir / \"logger\" / \"server.py\"], env=logger_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b152f53e-78c7-4900-b8d1-72352189b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|███████████████| 1.60k/1.60k [00:00<00:00, 6.33MB/s]\n",
      "tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 6.78MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:00<00:00, 3.17MB/s]\n",
      "added_tokens.json: 100%|██████████████████████| 51.0/51.0 [00:00<00:00, 162kB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 547/547 [00:00<00:00, 1.63MB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "!python ../wrappers/python/examples/causal_lm_data_preparation.py --seq-len=1024 --batch-size=1024 --dataset-select=25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bd4e10-4626-4140-a989-aced3dfac91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/llama_1.3b_config.json', save_checkpoint_path='.model', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=1024, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=True, logger_server_addr='127.0.0.1', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaCasualForLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"fp32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"flashattention\": false,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"redux\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "Trying to connect to 127.0.0.1:5001\n",
      "WORKER COUNT: 2\n",
      "BUS COUNT: 2\n",
      "MEMNODES COUNT: 2\n",
      "IS initialized : 1\n",
      "StarPU + NNTile + MPI init in 0.728299617767334 seconds\n",
      "LlamaConfigNNTile(vocab_size=32002, vocab_embed_dim_tile=2048, hidden_size=2048, hidden_size_tile=2048, max_position_embeddings=4096, intermediate_size=5504, intermediate_size_tile=5504, n_attention_head=16, n_head_tile=16, num_key_value_heads=16, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=24, mlp_bias=False, flash_attention=False)\n",
      "Converting PyTorch model to NNTile requires 20.221343517303467 seconds\n",
      "From PyTorch loader to NNTile batches in 0.06780529022216797 seconds\n",
      "Params+grads (GB): 5.012\n",
      "Activations  (GB): 26.227\n",
      "Optimizer    (GB): 5.012\n",
      "Persistent   (GB): 36.251\n",
      "Temporaries  (GB): 33.152\n",
      "Batch=1/5 Epoch=1/1 Loss=10.25\n",
      "Batch=2/5 Epoch=1/1 Loss=10.1875\n",
      "Batch=3/5 Epoch=1/1 Loss=10.5\n",
      "Batch=4/5 Epoch=1/1 Loss=8.75\n",
      "Batch=5/5 Epoch=1/1 Loss=8.5\n",
      "NNTile training time: 1042.6764860153198 seconds\n",
      "NNTile training throughput tokens/sec: 5028.290241814245\n",
      "NNTile performance (model flops): 41.64795233273723 Tflops/s\n",
      "NNTile loss on the last batch: 8.5\n",
      "LOGGER SHUTDOWN\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained LLaMa model on TinyStories\n",
    "# If logger server is launched, then TensorBoard results can be accessed at localhost:6006\n",
    "!python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/llama_1.3b_config.json\" \\\n",
    "    --optimizer=\"adam\" --lr=1e-4 --dtype=bf16 --nepochs=1 --batch-size=1024 --minibatch-size=8 \\\n",
    "    --dataset-file=\"tinystories/train.bin\" --logger --logger-server-addr=127.0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af2a7b-0538-4956-a776-25febb4447f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
