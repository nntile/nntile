{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7c9e3f-1c2f-44e3-a887-025863461efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of execution environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"1\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"GPT2_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled\n",
    "\n",
    "# NNTile-related\n",
    "os.environ[\"NNTILE_LOGGER\"] = \"1\" # Enable logger\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_ADDR\"] = \"127.0.0.1\" # Logger will be running on the localhost\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_PORT\"] = \"5001\" # Port for logger server\n",
    "os.environ[\"NNTILE_LOGGER_CLIENT_PORT\"] = \"6006\" # Port for client web interface of the logger\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_DIR\"] = str(Path.cwd() / \"logs\") # Directory to store logs on the logger server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eec0e3-bd27-4caa-8081-202cd423edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch logger if needed\n",
    "if os.getenv(\"NNTILE_LOGGER\", \"0\") != \"0\":\n",
    "    logger_env = os.environ.copy()\n",
    "    logger_env.update({\n",
    "        \"LOG_DIR\": os.getenv(\"NNTILE_LOGGER_SERVER_DIR\"),\n",
    "        \"SPLIT_HOURS\": \"720\",\n",
    "        \"CLEAR_LOGS\": \"0\",\n",
    "        \"SERVER_PORT\": os.getenv(\"NNTILE_LOGGER_SERVER_PORT\")\n",
    "    })\n",
    "    logger_proc = subprocess.Popen([\"python\", nntile_dir / \"logger\" / \"server.py\"], env=logger_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152f53e-78c7-4900-b8d1-72352189b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "!python ../wrappers/python/examples/causal_lm_data_preparation.py --seq-len=1024 --batch-size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd4e10-4626-4140-a989-aced3dfac91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch an external python process to finetune a pretrained LLaMa model on TinyStories\n",
    "# If logger server is launched, then TensorBoard results can be accessed at localhost:6006\n",
    "!python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=./wrappers/python/examples/llama_1.3b_config.json \\\n",
    "    --optimizer=\"adam\" --lr=1e-4 --nepochs=1 --batch-size=2 --minibatch-size=1 \\\n",
    "    --dataset-file=\"tinystories/train.bin\" --nntile-logger --nntile-logger-server-addr=127.0.0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
