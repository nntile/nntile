{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7c9e3f-1c2f-44e3-a887-025863461efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of execution environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"1\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"1\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"Llama_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled\n",
    "\n",
    "# NNTile-related\n",
    "os.environ[\"NNTILE_LOGGER\"] = \"0\" # Enable logger\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_ADDR\"] = \"127.0.0.1\" # Logger will be running on the localhost\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_PORT\"] = \"5001\" # Port for logger server\n",
    "os.environ[\"NNTILE_LOGGER_CLIENT_PORT\"] = \"6006\" # Port for client web interface of the logger\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_DIR\"] = str(Path.cwd() / \"logs\") # Directory to store logs on the logger server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02eec0e3-bd27-4caa-8081-202cd423edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch logger if needed\n",
    "if os.getenv(\"NNTILE_LOGGER\", \"0\") != \"0\":\n",
    "    logger_env = os.environ.copy()\n",
    "    logger_env.update({\n",
    "        \"LOG_DIR\": os.getenv(\"NNTILE_LOGGER_SERVER_DIR\"),\n",
    "        \"SPLIT_HOURS\": \"720\",\n",
    "        \"CLEAR_LOGS\": \"0\",\n",
    "        \"SERVER_PORT\": os.getenv(\"NNTILE_LOGGER_SERVER_PORT\")\n",
    "    })\n",
    "    logger_proc = subprocess.Popen([\"python\", nntile_dir / \"logger\" / \"server.py\"], env=logger_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b152f53e-78c7-4900-b8d1-72352189b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "!python ../wrappers/python/examples/causal_lm_data_preparation.py --seq-len=1024 --batch-size=64 --dataset-select=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bd4e10-4626-4140-a989-aced3dfac91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/llama_1.3b_config.json', save_checkpoint_path='.model', optimizer='adamw', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=64, minibatch_size=8, minibatch_size_tile=8, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaCasualForLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"fp32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"flashattention\": false,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"redux\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 7.6648108959198 seconds\n",
      "LlamaConfigNNTile(vocab_size=32002, vocab_embed_dim_tile=2048, hidden_size=2048, hidden_size_tile=2048, max_position_embeddings=4096, intermediate_size=5504, intermediate_size_tile=5504, n_attention_head=16, n_head_tile=16, num_key_value_heads=16, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=24, mlp_bias=False, flash_attention=False, name='llama')\n",
      "Converting PyTorch model to NNTile requires 17.046820878982544 seconds\n",
      "From PyTorch loader to NNTile batches in 0.0007214546203613281 seconds\n",
      "Params+grads (GB): 5.012\n",
      "Activations  (GB): 26.227\n",
      "Optimizer    (GB): 5.012\n",
      "Persistent   (GB): 36.251\n",
      "Temporaries  (GB): 33.152\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "Batch=1/1 Epoch=1/1 Loss=10.803102493286133\n",
      "NNTile training time: 5.948832035064697 seconds\n",
      "NNTile training throughput tokens/sec: 11016.616306143069\n",
      "NNTile performance (model flops): 91.24761871756166 Tflops/s\n",
      "NNTile loss on the last batch: 10.803102493286133\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained LLaMa model on TinyStories\n",
    "# If logger server is launched, then TensorBoard results can be accessed at localhost:6006\n",
    "!PYTHONPATH=${HOME}/mikhalev/nntile/build-1.4.7/wrappers/python LD_LIBRARY_PATH=${HOME}/mikhalev/install/starpu-1.4.7/lib python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/llama_1.3b_config.json\" \\\n",
    "    --optimizer=\"adamw\" --lr=1e-4 --dtype=bf16 --nepochs=1 --batch-size=64 --minibatch-size=8 --minibatch-size-tile=8 \\\n",
    "    --dataset-file=\"tinystories/train.bin\" `#--logger --logger-server-addr=127.0.0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79af2a7b-0538-4956-a776-25febb4447f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/llama_1.3b_config.json', save_checkpoint_path='.model', optimizer='adamw', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=1024, minibatch_size=64, minibatch_size_tile=8, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaCasualForLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"fp32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"flashattention\": false,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"redux\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 2.1801581382751465 seconds\n",
      "LlamaConfigNNTile(vocab_size=32002, vocab_embed_dim_tile=2048, hidden_size=2048, hidden_size_tile=2048, max_position_embeddings=4096, intermediate_size=5504, intermediate_size_tile=5504, n_attention_head=16, n_head_tile=16, num_key_value_heads=16, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=24, mlp_bias=False, flash_attention=False, name='llama')\n",
      "Converting PyTorch model to NNTile requires 13.633651971817017 seconds\n",
      "From PyTorch loader to NNTile batches in 0.058620452880859375 seconds\n",
      "Params+grads (GB): 5.012\n",
      "Activations  (GB): 209.813\n",
      "Optimizer    (GB): 5.012\n",
      "Persistent   (GB): 219.838\n",
      "Temporaries  (GB): 265.051\n",
      "Batch=1/5 Epoch=1/1 Loss=10.77939510345459\n",
      "Batch=2/5 Epoch=1/1 Loss=9.364055633544922\n",
      "Batch=3/5 Epoch=1/1 Loss=10.95502758026123\n",
      "Batch=4/5 Epoch=1/1 Loss=10.202980041503906\n",
      "Batch=5/5 Epoch=1/1 Loss=9.361103057861328\n",
      "NNTile training time: 403.22593784332275 seconds\n",
      "NNTile training throughput tokens/sec: 13002.338163169381\n",
      "NNTile performance (model flops): 107.69480956581053 Tflops/s\n",
      "NNTile loss on the last batch: 9.361103057861328\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained LLaMa model on TinyStories\n",
    "# If logger server is launched, then TensorBoard results can be accessed at localhost:6006\n",
    "!PYTHONPATH=${HOME}/mikhalev/nntile/build-1.4.7/wrappers/python LD_LIBRARY_PATH=${HOME}/mikhalev/install/starpu-1.4.7/lib python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/llama_1.3b_config.json\" \\\n",
    "    --optimizer=\"adamw\" --lr=1e-4 --dtype=bf16 --nepochs=1 --batch-size=1024 --minibatch-size=64 --minibatch-size-tile=8 \\\n",
    "    --dataset-file=\"tinystories/train.bin\" `#--logger --logger-server-addr=127.0.0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2321c2-c7c0-49b2-9190-c67a1eedf80d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-mikhalev]",
   "language": "python",
   "name": "conda-env-.mlspace-mikhalev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
