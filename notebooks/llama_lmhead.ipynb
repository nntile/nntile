{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e9abb8-4ac9-47e9-bfdb-59e14215d1bc",
   "metadata": {},
   "source": [
    "### 1. Environment variable setting block:\n",
    "\n",
    "The following block is required to set environment variables that are read during the execution of the program code. \n",
    "\n",
    "User can change these environment variables between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7c9e3f-1c2f-44e3-a887-025863461efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of experimental environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"1\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"0\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_BUS_STATS\"] = \"1\" # This enables logging of bus usage, prined at the end of execution\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"Llama_LMHead_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846f078-0029-461c-9081-f923190b7e29",
   "metadata": {},
   "source": [
    "### 2. Data Preparation Block: \n",
    "\n",
    "This block uses the interpreted file \"causal_lm_data_preparation.py\". This Python script supports the following arguments when run:\n",
    "- hf-dataset, (default=`\"roneneldan/TinyStories\"`): The name of the dataset to be processed and prepared for use in the training process. By default, the \"TinyStories\" dataset from the Huggingface infrastructure is specified,\n",
    "- dataset-path, (default=`\".data\"`): path to the directory where previously downloaded datasets from remote sources are saved, making it easy to access for the future use,\n",
    "- dataset-select, (`int`, default=`100`): specifies the number of records from the original dataset that fall into the training set,\n",
    "- hf-tokenizer, (`str`, default=`\"kimihailv/llama-1.3b\"`): specifies the repository from the Huggingface infrastructure used as a tokenizer,\n",
    "- tokenizer-path, (`str`, default=`\".model\"`): path to the directory where previously downloaded tokenizers are saved,\n",
    "- seq-len, (`int`, default=`1024`): length of the input token sequence for the training process,\n",
    "- batch-size, (`int`, default=`1`): batch size for the training process, then is the number of input data sentences between which the loss function optimizer step is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b152f53e-78c7-4900-b8d1-72352189b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|█| 2119719/2119719 [00:07<00:00, 287866.41 examples\n",
      "Generating validation split: 100%|█| 21990/21990 [00:00<00:00, 300242.66 example\n",
      "tokenizer_config.json: 1.60kB [00:00, 5.64MB/s]\n",
      "tokenizer.model: 100%|████████████████████████| 500k/500k [00:01<00:00, 477kB/s]\n",
      "tokenizer.json: 1.84MB [00:00, 19.0MB/s]\n",
      "added_tokens.json: 100%|██████████████████████| 51.0/51.0 [00:00<00:00, 145kB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 547/547 [00:00<00:00, 1.89MB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "!python ../wrappers/python/examples/causal_lm_data_preparation.py --seq-len=512 --batch-size=256 --dataset-select=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637d8fc-db43-44f2-a472-b2085dccfd7f",
   "metadata": {},
   "source": [
    "### 3. Example Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8519c7-2183-484c-b9aa-c1b4b3f0814a",
   "metadata": {},
   "source": [
    "Below we show an example of utilizing the Llama model, implemented using the NNTile framework. We explore the following scenarios:\n",
    "\n",
    "- **Training the model from a random initial state and saving it to a checkpoint.**\n",
    "- **Loading the model weights from a checkpoint and continuing training with a different data type.**\n",
    "- **Loading pretrained model from remote source and continuing training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143bb3c-27e1-4706-ab0d-e3f6506d81f7",
   "metadata": {},
   "source": [
    "For training and continuing retraining scenarios, the interpreted file \"gpt_neox_training.py\" is used. This Python script supports the following arguments when running:\n",
    "\n",
    "- remote_model_name, (`str`, default=`\"kimihailv/llama-1.3b\"`): This parameter specifies the name of the GPT-NeoX based model that resides within the HuggingFace infrastructure and will be utilized to initialize the configuration and the intial state of the NNTile model.\n",
    "\n",
    "- pretrained, (choices=`[\"local\", \"remote\"]`, default=`\"local\"`): This flag indicates the location of the pretrained model, with the `local` option requiring a configuration path (`config-path`) to start training from a randomly initialized state unless the checkpoint (`checkpoint-path`) is provided, in which case training continues from the last saved checkpoint state.\n",
    "\n",
    "- checkpoint-path, (`str`, default=`\"\"`): This refers to the file path where a saved checkpoint can be found, allowing for the resumption of training from a specific point if available.\n",
    "\n",
    "- config-path, (`str`, default=`\"\"`): This denotes the path to the configuration .json file that must be provided in the current version if the `pretrained` parameter is set to `\"local\"`.\n",
    "\n",
    "- save-checkpoint-path, (`str`, default=`\".model\"`): This parameter specifies the directory path where intermediate checkpoints will be saved during the training process for future reference.\n",
    "\n",
    "- optimizer, (choices=`[\"sgd\", \"adam\", \"adamw\"]`, default=`\"adam\"`): This defines the type of optimizer that will be employed during the training process; the current version of NNTile supports three distinct optimization methods.\n",
    "\n",
    "- model-path, (`str`, default=`\".model\"`): This indicates the directory path where previously loaded remote models are stored, facilitating easy access for further use.\n",
    "\n",
    "- seq-len, (`int`, default=`1024`): length of the input token sequence for training.\n",
    "\n",
    "- seq-len-tile, (`int`, default=`1024`): split size of sequence length into tiles\n",
    "\n",
    "- batch-size, (`int`, default=`1`): batch size for the training process, which specifies the number of sentences processed by seq-len tokens between steps of the loss function optimizer.\n",
    "\n",
    "- minibatch-size, (`int`, default=`-1`): batch size for which memory is allocated during training. The entire batch is divided into whole minibatches. All minibatches from one batch are fed through the model one by one to accumulate parameter gradients.\n",
    "\n",
    "- minibatch-size-tile, (`int`, default=`-1`): batch size that goes to the CPU or GPU for calculations. Each minibatch must be divisible by an integer number of minibatch tiles.\n",
    "\n",
    "- hidden-size-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the \"hidden size\" dimension is divided (also known as \"embedding size\") – the size of the multidimensional space into which incoming tokens are mapped. Only \"piecewise\" tensors of size hidden-size-tile along the corresponding axis are processed on the CPU and GPU.\n",
    "\n",
    "- intermediate-size-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the \"intermediate size\" dimension is divided. Only \"piecewise\" tensors of size intermediate-size-tile along the corresponding axis are processed on the CPU and GPU.\n",
    "\n",
    "- n-head-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the number of heads of the transformer layer is divided. Only “piecewise” tensors with a size of n-head-tile along the corresponding axis are processed by the CPU and GPU.\n",
    "\n",
    "- dtype, (choices=`[\"fp32\", \"fp64\", \"tf32\", \"bf16\", \"fp32_fast_fp16\", \"fp32_fast_bf16\"]`, default=`\"fp32\"`): This parameter outlines the various data types supported by NNTile, allowing users the flexibility to choose based on their model requirements.\n",
    "\n",
    "- restrict, (choices=`[\"cpu\", \"cuda\", None]`, default=`None`): This option allows users to specify restrictions on the computational resources utilized during training; selecting `\"cpu\"` restricts training to CPU-only cores, `\"cuda\"` limits it to GPU cores, while setting it to None allows for training across all available cores.\n",
    "\n",
    "- use-redux, (action=`\"store_true\"`): a boolean flag that, when used in the argument string, allows for the computation of dependent tasks simultaneously, with the subsequent reduction of the results into a single tensor.\n",
    "\n",
    "- dataset-path, (default=`\".data\"`): path to the directory where previously prepared datasets are saved.\n",
    "\n",
    "- dataset-file, (default=`\"\"`): path (relative to dataset-path) to the .bin file that is created in the block of data preparation for training.\n",
    "\n",
    "- lr, (`float`, default=`1e-4`): step length for the optimization algorithm.\n",
    "\n",
    "- nepochs, (`int`, default=`1`): number of full passes through the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7fec0-d2f8-4e6b-914f-80234692a029",
   "metadata": {},
   "source": [
    "#### 3.1. Training from the random initial state and saving into checkpoint.\n",
    "\n",
    "This requires option `pretrained` set to `local` and `config-path` to point on previously created `.json` configuration file.\n",
    "\n",
    "In this example, we start training in the fp32 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bd4e10-4626-4140-a989-aced3dfac91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 15:55:55.632791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/llama_1.3b_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaCasualForLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"fp32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"flashattention\": false,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"redux\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.3475184440612793 seconds\n",
      "LlamaConfigNNTile(vocab_size=32002, vocab_embed_dim_tile=2048, hidden_size=2048, hidden_size_tile=2048, max_position_embeddings=4096, intermediate_size=5504, intermediate_size_tile=5504, n_attention_head=16, n_head_tile=16, num_key_value_heads=16, activation_function='silu', redux=False, dtype='fp32', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=24, mlp_bias=False, flash_attention=False, name='llama')\n",
      "Converting PyTorch model to NNTile requires 10.41123914718628 seconds\n",
      "From PyTorch loader to NNTile batches in 0.0319063663482666 seconds\n",
      "Params+grads (GB): 10.024\n",
      "Activations  (GB): 26.227\n",
      "Optimizer    (GB): 10.024\n",
      "Persistent   (GB): 46.275\n",
      "Temporaries  (GB): 27.134\n",
      "Batch=1/8 Epoch=1/1 Loss=0.3354339897632599\n",
      "Batch=2/8 Epoch=1/1 Loss=0.2970781624317169\n",
      "Batch=3/8 Epoch=1/1 Loss=0.33374398946762085\n",
      "Batch=4/8 Epoch=1/1 Loss=0.3281177580356598\n",
      "Batch=5/8 Epoch=1/1 Loss=0.29986387491226196\n",
      "Batch=6/8 Epoch=1/1 Loss=0.2838857173919678\n",
      "Batch=7/8 Epoch=1/1 Loss=0.27122732996940613\n",
      "Batch=8/8 Epoch=1/1 Loss=0.2612197697162628\n",
      "NNTile training time: 2792.04118847847 seconds\n",
      "NNTile training throughput tokens/sec: 375.558929548394\n",
      "NNTile performance (model flops): 2.9972368614545037 Tflops/s\n",
      "NNTile loss on the last batch: 0.2612197697162628\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t8071.7466 GB\t2824.5676 MB/s\t(transfers : 284742 - avg 29.0279 MB)\n",
      "\tCUDA 0 -> NUMA 0\t7545.5220 GB\t2640.4244 MB/s\t(transfers : 192363 - avg 40.1668 MB)\n",
      "Total transfers: 15617.2686 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained LLaMa model on TinyStories\n",
    "!python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/llama_1.3b_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --seq-len=512 --lr=1e-4 --dtype=fp32 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953c6ec-b527-4f10-88f8-c203bfce9427",
   "metadata": {},
   "source": [
    "#### 3.2. Resume training from the local checkpoint.\n",
    "\n",
    "This requires option `pretrained` again to be set to `local`, `config-path` to point on previously created `.json` configuration file, and also `checkpoint-path` to point on the pre-existing checkpoint file in the PyTorch format.\n",
    "\n",
    "Training process can be resumed using a different data type and on a different set of compute nodes. For example, here we switch to the bf16 type and restrict to using only GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e83130-500f-49df-ad59-edfb60c3a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 16:46:03.845496: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='.model/nntile_checkpoint.pt', config_path='../wrappers/python/examples/llama_1.3b_config.json', save_checkpoint_path='.model/nntile_further_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaCasualForLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"fp32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"flashattention\": false,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"redux\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.35689830780029297 seconds\n",
      "LlamaConfigNNTile(vocab_size=32002, vocab_embed_dim_tile=2048, hidden_size=2048, hidden_size_tile=2048, max_position_embeddings=4096, intermediate_size=5504, intermediate_size_tile=5504, n_attention_head=16, n_head_tile=16, num_key_value_heads=16, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=24, mlp_bias=False, flash_attention=False, name='llama')\n",
      "Converting PyTorch model to NNTile requires 17.83967638015747 seconds\n",
      "From PyTorch loader to NNTile batches in 0.036458730697631836 seconds\n",
      "Params+grads (GB): 5.012\n",
      "Activations  (GB): 13.113\n",
      "Optimizer    (GB): 5.012\n",
      "Persistent   (GB): 23.138\n",
      "Temporaries  (GB): 13.570\n",
      "Batch=1/8 Epoch=1/1 Loss=0.2469763308763504\n",
      "Batch=2/8 Epoch=1/1 Loss=0.28404250741004944\n",
      "Batch=3/8 Epoch=1/1 Loss=0.24799437820911407\n",
      "Batch=4/8 Epoch=1/1 Loss=0.23006856441497803\n",
      "Batch=5/8 Epoch=1/1 Loss=0.22180280089378357\n",
      "Batch=6/8 Epoch=1/1 Loss=0.21400131285190582\n",
      "Batch=7/8 Epoch=1/1 Loss=0.20519916713237762\n",
      "Batch=8/8 Epoch=1/1 Loss=0.2059435248374939\n",
      "NNTile training time: 1973.5199048519135 seconds\n",
      "NNTile training throughput tokens/sec: 531.3227383326958\n",
      "NNTile performance (model flops): 4.240346777467567 Tflops/s\n",
      "NNTile loss on the last batch: 0.2059435248374939\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t2527.2781 GB\t1231.2750 MB/s\t(transfers : 183642 - avg 14.0923 MB)\n",
      "\tCUDA 0 -> NUMA 0\t2691.2605 GB\t1311.1662 MB/s\t(transfers : 145221 - avg 18.9769 MB)\n",
      "Total transfers: 5218.5386 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained NNTile llama model on TinyStories\n",
    "!python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "    --config-path=\"../wrappers/python/examples/llama_1.3b_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_further_checkpoint.pt\" --optimizer=\"adam\" --seq-len=512 --lr=1e-4 --dtype=bf16 \\\n",
    "    --restrict=\"cuda\" --nepochs=1 --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0edc3-6d79-49bc-af3b-6aee0a5735de",
   "metadata": {},
   "source": [
    "#### 3.3. Training from remote and saving into checkpoint.\n",
    "\n",
    "Our framework currently supports the continuation of model training obtained from a remote source, as we show here with the Hugging Face library. The weights from the loaded model are transferred into the model implemented in NNTile. Consequently, training can be further advanced using any data type and across any set of computing nodes that accommodate the selected data type.\n",
    "\n",
    "This requires option `pretrained` to be set to `remote`. Options `config-path` and `checkpoint-path` are no longer needed since model config is obtained from the remote model as well as layers' weights. Training can be resumed using any data type and on any set of compute nodes that support the selected data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ad38c0-560f-40f1-ab59-e3f59207a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 17:31:26.153856: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='remote', checkpoint_path='', config_path='', save_checkpoint_path='.model/nntile_remote_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=512, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "config.json: 100%|█████████████████████████████| 673/673 [00:00<00:00, 2.67MB/s]\n",
      "model.safetensors.index.json: 18.0kB [00:00, 31.4MB/s]\n",
      "Fetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|              | 0.00/397M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 660k/4.98G [00:03<6:21:57, 217kB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 62.2M/397M [00:04<00:24, 13.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|   | 2.23M/4.98G [00:05<2:50:37, 487kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 69.3M/4.98G [00:05<04:06, 19.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 203M/4.98G [00:05<01:13, 65.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 270M/4.98G [00:06<00:50, 93.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 340M/4.98G [00:06<00:43, 106MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  49%|██▉   | 196M/397M [00:08<00:07, 25.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 407M/4.98G [00:08<01:18, 58.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▋     | 541M/4.98G [00:08<00:41, 106MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▋     | 608M/4.98G [00:09<00:34, 126MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▊     | 675M/4.98G [00:09<00:38, 112MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▉     | 809M/4.98G [00:10<00:23, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|█     | 876M/4.98G [00:10<00:20, 197MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|█▏    | 944M/4.98G [00:10<00:21, 188MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 1.01G/4.98G [00:10<00:17, 230MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  66%|███▉  | 263M/397M [00:11<00:05, 25.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 1.14G/4.98G [00:11<00:14, 265MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 1.21G/4.98G [00:11<00:19, 192MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 1.28G/4.98G [00:12<00:19, 191MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 1.35G/4.98G [00:12<00:21, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 1.41G/4.98G [00:12<00:16, 212MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▍   | 1.48G/4.98G [00:13<00:19, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 1.55G/4.98G [00:13<00:16, 206MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 1.61G/4.98G [00:14<00:20, 167MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  83%|████▉ | 330M/397M [00:14<00:02, 23.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 1.68G/4.98G [00:15<00:24, 133MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 1.82G/4.98G [00:15<00:19, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 1.95G/4.98G [00:15<00:12, 235MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 2.09G/4.98G [00:15<00:09, 321MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 2.15G/4.98G [00:16<00:08, 337MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▏  | 2.22G/4.98G [00:16<00:13, 200MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 2.35G/4.98G [00:17<00:08, 295MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 2.42G/4.98G [00:17<00:11, 231MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▍  | 2.49G/4.98G [00:18<00:15, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 2.62G/4.98G [00:18<00:09, 243MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▊  | 2.75G/4.98G [00:18<00:06, 335MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 2.89G/4.98G [00:18<00:05, 405MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▉  | 2.98G/4.98G [00:19<00:05, 401MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 3.11G/4.98G [00:19<00:03, 507MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors: 100%|██████| 397M/397M [00:19<00:00, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  65%|███▎ | 3.25G/4.98G [00:19<00:02, 621MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 3.38G/4.98G [00:19<00:02, 721MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▌ | 3.51G/4.98G [00:19<00:02, 497MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 3.65G/4.98G [00:20<00:02, 469MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 3.78G/4.98G [00:20<00:02, 491MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 3.92G/4.98G [00:20<00:02, 463MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▉ | 3.98G/4.98G [00:20<00:02, 473MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 4.05G/4.98G [00:21<00:01, 468MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 4.12G/4.98G [00:21<00:01, 499MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 4.18G/4.98G [00:21<00:02, 343MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 4.32G/4.98G [00:21<00:01, 464MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 4.45G/4.98G [00:21<00:00, 593MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 4.58G/4.98G [00:22<00:01, 387MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▋| 4.72G/4.98G [00:22<00:00, 433MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 4.85G/4.98G [00:22<00:00, 470MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.98G/4.98G [00:23<00:00, 217MB/s]\u001b[A\n",
      "Fetching 2 files: 100%|███████████████████████████| 2/2 [00:23<00:00, 11.83s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 21.32it/s]\n",
      "generation_config.json: 100%|███████████████████| 132/132 [00:00<00:00, 615kB/s]\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.36271023750305176 seconds\n",
      "LlamaConfigNNTile(vocab_size=32002, vocab_embed_dim_tile=2048, hidden_size=2048, hidden_size_tile=2048, max_position_embeddings=4096, intermediate_size=5504, intermediate_size_tile=5504, n_attention_head=16, n_head_tile=16, num_key_value_heads=16, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=24, mlp_bias=False, flash_attention=False, name='llama')\n",
      "Converting PyTorch model to NNTile requires 12.856295108795166 seconds\n",
      "From PyTorch loader to NNTile batches in 0.04572796821594238 seconds\n",
      "Params+grads (GB): 5.012\n",
      "Activations  (GB): 13.113\n",
      "Optimizer    (GB): 5.012\n",
      "Persistent   (GB): 23.138\n",
      "Temporaries  (GB): 13.570\n",
      "Batch=1/8 Epoch=1/1 Loss=0.06210063397884369\n",
      "Batch=2/8 Epoch=1/1 Loss=0.22045709192752838\n",
      "Batch=3/8 Epoch=1/1 Loss=0.15293794870376587\n",
      "Batch=4/8 Epoch=1/1 Loss=0.06821388006210327\n",
      "Batch=5/8 Epoch=1/1 Loss=0.06651700288057327\n",
      "Batch=6/8 Epoch=1/1 Loss=0.0655069649219513\n",
      "Batch=7/8 Epoch=1/1 Loss=0.05244109034538269\n",
      "Batch=8/8 Epoch=1/1 Loss=0.05553815886378288\n",
      "NNTile training time: 2051.900369167328 seconds\n",
      "NNTile training throughput tokens/sec: 511.02676121917057\n",
      "NNTile performance (model flops): 4.07836993187094 Tflops/s\n",
      "NNTile loss on the last batch: 0.05553815886378288\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t2527.3132 GB\t1187.3747 MB/s\t(transfers : 183511 - avg 14.1025 MB)\n",
      "\tCUDA 0 -> NUMA 0\t2691.1423 GB\t1264.3443 MB/s\t(transfers : 145218 - avg 18.9765 MB)\n",
      "Total transfers: 5218.4556 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a downloaded from remote source pretrained gpt_neo model on TinyStories\n",
    "!python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=remote --save-checkpoint-path=\".model/nntile_remote_checkpoint.pt\"\\\n",
    "    --optimizer=\"adam\" --seq-len=512 --lr=1e-4 --dtype=bf16 --nepochs=1 --batch-size=256 --minibatch-size=8 \\\n",
    "    --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc25bd-e8b8-49a8-987a-4a6299c98819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da225c9-a5d3-4d83-8198-02249b616f92",
   "metadata": {},
   "source": [
    "### 4. Inference process.\n",
    "\n",
    "In the current version of the Llama scenario, the NNTile framework model is created from a (pre-)loaded pre-trained LLama model from the Huggingface library. The model layer weights are passed to the corresponding NNTile model layers, and then the inference process is performed solely by NNTile, without any involvement of third-party models and mechanisms. To perform the inference, we use another program file - \"llama_generate.py\". The program code supports the following arguments when running:\n",
    "\n",
    "- cache_dir, (`str`, default=\"cache_hf\"): path to the directory where previously loaded models from a remote source are saved,\n",
    "- max-seq-len, (`int`, default=1024): maximum length of the input token sequence,\n",
    "- remote-model-name, (`str`, default=`\"kimihailv/llama-1.3b\"`): This parameter specifies the name of the GPT-NeoX based model that resides within the HuggingFace infrastructure and will be utilized to initialize the configuration and the intial state of the NNTile model.\n",
    "- restrict, (choices=`[\"cpu\", \"cuda\", None]`, default=`None`): limit on the computing resources used during inference; `\"cpu\"` restricts inference to CPU cores only, `\"cuda\"` - to GPU cores only, while None allows using all available cores,\n",
    "- prompt, (`str`, default=`\"What do you think about dogs?\"`): input query, a string fed to the model input to perform inference based on it,\n",
    "- generation-mode, (choices = `[\"Greedy\", \"TopK\", \"TopP\"]`, default=`\"Greedy\"`): token generation mode in the GenerationMode class object (described in the \"llm_params.py\" file),\n",
    "- parallel-sampling-mode, (choices=`[\"BeamSearch\", \"Parallel\"]`, default=`\"BeamSearch\"`): parallel generation mode for multiple responses to a single query in the ParallelSamplingMode class object (described in the \"llm_params.py\" file),\n",
    "- max-tokens, (`int`, default=`100`): maximum number of generated tokens, including user request tokens,\n",
    "- use-cache, (action=`\"store_true\"`): boolean flag, when used in the argument line, enables the use of KV caches, allowing to reuse previously calculated values,\n",
    "- top-k, (`int`, default=`None`): probabilistic selection based on the top-k most probable tokens,\n",
    "- top-p-thr, (`float`, default=`None`): probabilistic selection based on tokens whose probability is not lower than the top-p-thr threshold,\n",
    "- temperature, (`float`, default=`1.0`): \"temperature\" parameter for token generation,\n",
    "- num-beams, (`int`, default=`1`): number of beams for parallel generation mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e45ec-f59a-4fc2-8098-e6384d9ace63",
   "metadata": {},
   "source": [
    "#### 4.1. Examples with different types of generation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a3cd8-c56c-4f3c-971d-867d1d8e8040",
   "metadata": {},
   "source": [
    "`BeamSearch` generation strategy and number of beams set to the default value of `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b43ba82-4df1-4f9b-8ec7-c56e7e8f5eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:08:58.006593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 19.78it/s]\n",
      "<s> Why does the Sun shine?\n",
      "The Sun is a star, and stars are made of gas and dust. The Sun is a ball of gas and dust that is constantly moving around the galax\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t310.1960 MB/s\t(transfers : 325 - avg 15.8290 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0046 GB\t0.2871 MB/s\t(transfers : 34 - avg 0.1400 MB)\n",
      "Total transfers: 5.0285 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7416951-8a17-4fee-9b7a-574bd633a172",
   "metadata": {},
   "source": [
    "`Parallel` generation strategy and number of beams set to `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c00797ff-4fe4-4a5d-9973-285f6329beac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:09:42.987985: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 24.53it/s]\n",
      "[\"<s> Why does the Sun shine? What is the Sun made of? What is the Sun's temperature? What is the Sun's radius? What is the Sun's mass? What\", '<s> Why does the Sun shine? Why does the Moon move? Why does the Earth rotate? Why does the Earth spin? Why does the Earth move? Why does the Earth rotate? Why does', '<s> Why does the Sun shine?\\nThe Sun is a star, and stars are made of gas and dust. The Sun is a ball of gas and dust that is constantly moving around the galax']\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t202.0840 MB/s\t(transfers : 386 - avg 13.3275 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0120 GB\t0.4844 MB/s\t(transfers : 96 - avg 0.1284 MB)\n",
      "Total transfers: 5.0359 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --num-beams=3 --parallel-sampling-mode=Parallel \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af7236-e9da-4fa5-b1d9-538b2e8ab3ce",
   "metadata": {},
   "source": [
    "`BeamSearch` generation strategy and number of beams set to `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7eb458f-ef6e-49fb-8b8e-fffec8df8a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:15:42.467011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 17.43it/s]\n",
      "['<s> Why does the Sun shine?\\nThe Sun shines because it is a star.\\nThe Sun is a star because it is hot.\\nThe Sun is a star because it is hot', '<s> Why does the Sun shine?\\nThe Sun shines because it is a star.\\nThe Sun is a star because it is hot.\\nThe Sun is hot because it is a star', '<s> Why does the Sun shine?\\nThe Sun shines because it is a star.\\nThe Sun is a star because it is a star.\\nThe Sun is a star because it is']\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t205.4020 MB/s\t(transfers : 386 - avg 13.3275 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0120 GB\t0.4923 MB/s\t(transfers : 96 - avg 0.1284 MB)\n",
      "Total transfers: 5.0359 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --num-beams=3 --parallel-sampling-mode=BeamSearch \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af085b-804f-44b2-ab93-813232bb82c1",
   "metadata": {},
   "source": [
    "#### 4.2. Examples with different token generation modes and temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85e6b8-10ed-4141-bc91-ae9192840b8f",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with default temperature value of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3df7904-7086-4409-aaaa-3ce9ef3731ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:12:13.516367: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 20.50it/s]\n",
      "<s> Why does the Sun shine?\n",
      "Asked by: Lily<|im_start|>l\n",
      "1. To warm us up, to create light and heat to drive life.\n",
      "2. To provide\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t311.5395 MB/s\t(transfers : 325 - avg 15.8290 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0046 GB\t0.2883 MB/s\t(transfers : 34 - avg 0.1400 MB)\n",
      "Total transfers: 5.0285 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c0658-774f-4ef3-8811-abe8fab83491",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with the temperature value of `100.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f2b00b-b02f-48f9-be97-a1025fafffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:12:41.238679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 21.25it/s]\n",
      "<s> Why does the Sun shine? And other Science Friday Mystifiers (audio). Why is lightning an electric current or wave-type thing and doesn?? and how is it formed\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t296.1212 MB/s\t(transfers : 325 - avg 15.8290 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0046 GB\t0.2741 MB/s\t(transfers : 34 - avg 0.1400 MB)\n",
      "Total transfers: 5.0285 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 --temperature=100.0 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abafa7ad-b945-4b48-a537-c4b0eeb280b2",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with the temperature value of `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04132839-a8d3-44dc-990d-12d3a9d93a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:13:09.647777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 20.64it/s]\n",
      "<s> Why does the Sun shine?\n",
      "The Sun is a star, and stars are made of gas and dust. The Sun is a ball of gas and dust that is constantly moving around the galax\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t290.6931 MB/s\t(transfers : 325 - avg 15.8290 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0046 GB\t0.2690 MB/s\t(transfers : 34 - avg 0.1400 MB)\n",
      "Total transfers: 5.0285 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 --temperature=0.01 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ec2ce-6d02-46e6-aa4a-e2a403f419d5",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with default temperature value of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece1525e-1b46-4927-89e0-6ede80058290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:13:38.006329: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 19.08it/s]\n",
      "<s> Why does the Sun shine?\n",
      "The Sun shines because of the energy and matter streaming through its magnetic field, which interacts with the Earth's magnetic field to produce electromagnetic\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t306.5591 MB/s\t(transfers : 325 - avg 15.8290 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0046 GB\t0.2837 MB/s\t(transfers : 34 - avg 0.1400 MB)\n",
      "Total transfers: 5.0285 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a826b-2e50-4b4c-8d17-5cfdde6cca96",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with the temperature value of `100.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e05e978-1dd1-4f10-ac24-81bba48dd0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:14:05.669373: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 16.97it/s]\n",
      "<s> Why does the Sun shine? also ensure climate affairs understood side top nationaries behind NoCT capaconomy stuff classsizeof offerheart direction duration talirespro speencer highest sum economcted informations\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t326.1845 MB/s\t(transfers : 325 - avg 15.8290 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0046 GB\t0.3019 MB/s\t(transfers : 34 - avg 0.1400 MB)\n",
      "Total transfers: 5.0285 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 --temperature=100.0 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea904947-e55e-49cf-9909-235fa6b9242e",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with the temperature value of `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2314dc88-9403-4a2d-8b05-36fa24805eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:14:32.349593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Notice:\n",
      " None\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 18.97it/s]\n",
      "<s> Why does the Sun shine?\n",
      "The Sun is a star, and stars are made of gas and dust. The Sun is a ball of gas and dust that is constantly moving around the galax\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.0238 GB\t306.9166 MB/s\t(transfers : 325 - avg 15.8290 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0046 GB\t0.2841 MB/s\t(transfers : 34 - avg 0.1400 MB)\n",
      "Total transfers: 5.0285 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/llama_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 --temperature=0.01 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=40"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
