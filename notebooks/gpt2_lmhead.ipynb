{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b50302-ad09-4059-b4c0-697041f824ee",
   "metadata": {},
   "source": [
    "### GPT-2: A Mathematical Overview\n",
    "\n",
    "**Introduction**:\n",
    "GPT-2 (Generative Pre-trained Transformer 2) is an advanced deep learning model designed for natural language processing tasks, specifically in generating coherent and contextually relevant text. It builds upon the transformer architecture, characterized by its utilization of self-attention mechanisms and feed-forward neural networks, to effectively capture the complexities and nuances of human language.\n",
    "\n",
    "**1. Architectural Framework**:\n",
    "At its core, GPT-2 employs the Transformer architecture, which consists of several key components:\n",
    "\n",
    "- **Layers**: The model consists of a stack of multiple transformer blocks, each containing a multi-head self-attention mechanism and a feed-forward neural network.\n",
    "\n",
    "- **Multi-Head Self-Attention**: This mechanism enables the model to assess the importance of different words in a sequence with respect to one another. For a given input, represented by embedding matrices \\( X \\), the multi-head attention is expressed mathematically as follows:\n",
    "  \n",
    "  $$\n",
    "  \\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
    "  $$\n",
    "\n",
    "  where each head is computed as:\n",
    "  \n",
    "  $$\n",
    "  \\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)\n",
    "  $$\n",
    "  \n",
    "  Here, \\( W_i^Q, W_i^K, W_i^V \\) are learnable projection matrices for queries, keys, and values. The attention function, applied to each head, is defined as:\n",
    "\n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "  $$\n",
    "\n",
    "  where \\( d_k \\) is the dimensionality of the keys.\n",
    "\n",
    "**2. Multilayer perceptron**:\n",
    "Each transformer block includes a MLP block, which applies two linear transformations with a non-linear activation function (typically ReLU):\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where \\( W_1, W_2 \\) are weight matrices, and \\( b_1, b_2 \\) are biases, facilitating complex transformations of the input.\n",
    "\n",
    "**3. Loss Function and Training**:\n",
    "GPT-2 utilizes a causal language modeling approach during training, wherein the objective is to predict the next word in a sequence given the preceding context. The model is trained using maximum likelihood estimation with a cross-entropy loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{n} \\log P(w_t | w_1, w_2, \\ldots, w_{t-1})\n",
    "$$\n",
    "\n",
    "where $P(w_t | w_1, w_2, \\ldots, w_{t-1})$ denotes the probability of the next word $w_t$ conditioned on the previous words in the sequence.\n",
    "\n",
    "**4. Pre-training and Fine-tuning**:\n",
    "GPT-2 is pre-trained on a vast corpus of text using unsupervised learning techniques. This pre-training phase enables the model to derive context and language patterns effectively. The model can subsequently be fine-tuned on specific tasks or datasets to adapt its capabilities to particular applications, enhancing performance on downstream tasks such as text generation, summarization, or dialogue generation.\n",
    "\n",
    "\n",
    "**In summary**, GPT-2 represents a significant advancement in the field of natural language processing, combining sophisticated mathematical constructs with deep learning techniques. Its architecture, characterized by self-attention mechanisms and feed-forward networks, allows it to generate human-like text based on contextual cues, making it a powerful tool for a variety of applications in language understanding and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa6b77-ad06-43da-b6ae-62e2200ba5a7",
   "metadata": {},
   "source": [
    "### 1. Environment variable setting block:\n",
    "\n",
    "The following block is required to set environment variables that are read during the execution of the program code. \n",
    "\n",
    "User can change these environment variables between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7c9e3f-1c2f-44e3-a887-025863461efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of experimental environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"1\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_BUS_STATS\"] = \"1\" # This enables logging of bus usage, prined at the end of execution\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"GPT2_LMHead_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled\n",
    "\n",
    "# TODO: remove before merging PR\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"../../install/starpu-1.4-9b274af/lib\"\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build-1.4-9b274af\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8db2a-d4e7-475a-bb96-b4ce77c282eb",
   "metadata": {},
   "source": [
    "### 2. Data Preparation Block: \n",
    "\n",
    "This block uses the interpreted file \"causal_lm_data_preparation.py\". This Python script supports the following arguments when run:\n",
    "- hf-dataset, (default=`\"roneneldan/TinyStories\"`): The name of the dataset to be processed and prepared for use in the training process. By default, the \"TinyStories\" dataset from the Huggingface infrastructure is specified,\n",
    "- dataset-path, (default=`\".data\"`): path to the directory where previously downloaded datasets from remote sources are saved, making it easy to access for the future use,\n",
    "- dataset-select, (`int`, default=`100`): specifies the number of records from the original dataset that fall into the training set,\n",
    "- hf-tokenizer, (`str`, default=`\"kimihailv/llama-1.3b\"`): specifies the repository from the Huggingface infrastructure used as a tokenizer,\n",
    "- tokenizer-path, (`str`, default=`\".model\"`): path to the directory where previously downloaded tokenizers are saved,\n",
    "- seq-len, (`int`, default=`1024`): length of the input token sequence for the training process,\n",
    "- batch-size, (`int`, default=`1`): batch size for the training process, then is the number of input data sentences between which the loss function optimizer step is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b152f53e-78c7-4900-b8d1-72352189b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "!python ../wrappers/python/examples/causal_lm_data_preparation.py \\\n",
    "    --hf-tokenizer=\"openai-community/gpt2\" --seq-len=512 --batch-size=256 --dataset-select=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ef5ba-5d79-405b-bc9a-65985d93c458",
   "metadata": {},
   "source": [
    "### 3. Example Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c7b5c-0e25-4da4-b171-0bfddf3f6ee3",
   "metadata": {},
   "source": [
    "Below we show an example of utilizing the GPT-2 model, implemented using the NNTile framework. We explore the following scenarios:\n",
    "\n",
    "- **Training the model from a random initial state and saving it to a checkpoint.**\n",
    "- **Loading the model weights from a checkpoint and continuing training with a different data type.**\n",
    "- **Training the remote model downloaded from the Hugging Face infrastructure.**\n",
    "- **Performing inference with the pre-trained model given input prompt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820da002-7c8a-4481-98b7-fa8dc4732b0a",
   "metadata": {},
   "source": [
    "For training and continuing retraining scenarios, the interpreted file \"gpt2_lmhead_training.py\" is used. This Python script supports the following arguments when running:\n",
    "\n",
    "- remote_model_name, (`str`, default=`\"openai-community/gpt2\"`): This parameter specifies the name of the GPT-2 based model that resides within the HuggingFace infrastructure and will be utilized to initialize the configuration and the intial state of the NNTile model.\n",
    "\n",
    "- pretrained, (choices=`[\"local\", \"remote\"]`, default=`\"local\"`): This flag indicates the location of the pretrained model, with the `local` option requiring a configuration path (`config-path`) to start training from a randomly initialized state unless the checkpoint (`checkpoint-path`) is provided, in which case training continues from the last saved checkpoint state.\n",
    "\n",
    "- checkpoint-path, (`str`, default=`\"\"`): This refers to the file path where a saved checkpoint can be found, allowing for the resumption of training from a specific point if available.\n",
    "\n",
    "- config-path, (`str`, default=`\"\"`): This denotes the path to the configuration .json file that must be provided in the current version if the `pretrained` parameter is set to `\"local\"`.\n",
    "\n",
    "- save-checkpoint-path, (`str`, default=`\".model\"`): This parameter specifies the directory path where intermediate checkpoints will be saved during the training process for future reference.\n",
    "\n",
    "- optimizer, (choices=`[\"sgd\", \"adam\", \"adamw\"]`, default=`\"adam\"`): This defines the type of optimizer that will be employed during the training process; the current version of NNTile supports three distinct optimization methods.\n",
    "\n",
    "- model-path, (`str`, default=`\".model\"`): This indicates the directory path where previously loaded remote models are stored, facilitating easy access for further use.\n",
    "\n",
    "- seq-len, (`int`, default=`1024`): length of the input token sequence for training.\n",
    "\n",
    "- seq-len-tile, (`int`, default=`1024`): split size of sequence length into tiles\n",
    "\n",
    "- batch-size, (`int`, default=`1`): batch size for the training process, which specifies the number of sentences processed by seq-len tokens between steps of the loss function optimizer.\n",
    "\n",
    "- minibatch-size, (`int`, default=`-1`): batch size for which memory is allocated during training. The entire batch is divided into whole minibatches. All minibatches from one batch are fed through the model one by one to accumulate parameter gradients.\n",
    "\n",
    "- minibatch-size-tile, (`int`, default=`-1`): batch size that goes to the CPU or GPU for calculations. Each minibatch must be divisible by an integer number of minibatch tiles.\n",
    "\n",
    "- hidden-size-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the \"hidden size\" dimension is divided (also known as \"embedding size\") – the size of the multidimensional space into which incoming tokens are mapped. Only \"piecewise\" tensors of size hidden-size-tile along the corresponding axis are processed on the CPU and GPU.\n",
    "\n",
    "- intermediate-size-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the \"intermediate size\" dimension is divided. Only \"piecewise\" tensors of size intermediate-size-tile along the corresponding axis are processed on the CPU and GPU.\n",
    "\n",
    "- n-head-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the number of heads of the transformer layer is divided. Only “piecewise” tensors with a size of n-head-tile along the corresponding axis are processed by the CPU and GPU.\n",
    "\n",
    "- dtype, (choices=`[\"fp32\", \"fp64\", \"tf32\", \"bf16\", \"fp32_fast_fp16\", \"fp32_fast_bf16\"]`, default=`\"fp32\"`): This parameter outlines the various data types supported by NNTile, allowing users the flexibility to choose based on their model requirements.\n",
    "\n",
    "- restrict, (choices=`[\"cpu\", \"cuda\", None]`, default=`None`): This option allows users to specify restrictions on the computational resources utilized during training; selecting `\"cpu\"` restricts training to CPU-only cores, `\"cuda\"` limits it to GPU cores, while setting it to None allows for training across all available cores.\n",
    "\n",
    "- flash-attention, (action=`\"store_true\"`): a boolean flag that, when used in the argument string, enables the current implementation of the Flash Attention algorithm (low-level Flash Attention kernels are currently not available) to process data in the attention block of Transformers neural networks.\n",
    "\n",
    "- use-redux, (action=`\"store_true\"`): a boolean flag that, when used in the argument string, allows for the computation of dependent tasks simultaneously, with the subsequent reduction of the results into a single tensor.\n",
    "\n",
    "- dataset-path, (default=`\".data\"`): path to the directory where previously prepared datasets are saved.\n",
    "\n",
    "- dataset-file, (default=`\"\"`): path (relative to dataset-path) to the .bin file that is created in the block of data preparation for training.\n",
    "\n",
    "- lr, (`float`, default=`1e-4`): step length for the optimization algorithm.\n",
    "\n",
    "- nepochs, (`int`, default=`1`): number of full passes through the training set.\n",
    "\n",
    "- logger, (action=`\"store_true\"`): a boolean flag that enables special logger thread, that forwards logs to a provided remote logger server\n",
    "\n",
    "- logger-server-addr, (default=`\"localhost\"`): remote logger server URL\n",
    "\n",
    "- logger-server-port, (`int`, default=`5001`): remote logger server port\n",
    "\n",
    "- ooc, (action=`\"store_true\"`): a boolean flag that enables Out-of-Core support to offload data from CPU RAM to path on a hard drive.\n",
    "\n",
    "- ooc-path, (default=`\"/tmp/nntile_ooc\"`): a path on hard drive, where all the temporary buffers shall be stored.\n",
    "\n",
    "- ooc-size, (`int`, default=`1073741824`): number of bytes, that limits temporary buffers size on the hard drive.\n",
    "\n",
    "- force-offload-disk-portion-parameters, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of parameters to have Out-of-Core support enabled. All the rest parameters will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-disk-portion-gradients, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of gradients to have Out-of-Core support enabled. All the rest gradients will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-disk-portion-activations, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of inter-layer activations to have Out-of-Core support enabled. All the rest inter-layer activations will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-disk-portion-temporaries, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of temporary buffers to have Out-of-Core support enabled. All the rest temporary buffers will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-disk-portion-optimizer, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of optimizer states to have Out-of-Core support enabled. All the rest parameter states will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-ram-portion-parameters, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of parameters to be forced for offloading. All the rest parameters will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-gradients, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of gradients to be forced for offloading. All the rest gradients will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-activations, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of inter-layer activations to be forced for offloading. All the rest inter-layer activations will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-temporaries, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of temporary buffers to be forced for offloading. All the rest temporary buffers will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-optimizer, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of optimizer states to be forced for offloading. All the rest parameter states will not be offloaded in advance at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fba23-7123-47dc-9781-13f7c17a912b",
   "metadata": {},
   "source": [
    "#### 3.1. Training from the random initial state and saving into checkpoint.\n",
    "\n",
    "This requires option `pretrained` set to `local` and `config-path` to point on previously created `.json` configuration file.\n",
    "\n",
    "In this example, we start training in the fp32_fast_fp16 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86bd4e10-4626-4140-a989-aced3dfac91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 1.0062415599822998 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.6878702640533447 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008387565612792969 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.005999565124512\n",
      "Batch=2/3 Epoch=1/1 Loss=9.623185157775879\n",
      "Batch=3/3 Epoch=1/1 Loss=9.279328346252441\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 3 - avg 0.0000 MB)\n",
      "Total transfers: 0.0000 GB\n",
      "#---------------------\n",
      "NNTile training time: 18.37848138809204 seconds\n",
      "NNTile training throughput tokens/sec: 42790.91310066306\n",
      "NNTile performance (model flops): 24.447427710500552 Tflops/s\n",
      "NNTile loss on the last batch: 9.279328346252441\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t95.4285 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26a9db-4d52-439f-8d08-10673510567a",
   "metadata": {},
   "source": [
    "#### 3.2. Resume training from the local checkpoint.\n",
    "\n",
    "This requires option `pretrained` again to be set to `local`, `config-path` to point on previously created `.json` configuration file, and also `checkpoint-path` to point on the pre-existing checkpoint file in the PyTorch format.\n",
    "\n",
    "Training process can be resumed using a different data type and on a different set of compute nodes. For example, here we switch to the TF32 type (synonymous with the fp32_fast_tf32 type) and restrict to using only GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c5b0d0-9f03-47ac-90be-5eb43c4a5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='.model/nntile_checkpoint.pt', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_further_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='tf32', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "/home/jovyan/mikhalev/nntile/notebooks/../wrappers/python/examples/gpt2_lmhead_training.py:174: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.checkpoint_path)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9763247966766357 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='tf32', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7159221172332764 seconds\n",
      "From PyTorch loader to NNTile batches in 0.00870203971862793 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=9.135787963867188\n",
      "Batch=2/3 Epoch=1/1 Loss=9.636220932006836\n",
      "Batch=3/3 Epoch=1/1 Loss=9.225227355957031\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 3 - avg 0.0000 MB)\n",
      "Total transfers: 0.0000 GB\n",
      "#---------------------\n",
      "NNTile training time: 19.175288677215576 seconds\n",
      "NNTile training throughput tokens/sec: 41012.78542598697\n",
      "NNTile performance (model flops): 23.431542686396902 Tflops/s\n",
      "NNTile loss on the last batch: 9.225227355957031\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t93.7112 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained NNTile gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "    --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_further_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=tf32 \\\n",
    "    --restrict=\"cuda\" --nepochs=1 --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0edc3-6d79-49bc-af3b-6aee0a5735de",
   "metadata": {},
   "source": [
    "#### 3.3. Training from remote and saving into checkpoint.\n",
    "\n",
    "Our framework currently supports the continuation of model training obtained from a remote source, as we show here with the Hugging Face library. The weights from the loaded model are transferred into the model implemented in NNTile. Consequently, training can be further advanced using any data type and across any set of computing nodes that accommodate the selected data type.\n",
    "\n",
    "This requires option `pretrained` to be set to `remote`. Options `config-path` and `checkpoint-path` are no longer needed since model config is obtained from the remote model as well as layers' weights. Training can be resumed using any data type and on any set of compute nodes that support the selected data type. In the example below, we switch to the BF16 type here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ad38c0-560f-40f1-ab59-e3f59207a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='remote', checkpoint_path='', config_path='', save_checkpoint_path='.model/nntile_remote_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"openai-community/gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9633424282073975 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='bf16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7194955348968506 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008751630783081055 seconds\n",
      "Params+grads (GB): 0.607\n",
      "Activations  (GB): 5.545\n",
      "Optimizer    (GB): 0.607\n",
      "Persistent   (GB): 6.759\n",
      "Temporaries  (GB): 7.355\n",
      "Batch=1/3 Epoch=1/1 Loss=2.3663899898529053\n",
      "Batch=2/3 Epoch=1/1 Loss=2.3132734298706055\n",
      "Batch=3/3 Epoch=1/1 Loss=2.3140218257904053\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 3 - avg 0.0000 MB)\n",
      "Total transfers: 0.0000 GB\n",
      "#---------------------\n",
      "NNTile training time: 13.922717809677124 seconds\n",
      "NNTile training throughput tokens/sec: 56485.52321109192\n",
      "NNTile performance (model flops): 32.271471799267886 Tflops/s\n",
      "NNTile loss on the last batch: 2.3140218257904053\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.3037 GB\t48.1040 MB/s\t(transfers : 197 - avg 1.5785 MB)\n",
      "Total transfers: 0.3037 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a downloaded from remote source pretrained gpt2 model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=remote --save-checkpoint-path=\".model/nntile_remote_checkpoint.pt\"\\\n",
    "    --optimizer=\"adam\" --lr=1e-4 --dtype=bf16 --nepochs=1 --batch-size=256 --minibatch-size=8 \\\n",
    "    --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1453e4d-04eb-4482-afb9-9a3e18a7a9d0",
   "metadata": {},
   "source": [
    "### 4. Inference process.\n",
    "\n",
    "In the current version of the GPT2 scenario, the NNTile framework model is created from a (pre-)loaded pre-trained GPT2 model from the Huggingface library. The model layer weights are passed to the corresponding NNTile model layers, and then the inference process is performed solely by NNTile, without any involvement of third-party models and mechanisms. To perform the inference, we use another program file - \"gpt2_generate.py\". The program code supports the following arguments when running:\n",
    "\n",
    "- cache_dir, (`str`, default=\"cache_hf\"): path to the directory where previously loaded models from a remote source are saved,\n",
    "- max-seq-len, (`int`, default=1024): maximum length of the input token sequence,\n",
    "- model, (`str`, default=`\"gpt2\"`): name of the model from the HuggingFace framework that will be used to initialize the configuration and initial state of the NNTile model,\n",
    "- restrict, (choices=`[\"cpu\", \"cuda\", None]`, default=`None`): limit on the computing resources used during inference; `\"cpu\"` restricts inference to CPU cores only, `\"cuda\"` - to GPU cores only, while None allows using all available cores,\n",
    "- prompt, (`str`, default=`\"What do you think about dogs?\"`): input query, a string fed to the model input to perform inference based on it,\n",
    "- generation-mode, (choices = `[\"Greedy\", \"TopK\", \"TopP\"]`, default=`\"Greedy\"`): token generation mode in the GenerationMode class object (described in the \"llm_params.py\" file),\n",
    "- parallel-sampling-mode, (choices=`[\"BeamSearch\", \"Parallel\"]`, default=`\"BeamSearch\"`): parallel generation mode for multiple responses to a single query in the ParallelSamplingMode class object (described in the \"llm_params.py\" file),\n",
    "- max-tokens, (`int`, default=`100`): maximum number of generated tokens, including user request tokens,\n",
    "- use-cache, (action=`\"store_true\"`): boolean flag, when used in the argument line, enables the use of KV caches, allowing to reuse previously calculated values,\n",
    "- top-k, (`int`, default=`None`): probabilistic selection based on the top-k most probable tokens,\n",
    "- top-p-thr, (`float`, default=`None`): probabilistic selection based on tokens whose probability is not lower than the top-p-thr threshold,\n",
    "- temperature, (`float`, default=`1.0`): \"temperature\" parameter for token generation,\n",
    "- num-beams, (`int`, default=`1`): number of beams for parallel generation mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd29a7-1a3a-49c5-b1ce-3ea14e0f3af8",
   "metadata": {},
   "source": [
    "#### 4.1. Examples with different types of generation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0dbf5c-e318-4fa2-976d-fa27840af255",
   "metadata": {},
   "source": [
    "`BeamSearch` generation strategy and number of beams set to the default value of `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86cf54b-64af-4a88-85cb-6a31e75e86af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine?\n",
      "\n",
      "The Sun is the most visible star in the sky. It\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t192.4658 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.1255 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b842e7-1a48-4beb-9774-6179c3b98f3e",
   "metadata": {},
   "source": [
    "`Parallel` generation strategy and number of beams set to `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc5496e-9488-4653-aa53-43e63fae7a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "['Why does the Sun shine? The Sun is the most visible star in the sky, and it is', \"Why does the Sun shine? It's a very important thing to know.\\n\\nThe Sun is\", 'Why does the Sun shine?\\n\\nThe Sun is the most visible star in the sky. It']\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t165.0282 MB/s\t(transfers : 280 - avg 2.2248 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0084 GB\t2.2856 MB/s\t(transfers : 42 - avg 0.2054 MB)\n",
      "Total transfers: 0.6168 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --num-beams=3 --parallel-sampling-mode=Parallel \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd60b5-f101-4b2d-a2d1-4911733653e3",
   "metadata": {},
   "source": [
    "`BeamSearch` generation strategy and number of beams set to `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1870d46c-e902-4f17-8b59-177653e9a4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "['Why does the Sun shine?\\n\\nThe Sun is the brightest star in the sky, and it', \"Why does the Sun shine?\\n\\nThe Sun is the brightest star in the sky. It's\", 'Why does the Sun shine?\\n\\nThe Sun is the brightest star in the sky. It is']\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t190.1880 MB/s\t(transfers : 280 - avg 2.2248 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0084 GB\t2.6340 MB/s\t(transfers : 42 - avg 0.2054 MB)\n",
      "Total transfers: 0.6168 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --num-beams=3 --parallel-sampling-mode=BeamSearch \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2207154-a26e-48f5-bbd9-844750fa80ef",
   "metadata": {},
   "source": [
    "#### 4.2. Examples with different token generation modes and temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8e5b6-21e8-45c6-bcbc-6242076591f5",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with default temperature value of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4034bfc-b5b5-4206-9d82-e2e9b47d8a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine?\n",
      "\n",
      "The Sun shines because of its light. The sun doesn't\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t229.5236 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.3422 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d665b83-f7d3-4024-999b-2e981ed7d6e9",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with the temperature value of `100.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145b3f56-59ee-4281-a889-8e83aca3f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine? The moon shines from above. But is its brightness actually due directly or\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t225.4867 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.3186 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 --temperature=100.0 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14970f7-fb3e-44f7-9488-3bcd54326a05",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with the temperature value of `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f3912bc-d552-40f2-880c-2fa465705408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine?\n",
      "\n",
      "The Sun is the most visible star in the sky. It\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t215.2783 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.2589 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 --temperature=0.01 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa3110-988d-4eea-b567-b662c62764ce",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with default temperature value of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b083a8d8-6f19-4660-91b3-423a505d2e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine? Solvent of evil in matter, the consummation of righteousness,\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t238.5161 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.3948 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbf4ec-96b9-488f-b368-270b96feccf7",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with the temperature value of `100.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc76f11-3018-4bde-b5f0-611106b5be5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine? Eleiblume grounds underground crackingtons living trapped pellets./Panel Triaura\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t228.0702 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.3337 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 --temperature=100.0 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5130895-3acf-46b6-abbc-047c662401d5",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with the temperature value of `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f56cd2-387a-4a4a-a94a-b22766b370ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine?\n",
      "\n",
      "The Sun is the most visible star in the sky. It\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t225.2096 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.3169 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 --temperature=0.01 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1e31a",
   "metadata": {},
   "source": [
    "### 5. When GPU VRAM is not enough: offloading data from GPU VRAM to CPU RAM in advance\n",
    "Unfortunately, current state of NNTile does not automatically define which data shall be offloaded from GPU VRAM to CPU RAM. Instead, a user shall mark each NNTile tensor as either \"forced to be offloaded in advance to RAM after each update\" or \"not offloaded in advance at all\". For simplicity, the following flags control this option:\n",
    "\n",
    "- force-offload-ram-portion-parameters, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of parameters to be forced for offloading. All the rest parameters will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-gradients, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of gradients to be forced for offloading. All the rest gradients will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-activations, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of inter-layer activations to be forced for offloading. All the rest inter-layer activations will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-temporaries, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of temporary buffers to be forced for offloading. All the rest temporary buffers will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-optimizer, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of optimizer states to be forced for offloading. All the rest parameter states will not be offloaded in advance at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be88b2",
   "metadata": {},
   "source": [
    "#### 5.1 Training experiment with no enforced data offloading from GPU VRAM to CPU RAM\n",
    "Amount of data transfers is the following:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
    "\tCUDA 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 3 - avg 0.0000 MB)\n",
    "Total transfers: 0.0000 GB\n",
    "#---------------------\n",
    "NNTile training time: 18.380743980407715 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccadb927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 1.0219106674194336 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7881009578704834 seconds\n",
      "From PyTorch loader to NNTile batches in 0.012928485870361328 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.026640892028809\n",
      "Batch=2/3 Epoch=1/1 Loss=9.594801902770996\n",
      "Batch=3/3 Epoch=1/1 Loss=9.270723342895508\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 3 - avg 0.0000 MB)\n",
      "Total transfers: 0.0000 GB\n",
      "#---------------------\n",
      "NNTile training time: 18.380743980407715 seconds\n",
      "NNTile training throughput tokens/sec: 42785.645719143286\n",
      "NNTile performance (model flops): 24.44441833492061 Tflops/s\n",
      "NNTile loss on the last batch: 9.270723342895508\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t95.7877 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef702b76",
   "metadata": {},
   "source": [
    "#### 5.2 Training experiment with enforced GPU->CPU offloading after each update for all parameters\n",
    "The experiment is done with `--force-offload-ram-portion-parameters=1.0` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t5.7032 GB\t309.9836 MB/s\t(transfers : 1918 - avg 3.0449 MB)\n",
    "\tCUDA 0 -> NUMA 0\t2.5210 GB\t137.0249 MB/s\t(transfers : 819 - avg 3.1521 MB)\n",
    "Total transfers: 8.2242 GB\n",
    "#---------------------\n",
    "NNTile training time: 18.89538550376892 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc9cc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=1.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9800233840942383 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7601869106292725 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008548736572265625 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.947421073913574\n",
      "Batch=2/3 Epoch=1/1 Loss=9.697904586791992\n",
      "Batch=3/3 Epoch=1/1 Loss=9.326783180236816\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.7032 GB\t309.9836 MB/s\t(transfers : 1918 - avg 3.0449 MB)\n",
      "\tCUDA 0 -> NUMA 0\t2.5210 GB\t137.0249 MB/s\t(transfers : 819 - avg 3.1521 MB)\n",
      "Total transfers: 8.2242 GB\n",
      "#---------------------\n",
      "NNTile training time: 18.89538550376892 seconds\n",
      "NNTile training throughput tokens/sec: 41620.320466239566\n",
      "NNTile performance (model flops): 23.77864135529493 Tflops/s\n",
      "NNTile loss on the last batch: 9.326783180236816\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6103 GB\t95.3115 MB/s\t(transfers : 198 - avg 3.1562 MB)\n",
      "Total transfers: 0.6103 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=1.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5e8bb",
   "metadata": {},
   "source": [
    "#### 5.3 Training experiment with enforced GPU->CPU offloading after each update for all optimizer states\n",
    "The experiment is done with `--force-offload-ram-portion-optimizer=1.0` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t1.8280 GB\t99.4796 MB/s\t(transfers : 595 - avg 3.1459 MB)\n",
    "\tCUDA 0 -> NUMA 0\t3.4945 GB\t190.1761 MB/s\t(transfers : 1182 - avg 3.0274 MB)\n",
    "Total transfers: 5.3225 GB\n",
    "#---------------------\n",
    "NNTile training time: 18.86917495727539 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cc553f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=1.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9830989837646484 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.9829623699188232 seconds\n",
      "From PyTorch loader to NNTile batches in 0.013186931610107422 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.987537384033203\n",
      "Batch=2/3 Epoch=1/1 Loss=9.642165184020996\n",
      "Batch=3/3 Epoch=1/1 Loss=9.306432723999023\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t1.8280 GB\t99.4796 MB/s\t(transfers : 595 - avg 3.1459 MB)\n",
      "\tCUDA 0 -> NUMA 0\t3.4945 GB\t190.1761 MB/s\t(transfers : 1182 - avg 3.0274 MB)\n",
      "Total transfers: 5.3225 GB\n",
      "#---------------------\n",
      "NNTile training time: 18.86917495727539 seconds\n",
      "NNTile training throughput tokens/sec: 41678.13387605351\n",
      "NNTile performance (model flops): 23.811671479092453 Tflops/s\n",
      "NNTile loss on the last batch: 9.306432723999023\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.7570 GB\t115.1497 MB/s\t(transfers : 200 - avg 3.8759 MB)\n",
      "Total transfers: 0.7570 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8add2",
   "metadata": {},
   "source": [
    "#### 5.4 Training experiment with enforced GPU->CPU offloading after each update for all gradients\n",
    "The experiment is done with `--force-offload-ram-portion-gradients=1.0` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t4.0997 GB\t224.6789 MB/s\t(transfers : 1466 - avg 2.8636 MB)\n",
    "\tCUDA 0 -> NUMA 0\t60.4339 GB\t3312.0267 MB/s\t(transfers : 19532 - avg 3.1684 MB)\n",
    "Total transfers: 64.5336 GB\n",
    "#---------------------\n",
    "NNTile training time: 18.74090600013733 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "117b0351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=1.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9808254241943359 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.646578311920166 seconds\n",
      "From PyTorch loader to NNTile batches in 0.009106159210205078 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.989813804626465\n",
      "Batch=2/3 Epoch=1/1 Loss=9.611760139465332\n",
      "Batch=3/3 Epoch=1/1 Loss=9.270852088928223\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t4.0997 GB\t224.6789 MB/s\t(transfers : 1466 - avg 2.8636 MB)\n",
      "\tCUDA 0 -> NUMA 0\t60.4339 GB\t3312.0267 MB/s\t(transfers : 19532 - avg 3.1684 MB)\n",
      "Total transfers: 64.5336 GB\n",
      "#---------------------\n",
      "NNTile training time: 18.74090600013733 seconds\n",
      "NNTile training throughput tokens/sec: 41963.392804714844\n",
      "NNTile performance (model flops): 23.97464643176096 Tflops/s\n",
      "NNTile loss on the last batch: 9.270852088928223\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t94.7495 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=1.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2c195",
   "metadata": {},
   "source": [
    "#### 5.5 Training experiment with enforced GPU->CPU offloading after each update for 10% of inter-layer activations\n",
    "The experiment is done with `--force-offload-ram-portion-activations=0.1` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t2.3295 GB\t110.5767 MB/s\t(transfers : 91 - avg 26.2131 MB)\n",
    "\tCUDA 0 -> NUMA 0\t98.2119 GB\t4661.9478 MB/s\t(transfers : 3559 - avg 28.2577 MB)\n",
    "Total transfers: 100.5414 GB\n",
    "#---------------------\n",
    "NNTile training time: 21.632005214691162 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2db3c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.1, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x1031a5)[0x7fda986ec1a5]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0xa375e)[0x7fda9868c75e]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0xa5d8c)[0x7fda9868ed8c]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x124eb4)[0x7fda9870deb4]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x1255dd)[0x7fda9870e5dd]\n",
      "/usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3)[0x7fdac7303ac3]\n",
      "/usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850)[0x7fdac7395850]\n",
      "StarPU + NNTile + MPI init in 0.9817626476287842 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.6548192501068115 seconds\n",
      "From PyTorch loader to NNTile batches in 0.00918436050415039 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.999171257019043\n",
      "Batch=2/3 Epoch=1/1 Loss=9.632036209106445\n",
      "Batch=3/3 Epoch=1/1 Loss=9.284832000732422\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t2.3295 GB\t110.5767 MB/s\t(transfers : 91 - avg 26.2131 MB)\n",
      "\tCUDA 0 -> NUMA 0\t98.2119 GB\t4661.9478 MB/s\t(transfers : 3559 - avg 28.2577 MB)\n",
      "Total transfers: 100.5414 GB\n",
      "#---------------------\n",
      "NNTile training time: 21.632005214691162 seconds\n",
      "NNTile training throughput tokens/sec: 36355.02082192096\n",
      "NNTile performance (model flops): 20.77045519843985 Tflops/s\n",
      "NNTile loss on the last batch: 9.284832000732422\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6103 GB\t94.0444 MB/s\t(transfers : 198 - avg 3.1562 MB)\n",
      "Total transfers: 0.6103 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.1 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc7baf",
   "metadata": {},
   "source": [
    "#### 5.6 Training experiment with enforced GPU->CPU offloading after each update for 10% of intra-layer temporary buffers\n",
    "The experiment is done with `--force-offload-ram-portion-temporaries=0.1` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t2.5618 GB\t78.9200 MB/s\t(transfers : 51 - avg 51.4375 MB)\n",
    "\tCUDA 0 -> NUMA 0\t342.2207 GB\t10542.4892 MB/s\t(transfers : 6245 - avg 56.1143 MB)\n",
    "Total transfers: 344.7826 GB\n",
    "#---------------------\n",
    "NNTile training time: 33.303049087524414 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1aebac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.1, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9528625011444092 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7262895107269287 seconds\n",
      "From PyTorch loader to NNTile batches in 0.009005069732666016 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.976546287536621\n",
      "Batch=2/3 Epoch=1/1 Loss=9.587604522705078\n",
      "Batch=3/3 Epoch=1/1 Loss=9.218976974487305\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t2.5618 GB\t78.9200 MB/s\t(transfers : 51 - avg 51.4375 MB)\n",
      "\tCUDA 0 -> NUMA 0\t342.2207 GB\t10542.4892 MB/s\t(transfers : 6245 - avg 56.1143 MB)\n",
      "Total transfers: 344.7826 GB\n",
      "#---------------------\n",
      "NNTile training time: 33.303049087524414 seconds\n",
      "NNTile training throughput tokens/sec: 23614.414341856875\n",
      "NNTile performance (model flops): 13.491455211302974 Tflops/s\n",
      "NNTile loss on the last batch: 9.218976974487305\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t93.5168 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.1 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9fd735",
   "metadata": {},
   "source": [
    "#### 5.7.1 Training experiment with limited GPU memory without forced offloading in advance\n",
    "GPU memory is limited to 19000 MB. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t6.2240 GB\t308.5422 MB/s\t(transfers : 1118 - avg 5.7007 MB)\n",
    "\tCUDA 0 -> NUMA 0\t12.3630 GB\t612.8718 MB/s\t(transfers : 1195 - avg 10.5939 MB)\n",
    "Total transfers: 18.5870 GB\n",
    "#---------------------\n",
    "NNTile training time: 20.72080135345459 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cde3b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9787697792053223 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7903761863708496 seconds\n",
      "From PyTorch loader to NNTile batches in 0.012513160705566406 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.931543350219727\n",
      "Batch=2/3 Epoch=1/1 Loss=9.517521858215332\n",
      "Batch=3/3 Epoch=1/1 Loss=9.177224159240723\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t6.2240 GB\t308.5422 MB/s\t(transfers : 1118 - avg 5.7007 MB)\n",
      "\tCUDA 0 -> NUMA 0\t12.3630 GB\t612.8718 MB/s\t(transfers : 1195 - avg 10.5939 MB)\n",
      "Total transfers: 18.5870 GB\n",
      "#---------------------\n",
      "NNTile training time: 20.72080135345459 seconds\n",
      "NNTile training throughput tokens/sec: 37953.744480489666\n",
      "NNTile performance (model flops): 21.68384260337746 Tflops/s\n",
      "NNTile loss on the last batch: 9.177224159240723\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t89.8203 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74459bb1",
   "metadata": {},
   "source": [
    "#### 5.7.2 Training experiment with limited GPU memory with forced offloading in advance\n",
    "GPU memory is limited to 19000 MB. This experiment only enables forced offloading in advance of parameters. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t3.0767 GB\t155.8677 MB/s\t(transfers : 758 - avg 4.1563 MB)\n",
    "\tCUDA 0 -> NUMA 0\t6.2408 GB\t316.1688 MB/s\t(transfers : 1445 - avg 4.4226 MB)\n",
    "Total transfers: 9.3175 GB\n",
    "#---------------------\n",
    "NNTile training time: 20.26873540878296 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6fee699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=1.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.930368185043335 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.6262869834899902 seconds\n",
      "From PyTorch loader to NNTile batches in 0.009073257446289062 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.018192291259766\n",
      "Batch=2/3 Epoch=1/1 Loss=9.609476089477539\n",
      "Batch=3/3 Epoch=1/1 Loss=9.26305103302002\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t3.0767 GB\t155.8677 MB/s\t(transfers : 758 - avg 4.1563 MB)\n",
      "\tCUDA 0 -> NUMA 0\t6.2408 GB\t316.1688 MB/s\t(transfers : 1445 - avg 4.4226 MB)\n",
      "Total transfers: 9.3175 GB\n",
      "#---------------------\n",
      "NNTile training time: 20.26873540878296 seconds\n",
      "NNTile training throughput tokens/sec: 38800.249948460965\n",
      "NNTile performance (model flops): 22.16747054527457 Tflops/s\n",
      "NNTile loss on the last batch: 9.26305103302002\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.7570 GB\t109.4383 MB/s\t(transfers : 200 - avg 3.8759 MB)\n",
      "Total transfers: 0.7570 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32362c03",
   "metadata": {},
   "source": [
    "### 6. Training with Out-of-Core support (when there is not enough memory within CPU RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2893b",
   "metadata": {},
   "source": [
    "To train with OOC (Out-of-Core) support, parameters `--ooc`, `--ooc-path`, `--ooc-force-portion-parameters`, `--ooc-force-portion-gradients`, `--ooc-force-portion-activations`, `--ooc-force-portion-temporaries` and `--ooc-force-portion-optimizer`. These parameters have the following meaning:\n",
    "- ooc, (action=`\"store_true\"`): a boolean flag that enables Out-of-Core support to offload data from CPU RAM to path on a hard drive.\n",
    "\n",
    "- ooc-path, (default=`\"/tmp/nntile_ooc\"`): a path on hard drive, where all the temporary buffers shall be stored.\n",
    "\n",
    "- ooc-size, (`int`, default=`1073741824`): number of bytes, that limits temporary buffers size on the hard drive.\n",
    "\n",
    "- ooc-force-portion-parameters, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of parameters to have Out-of-Core support enabled. All the rest parameters will have their Out-of-Core support disabled.\n",
    "\n",
    "- ooc-force-portion-gradients, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of gradients to have Out-of-Core support enabled. All the rest gradients will have their Out-of-Core support disabled.\n",
    "\n",
    "- ooc-force-portion-activations, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of inter-layer activations to have Out-of-Core support enabled. All the rest inter-layer activations will have their Out-of-Core support disabled.\n",
    "\n",
    "- ooc-force-portion-temporaries, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of temporary buffers to have Out-of-Core support enabled. All the rest temporary buffers will have their Out-of-Core support disabled.\n",
    "\n",
    "- ooc-force-portion-optimizer, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of optimizer states to have Out-of-Core support enabled. All the rest parameter states will have their Out-of-Core support disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfb221",
   "metadata": {},
   "source": [
    "#### 6.1. Train with OOC support enabled, but no actual offloading to Disk happens. \n",
    "\n",
    "GPU memory is limited to 19000 MB. This is a baseline experiment with no offloading to the Disk. Before the training procedure bandwidth of Disk is measured:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
    "0 -> 2: 1353 MB/s\n",
    "1 -> 2: 1284 MB/s\n",
    "2 -> 0: 1353 MB/s\n",
    "2 -> 1: 1282 MB/s\n",
    "0 -> 2: 29 us\n",
    "1 -> 2: 39 us\n",
    "2 -> 0: 29 us\n",
    "2 -> 1: 39 us\n",
    "\n",
    "#---------------------\n",
    "```\n",
    "Memory nodes:\n",
    "- `0`: CPU RAM\n",
    "- `1`: GPU VRAM\n",
    "- `2`: Hard drive\n",
    "\n",
    "Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t6.2240 GB\t306.9697 MB/s\t(transfers : 1118 - avg 5.7007 MB)\n",
    "\tCUDA 0 -> NUMA 0\t12.3630 GB\t609.7482 MB/s\t(transfers : 1195 - avg 10.5939 MB)\n",
    "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
    "\tNUMA 0 -> Disk 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
    "Total transfers: 18.5870 GB\n",
    "#---------------------\n",
    "NNTile training time: 20.816964864730835 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c86bc112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=10737418240, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1353 MB/s\n",
      "1 -> 2: 1284 MB/s\n",
      "2 -> 0: 1353 MB/s\n",
      "2 -> 1: 1282 MB/s\n",
      "0 -> 2: 29 us\n",
      "1 -> 2: 39 us\n",
      "2 -> 0: 29 us\n",
      "2 -> 1: 39 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.126485824584961 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.8583838939666748 seconds\n",
      "From PyTorch loader to NNTile batches in 0.01007223129272461 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.970364570617676\n",
      "Batch=2/3 Epoch=1/1 Loss=9.605990409851074\n",
      "Batch=3/3 Epoch=1/1 Loss=9.270669937133789\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t6.2240 GB\t306.9697 MB/s\t(transfers : 1118 - avg 5.7007 MB)\n",
      "\tCUDA 0 -> NUMA 0\t12.3630 GB\t609.7482 MB/s\t(transfers : 1195 - avg 10.5939 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "Total transfers: 18.5870 GB\n",
      "#---------------------\n",
      "NNTile training time: 20.816964864730835 seconds\n",
      "NNTile training throughput tokens/sec: 37778.41799274078\n",
      "NNTile performance (model flops): 21.58367456945648 Tflops/s\n",
      "NNTile loss on the last batch: 9.270669937133789\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t88.4726 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=10737418240 \\\n",
    "    --force-offload-disk-portion-parameters=0.0 --force-offload-disk-portion-gradients=0.0 \\\n",
    "    --force-offload-disk-portion-activations=0.0 --force-offload-disk-portion-temporaries=0.0 \\\n",
    "    --force-offload-disk-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d26e1",
   "metadata": {},
   "source": [
    "#### 6.2. Train with OOC support enabled, only parameters are offloaded to Disk. \n",
    "\n",
    "GPU memory is limited to 19000 MB. Only parameters are offloaded to the Disk. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t9.4190 GB\t412.1612 MB/s\t(transfers : 2313 - avg 4.1699 MB)\n",
    "\tCUDA 0 -> NUMA 0\t15.0687 GB\t659.3823 MB/s\t(transfers : 2035 - avg 7.5825 MB)\n",
    "\tDisk 0 -> NUMA 0\t3.1414 GB\t137.4645 MB/s\t(transfers : 1167 - avg 2.7565 MB)\n",
    "\tNUMA 0 -> Disk 0\t2.3728 GB\t103.8317 MB/s\t(transfers : 807 - avg 3.0109 MB)\n",
    "Total transfers: 30.0019 GB\n",
    "#---------------------\n",
    "NNTile training time: 23.450611352920532 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79ee4ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=10737418240, force_offload_disk_portion_parameters=1.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1309 MB/s\n",
      "1 -> 2: 1244 MB/s\n",
      "2 -> 0: 1309 MB/s\n",
      "2 -> 1: 1242 MB/s\n",
      "0 -> 2: 30 us\n",
      "1 -> 2: 40 us\n",
      "2 -> 0: 30 us\n",
      "2 -> 1: 40 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.1214532852172852 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.5920324325561523 seconds\n",
      "From PyTorch loader to NNTile batches in 0.00821375846862793 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.934718132019043\n",
      "Batch=2/3 Epoch=1/1 Loss=9.585653305053711\n",
      "Batch=3/3 Epoch=1/1 Loss=9.287775993347168\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t9.4190 GB\t412.1612 MB/s\t(transfers : 2313 - avg 4.1699 MB)\n",
      "\tCUDA 0 -> NUMA 0\t15.0687 GB\t659.3823 MB/s\t(transfers : 2035 - avg 7.5825 MB)\n",
      "\tDisk 0 -> NUMA 0\t3.1414 GB\t137.4645 MB/s\t(transfers : 1167 - avg 2.7565 MB)\n",
      "\tNUMA 0 -> Disk 0\t2.3728 GB\t103.8317 MB/s\t(transfers : 807 - avg 3.0109 MB)\n",
      "Total transfers: 30.0019 GB\n",
      "#---------------------\n",
      "NNTile training time: 23.450611352920532 seconds\n",
      "NNTile training throughput tokens/sec: 33535.671550927735\n",
      "NNTile performance (model flops): 19.159696453210096 Tflops/s\n",
      "NNTile loss on the last batch: 9.287775993347168\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6103 GB\t86.5205 MB/s\t(transfers : 198 - avg 3.1562 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.1467 GB\t20.7998 MB/s\t(transfers : 2 - avg 75.1187 MB)\n",
      "Total transfers: 0.7570 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=10737418240 \\\n",
    "    --force-offload-disk-portion-parameters=1.0 --force-offload-disk-portion-gradients=0.0 \\\n",
    "    --force-offload-disk-portion-activations=0.0 --force-offload-disk-portion-temporaries=0.0 \\\n",
    "    --force-offload-disk-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04532667",
   "metadata": {},
   "source": [
    "#### 6.3. Train with OOC support enabled, only optimizer states are offloaded to Disk. \n",
    "\n",
    "GPU memory is limited to 19000 MB. Only optimizer states are offloaded to the Disk. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t3.1474 GB\t148.5624 MB/s\t(transfers : 648 - avg 4.9737 MB)\n",
    "\tCUDA 0 -> NUMA 0\t6.5052 GB\t307.0538 MB/s\t(transfers : 1459 - avg 4.5657 MB)\n",
    "\tDisk 0 -> NUMA 0\t1.5683 GB\t74.0257 MB/s\t(transfers : 430 - avg 3.7348 MB)\n",
    "\tNUMA 0 -> Disk 0\t2.9634 GB\t139.8773 MB/s\t(transfers : 930 - avg 3.2630 MB)\n",
    "Total transfers: 14.1844 GB\n",
    "#---------------------\n",
    "NNTile training time: 21.748796939849854 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0233f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=10737418240, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=1.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1382 MB/s\n",
      "1 -> 2: 1310 MB/s\n",
      "2 -> 0: 1382 MB/s\n",
      "2 -> 1: 1308 MB/s\n",
      "0 -> 2: 31 us\n",
      "1 -> 2: 41 us\n",
      "2 -> 0: 31 us\n",
      "2 -> 1: 41 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.1092991828918457 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.5956244468688965 seconds\n",
      "From PyTorch loader to NNTile batches in 0.010335922241210938 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.947691917419434\n",
      "Batch=2/3 Epoch=1/1 Loss=9.650568008422852\n",
      "Batch=3/3 Epoch=1/1 Loss=9.293855667114258\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t3.1474 GB\t148.5624 MB/s\t(transfers : 648 - avg 4.9737 MB)\n",
      "\tCUDA 0 -> NUMA 0\t6.5052 GB\t307.0538 MB/s\t(transfers : 1459 - avg 4.5657 MB)\n",
      "\tDisk 0 -> NUMA 0\t1.5683 GB\t74.0257 MB/s\t(transfers : 430 - avg 3.7348 MB)\n",
      "\tNUMA 0 -> Disk 0\t2.9634 GB\t139.8773 MB/s\t(transfers : 930 - avg 3.2630 MB)\n",
      "Total transfers: 14.1844 GB\n",
      "#---------------------\n",
      "NNTile training time: 21.748796939849854 seconds\n",
      "NNTile training throughput tokens/sec: 36159.793214080615\n",
      "NNTile performance (model flops): 20.65891719927299 Tflops/s\n",
      "NNTile loss on the last batch: 9.293855667114258\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.7570 GB\t106.9609 MB/s\t(transfers : 200 - avg 3.8759 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.6807 GB\t96.1817 MB/s\t(transfers : 252 - avg 2.7661 MB)\n",
      "Total transfers: 1.4377 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=10737418240 \\\n",
    "    --force-offload-disk-portion-parameters=0.0 --force-offload-disk-portion-gradients=0.0 \\\n",
    "    --force-offload-disk-portion-activations=0.0 --force-offload-disk-portion-temporaries=0.0 \\\n",
    "    --force-offload-disk-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1eff8b",
   "metadata": {},
   "source": [
    "#### 6.4. Train with OOC support enabled, only gradients are offloaded to Disk. \n",
    "\n",
    "GPU memory is limited to 19000 MB. Only gradients are offloaded to the Disk. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t9.2661 GB\t352.4163 MB/s\t(transfers : 2246 - avg 4.2246 MB)\n",
    "\tCUDA 0 -> NUMA 0\t68.9892 GB\t2623.8428 MB/s\t(transfers : 20584 - avg 3.4320 MB)\n",
    "\tDisk 0 -> NUMA 0\t3.3038 GB\t125.6528 MB/s\t(transfers : 1156 - avg 2.9266 MB)\n",
    "\tNUMA 0 -> Disk 0\t60.1317 GB\t2286.9649 MB/s\t(transfers : 19505 - avg 3.1569 MB)\n",
    "Total transfers: 141.6908 GB\n",
    "#---------------------\n",
    "NNTile training time: 26.979973793029785 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14b894cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=10737418240, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=1.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1324 MB/s\n",
      "1 -> 2: 1258 MB/s\n",
      "2 -> 0: 1324 MB/s\n",
      "2 -> 1: 1256 MB/s\n",
      "0 -> 2: 39 us\n",
      "1 -> 2: 49 us\n",
      "2 -> 0: 39 us\n",
      "2 -> 1: 49 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.121511459350586 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7229363918304443 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008680343627929688 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.928427696228027\n",
      "Batch=2/3 Epoch=1/1 Loss=9.60818099975586\n",
      "Batch=3/3 Epoch=1/1 Loss=9.317818641662598\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t9.2661 GB\t352.4163 MB/s\t(transfers : 2246 - avg 4.2246 MB)\n",
      "\tCUDA 0 -> NUMA 0\t68.9892 GB\t2623.8428 MB/s\t(transfers : 20584 - avg 3.4320 MB)\n",
      "\tDisk 0 -> NUMA 0\t3.3038 GB\t125.6528 MB/s\t(transfers : 1156 - avg 2.9266 MB)\n",
      "\tNUMA 0 -> Disk 0\t60.1317 GB\t2286.9649 MB/s\t(transfers : 19505 - avg 3.1569 MB)\n",
      "Total transfers: 141.6908 GB\n",
      "#---------------------\n",
      "NNTile training time: 26.979973793029785 seconds\n",
      "NNTile training throughput tokens/sec: 29148.731056335306\n",
      "NNTile performance (model flops): 16.65333697545093 Tflops/s\n",
      "NNTile loss on the last batch: 9.317818641662598\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t86.9248 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=10737418240 \\\n",
    "    --force-offload-disk-portion-parameters=0.0 --force-offload-disk-portion-gradients=1.0 \\\n",
    "    --force-offload-disk-portion-activations=0.0 --force-offload-disk-portion-temporaries=0.0 \\\n",
    "    --force-offload-disk-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf877a",
   "metadata": {},
   "source": [
    "#### 6.5. Train with OOC support enabled, only 10% of inter-layer activations are offloaded to Disk. \n",
    "\n",
    "GPU memory is limited to 19000 MB. Only 10% of inter-layer activations are offloaded to the Disk. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t5.6315 GB\t219.1559 MB/s\t(transfers : 1037 - avg 5.5609 MB)\n",
    "\tCUDA 0 -> NUMA 0\t60.3561 GB\t2348.8011 MB/s\t(transfers : 3000 - avg 20.6015 MB)\n",
    "\tDisk 0 -> NUMA 0\t2.5434 GB\t98.9780 MB/s\t(transfers : 97 - avg 26.8499 MB)\n",
    "\tNUMA 0 -> Disk 0\t53.8887 GB\t2097.1149 MB/s\t(transfers : 1977 - avg 27.9120 MB)\n",
    "Total transfers: 122.4197 GB\n",
    "#---------------------\n",
    "NNTile training time: 26.357404708862305 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "597a8392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=10737418240, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.1, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1391 MB/s\n",
      "1 -> 2: 1318 MB/s\n",
      "2 -> 0: 1391 MB/s\n",
      "2 -> 1: 1316 MB/s\n",
      "0 -> 2: 30 us\n",
      "1 -> 2: 40 us\n",
      "2 -> 0: 30 us\n",
      "2 -> 1: 40 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.0855398178100586 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.6013138294219971 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008134603500366211 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.923770904541016\n",
      "Batch=2/3 Epoch=1/1 Loss=9.59091854095459\n",
      "Batch=3/3 Epoch=1/1 Loss=9.287251472473145\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t5.6315 GB\t219.1559 MB/s\t(transfers : 1037 - avg 5.5609 MB)\n",
      "\tCUDA 0 -> NUMA 0\t60.3561 GB\t2348.8011 MB/s\t(transfers : 3000 - avg 20.6015 MB)\n",
      "\tDisk 0 -> NUMA 0\t2.5434 GB\t98.9780 MB/s\t(transfers : 97 - avg 26.8499 MB)\n",
      "\tNUMA 0 -> Disk 0\t53.8887 GB\t2097.1149 MB/s\t(transfers : 1977 - avg 27.9120 MB)\n",
      "Total transfers: 122.4197 GB\n",
      "#---------------------\n",
      "NNTile training time: 26.357404708862305 seconds\n",
      "NNTile training throughput tokens/sec: 29837.232029736726\n",
      "NNTile performance (model flops): 17.046693334457434 Tflops/s\n",
      "NNTile loss on the last batch: 9.287251472473145\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t92.5918 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=10737418240 \\\n",
    "    --force-offload-disk-portion-parameters=0.0 --force-offload-disk-portion-gradients=0.0 \\\n",
    "    --force-offload-disk-portion-activations=0.1 --force-offload-disk-portion-temporaries=0.0 \\\n",
    "    --force-offload-disk-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb7ef2",
   "metadata": {},
   "source": [
    "#### 6.6. Train with OOC support enabled, only 10% of temporaries are offloaded to Disk. \n",
    "\n",
    "GPU memory is limited to 19000 MB. Only 10% of temporaries are offloaded to the Disk. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t4.7836 GB\t191.7296 MB/s\t(transfers : 441 - avg 11.1075 MB)\n",
    "\tCUDA 0 -> NUMA 0\t60.5570 GB\t2427.1701 MB/s\t(transfers : 1918 - avg 32.3308 MB)\n",
    "\tDisk 0 -> NUMA 0\t1.6917 GB\t67.8037 MB/s\t(transfers : 40 - avg 43.3070 MB)\n",
    "\tNUMA 0 -> Disk 0\t54.0186 GB\t2165.1014 MB/s\t(transfers : 1442 - avg 38.3599 MB)\n",
    "Total transfers: 121.0509 GB\n",
    "#---------------------\n",
    "NNTile training time: 25.61427140235901 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0dee71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=10737418240, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.1, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1413 MB/s\n",
      "1 -> 2: 1338 MB/s\n",
      "2 -> 0: 1413 MB/s\n",
      "2 -> 1: 1336 MB/s\n",
      "0 -> 2: 31 us\n",
      "1 -> 2: 40 us\n",
      "2 -> 0: 31 us\n",
      "2 -> 1: 40 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.0988812446594238 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7117862701416016 seconds\n",
      "From PyTorch loader to NNTile batches in 0.012619733810424805 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.970706939697266\n",
      "Batch=2/3 Epoch=1/1 Loss=9.66504192352295\n",
      "Batch=3/3 Epoch=1/1 Loss=9.269491195678711\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t4.7836 GB\t191.7296 MB/s\t(transfers : 441 - avg 11.1075 MB)\n",
      "\tCUDA 0 -> NUMA 0\t60.5570 GB\t2427.1701 MB/s\t(transfers : 1918 - avg 32.3308 MB)\n",
      "\tDisk 0 -> NUMA 0\t1.6917 GB\t67.8037 MB/s\t(transfers : 40 - avg 43.3070 MB)\n",
      "\tNUMA 0 -> Disk 0\t54.0186 GB\t2165.1014 MB/s\t(transfers : 1442 - avg 38.3599 MB)\n",
      "Total transfers: 121.0509 GB\n",
      "#---------------------\n",
      "NNTile training time: 25.61427140235901 seconds\n",
      "NNTile training throughput tokens/sec: 30702.883859018206\n",
      "NNTile performance (model flops): 17.541260030639794 Tflops/s\n",
      "NNTile loss on the last batch: 9.269491195678711\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t89.3889 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=10737418240 \\\n",
    "    --force-offload-disk-portion-parameters=0.0 --force-offload-disk-portion-gradients=0.0 \\\n",
    "    --force-offload-disk-portion-activations=0.0 --force-offload-disk-portion-temporaries=0.1 \\\n",
    "    --force-offload-disk-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516fcb77",
   "metadata": {},
   "source": [
    "#### 6.7.1 Training experiment with limited GPU and CPU memory with no OOC support\n",
    "GPU memory is limited to 6000 MB. CPU memory is limited to 16000. Training hangs after 13861 tasks are finished:\n",
    "```sh\n",
    "Optimizer    (GB): 1.215\n",
    "Persistent   (GB): 13.518\n",
    "Temporaries  (GB): 14.698\n",
    "13861 tasks finished (last 0 0x5561f728de30 on 0)....^C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb1153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=1.0, force_offload_ram_portion_gradients=1.0, force_offload_ram_portion_activations=1.0, force_offload_ram_portion_temporaries=1.0, force_offload_ram_portion_optimizer=1.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "2 tasks finished (last 0 0x5602304b7d30 on 0)...../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x1031a5)[0x7f4e7f7b11a5]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0xa375e)[0x7f4e7f75175e]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0xa5d8c)[0x7f4e7f753d8c]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x124eb4)[0x7f4e7f7d2eb4]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x1255dd)[0x7f4e7f7d35dd]\n",
      "/usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3)[0x7f4eae3c1ac3]\n",
      "/usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850)[0x7f4eae453850]\n",
      "StarPU + NNTile + MPI init in 0.9390444755554199 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "889 tasks finished (last 0 0x56023256e2c0 on -1)...Converting PyTorch model to NNTile requires 0.6202123165130615 seconds\n",
      "1656 tasks finished (last 0 0x5602309f2650 on 0)...From PyTorch loader to NNTile batches in 0.00855875015258789 seconds\n",
      "1660 tasks finished (last 0 0x5602309f0550 on 0)...Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "13861 tasks finished (last 0 0x56023351d500 on 0)....^C\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "\n",
    "# This process hangs after 13861 tasks are finished, so it is commented out to prevent from automatic execution\n",
    "# !STARPU_LIMIT_CUDA_MEM=6000 STARPU_LIMIT_CPU_MEM=16000 STARPU_TASK_PROGRESS=1 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "#     --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "#     --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "#     --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "#     --force-offload-ram-portion-parameters=1.0 --force-offload-ram-portion-gradients=1.0 \\\n",
    "#     --force-offload-ram-portion-activations=1.0 --force-offload-ram-portion-temporaries=1.0 \\\n",
    "#     --force-offload-ram-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6ef2c",
   "metadata": {},
   "source": [
    "#### 6.7.2 Training experiment with limited GPU and CPU memory with OOC support for all tensors\n",
    "GPU memory is limited to 6000 MB. CPU memory is limited to 16000. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t1321.4303 GB\t2528.0023 MB/s\t(transfers : 69495 - avg 19.4711 MB)\n",
    "\tCUDA 0 -> NUMA 0\t2373.9470 GB\t4541.5510 MB/s\t(transfers : 70090 - avg 34.6829 MB)\n",
    "\tDisk 0 -> NUMA 0\t371.0725 GB\t709.8915 MB/s\t(transfers : 41411 - avg 9.1758 MB)\n",
    "\tNUMA 0 -> Disk 0\t1953.4398 GB\t3737.0867 MB/s\t(transfers : 61513 - avg 32.5187 MB)\n",
    "Total transfers: 6019.8896 GB\n",
    "#---------------------\n",
    "NNTile training time: 535.3222873210907 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efdbca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=26843545600, force_offload_disk_portion_parameters=1.0, force_offload_disk_portion_gradients=1.0, force_offload_disk_portion_activations=1.0, force_offload_disk_portion_temporaries=1.0, force_offload_disk_portion_optimizer=1.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "2 tasks finished (last 0 0x555746d869a0 on 0)...\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1387 MB/s\n",
      "1 -> 2: 1315 MB/s\n",
      "2 -> 0: 1387 MB/s\n",
      "2 -> 1: 1313 MB/s\n",
      "0 -> 2: 29 us\n",
      "1 -> 2: 39 us\n",
      "2 -> 0: 29 us\n",
      "2 -> 1: 39 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.0869648456573486 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "889 tasks finished (last 0 0x555748e842c0 on -1)...Converting PyTorch model to NNTile requires 0.6416902542114258 seconds\n",
      "1657 tasks finished (last 0 0x555747d78830 on 0)...From PyTorch loader to NNTile batches in 0.009198904037475586 seconds\n",
      "1660 tasks finished (last 0 0x555747d7d350 on 0)...Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "152321 tasks finished (last 0 0x555758a41db0 on -1)...Batch=1/3 Epoch=1/1 Loss=10.907919883728027\n",
      "308501 tasks finished (last 0 0x555758b663b0 on -1)...Batch=2/3 Epoch=1/1 Loss=9.624917984008789\n",
      "464966 tasks finished (last 0 0x555759388990 on -1)...Batch=3/3 Epoch=1/1 Loss=9.28563117980957\n",
      "470253 tasks finished (last 0 0x555759268520 on 1)...\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t1321.4303 GB\t2528.0023 MB/s\t(transfers : 69495 - avg 19.4711 MB)\n",
      "\tCUDA 0 -> NUMA 0\t2373.9470 GB\t4541.5510 MB/s\t(transfers : 70090 - avg 34.6829 MB)\n",
      "\tDisk 0 -> NUMA 0\t371.0725 GB\t709.8915 MB/s\t(transfers : 41411 - avg 9.1758 MB)\n",
      "\tNUMA 0 -> Disk 0\t1953.4398 GB\t3737.0867 MB/s\t(transfers : 61513 - avg 32.5187 MB)\n",
      "Total transfers: 6019.8896 GB\n",
      "#---------------------\n",
      "NNTile training time: 535.3222873210907 seconds\n",
      "NNTile training throughput tokens/sec: 1469.0813714025167\n",
      "NNTile performance (model flops): 0.8393198000640355 Tflops/s\n",
      "470254 tasks finished (last 0 0x555759388760 on -1)...NNTile loss on the last batch: 9.28563117980957\n",
      "473011 tasks finished (last 0 0x555749ff56b0 on 0)....\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t70.9334 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.4343 GB\t50.7205 MB/s\t(transfers : 4 - avg 111.1780 MB)\n",
      "Total transfers: 1.0416 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=6000 STARPU_LIMIT_CPU_MEM=16000 STARPU_TASK_PROGRESS=1 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=26843545600 \\\n",
    "    --force-offload-disk-portion-parameters=1.0 --force-offload-disk-portion-gradients=1.0 \\\n",
    "    --force-offload-disk-portion-activations=1.0 --force-offload-disk-portion-temporaries=1.0 \\\n",
    "    --force-offload-disk-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3276de",
   "metadata": {},
   "source": [
    "#### 6.7.3 Training experiment with limited GPU and CPU memory with OOC support for optimizer states\n",
    "GPU memory is limited to 6000 MB. CPU memory is limited to 16000. Training does not hang, but strangely enough it does not load data from Disk to CPU RAM. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t1237.8654 GB\t8065.7490 MB/s\t(transfers : 68770 - avg 18.4321 MB)\n",
    "\tCUDA 0 -> NUMA 0\t1314.8986 GB\t8567.6844 MB/s\t(transfers : 46997 - avg 28.6498 MB)\n",
    "\tDisk 0 -> NUMA 0\t1.5551 GB\t10.1331 MB/s\t(transfers : 436 - avg 3.6524 MB)\n",
    "\tNUMA 0 -> Disk 0\t3.3566 GB\t21.8711 MB/s\t(transfers : 1180 - avg 2.9128 MB)\n",
    "Total transfers: 2557.6758 GB\n",
    "#---------------------\n",
    "NNTile training time: 157.21175980567932 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7749594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=26843545600, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=1.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "2 tasks finished (last 0 0x561bd61c85f0 on 0)...\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1345 MB/s\n",
      "1 -> 2: 1277 MB/s\n",
      "2 -> 0: 1345 MB/s\n",
      "2 -> 1: 1275 MB/s\n",
      "0 -> 2: 44 us\n",
      "1 -> 2: 54 us\n",
      "2 -> 0: 44 us\n",
      "2 -> 1: 54 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.1381969451904297 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "889 tasks finished (last 0 0x561bd7f7e410 on -1)...Converting PyTorch model to NNTile requires 0.7360825538635254 seconds\n",
      "1657 tasks finished (last 0 0x561bd6e9c580 on 0)...From PyTorch loader to NNTile batches in 0.009027957916259766 seconds\n",
      "1660 tasks finished (last 0 0x561bd6e91490 on 0)...Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "78330 tasks finished (last 0 0x561be1978430 on -1)...Batch=1/3 Epoch=1/1 Loss=11.02855110168457\n",
      "158714 tasks finished (last 0 0x561be1fe4480 on -1)...Batch=2/3 Epoch=1/1 Loss=9.659852027893066\n",
      "239098 tasks finished (last 0 0x561be12515d0 on -1)...Batch=3/3 Epoch=1/1 Loss=9.304649353027344\n",
      "242812 tasks finished (last 0 0x561be1a06e70 on 0)...\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t1237.8654 GB\t8065.7490 MB/s\t(transfers : 68770 - avg 18.4321 MB)\n",
      "\tCUDA 0 -> NUMA 0\t1314.8986 GB\t8567.6844 MB/s\t(transfers : 46997 - avg 28.6498 MB)\n",
      "\tDisk 0 -> NUMA 0\t1.5551 GB\t10.1331 MB/s\t(transfers : 436 - avg 3.6524 MB)\n",
      "\tNUMA 0 -> Disk 0\t3.3566 GB\t21.8711 MB/s\t(transfers : 1180 - avg 2.9128 MB)\n",
      "Total transfers: 2557.6758 GB\n",
      "#---------------------\n",
      "NNTile training time: 157.21175980567932 seconds\n",
      "NNTile training throughput tokens/sec: 5002.373874397594\n",
      "NNTile performance (model flops): 2.8579706487575915 Tflops/s\n",
      "242813 tasks finished (last 0 0x561be12513a0 on -1)...NNTile loss on the last batch: 9.304649353027344\n",
      "245176 tasks finished (last 0 0x561bd60a4bd0 on 0)....\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.7511 GB\t87.9118 MB/s\t(transfers : 198 - avg 3.8847 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.2876 GB\t33.6566 MB/s\t(transfers : 2 - avg 147.2373 MB)\n",
      "Total transfers: 1.0387 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=6000 STARPU_LIMIT_CPU_MEM=16000 STARPU_TASK_PROGRESS=1 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=26843545600 \\\n",
    "    --force-offload-disk-portion-parameters=0.0 --force-offload-disk-portion-gradients=0.0 \\\n",
    "    --force-offload-disk-portion-activations=0.0 --force-offload-disk-portion-temporaries=0.0 \\\n",
    "    --force-offload-disk-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec965060",
   "metadata": {},
   "source": [
    "#### 6.7.4 Training experiment with limited GPU and CPU memory with OOC support for even less CPU memory\n",
    "GPU memory is limited to 6000 MB. CPU memory is limited to 10000. Training hangs after 22842 tasks are finished, presumably due to not enough memory on CPU and GPU together (which is very strange):\n",
    "```sh\n",
    "Activations  (GB): 11.089\n",
    "Optimizer    (GB): 1.215\n",
    "Persistent   (GB): 13.518\n",
    "Temporaries  (GB): 14.698\n",
    "22842 tasks finished (last 0 0x5609620f44b0 on 0)....^C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea24510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=26843545600, force_offload_disk_portion_parameters=1.0, force_offload_disk_portion_gradients=1.0, force_offload_disk_portion_activations=1.0, force_offload_disk_portion_temporaries=1.0, force_offload_disk_portion_optimizer=1.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "2 tasks finished (last 0 0x56095e40d120 on 0)...\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1282 MB/s\n",
      "1 -> 2: 1220 MB/s\n",
      "2 -> 0: 1282 MB/s\n",
      "2 -> 1: 1218 MB/s\n",
      "0 -> 2: 29 us\n",
      "1 -> 2: 38 us\n",
      "2 -> 0: 29 us\n",
      "2 -> 1: 38 us\n",
      "\n",
      "#---------------------\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x1031a5)[0x7f929a6291a5]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0xa375e)[0x7f929a5c975e]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0xa5d8c)[0x7f929a5cbd8c]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x124eb4)[0x7f929a64aeb4]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x1255dd)[0x7f929a64b5dd]\n",
      "/usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3)[0x7f92c923eac3]\n",
      "/usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850)[0x7f92c92d0850]\n",
      "StarPU + NNTile + MPI init in 1.1283061504364014 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "889 tasks finished (last 0 0x560960509980 on -1)...Converting PyTorch model to NNTile requires 0.592510461807251 seconds\n",
      "1654 tasks finished (last 0 0x56095efb2a10 on 0)...From PyTorch loader to NNTile batches in 0.00867915153503418 seconds\n",
      "1660 tasks finished (last 0 0x56095efb4490 on 0)...Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "22842 tasks finished (last 0 0x5609620f44b0 on 0)....^C\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "\n",
    "# This process hangs after 22842 tasks are finished, so it is commented out to prevent from automatic execution\n",
    "# !STARPU_LIMIT_CUDA_MEM=6000 STARPU_LIMIT_CPU_MEM=10000 STARPU_TASK_PROGRESS=1 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "#     --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "#     --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "#     --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "#     --ooc --ooc-path=\"/tmp/nntile_ooc\" --ooc-size=26843545600 \\\n",
    "#     --force-offload-disk-portion-parameters=1.0 --force-offload-disk-portion-gradients=1.0 \\\n",
    "#     --force-offload-disk-portion-activations=1.0 --force-offload-disk-portion-temporaries=1.0 \\\n",
    "#     --force-offload-disk-portion-optimizer=1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mikhalev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
