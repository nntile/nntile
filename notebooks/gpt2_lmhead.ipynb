{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b50302-ad09-4059-b4c0-697041f824ee",
   "metadata": {},
   "source": [
    "### GPT-2: A Mathematical Overview\n",
    "\n",
    "**Introduction**:\n",
    "GPT-2 (Generative Pre-trained Transformer 2) is an advanced deep learning model designed for natural language processing tasks, specifically in generating coherent and contextually relevant text. It builds upon the transformer architecture, characterized by its utilization of self-attention mechanisms and feed-forward neural networks, to effectively capture the complexities and nuances of human language.\n",
    "\n",
    "**1. Architectural Framework**:\n",
    "At its core, GPT-2 employs the Transformer architecture, which consists of several key components:\n",
    "\n",
    "- **Layers**: The model consists of a stack of multiple transformer blocks, each containing a multi-head self-attention mechanism and a feed-forward neural network.\n",
    "\n",
    "- **Multi-Head Self-Attention**: This mechanism enables the model to assess the importance of different words in a sequence with respect to one another. For a given input, represented by embedding matrices \\( X \\), the multi-head attention is expressed mathematically as follows:\n",
    "  \n",
    "  $$\n",
    "  \\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
    "  $$\n",
    "\n",
    "  where each head is computed as:\n",
    "  \n",
    "  $$\n",
    "  \\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)\n",
    "  $$\n",
    "  \n",
    "  Here, \\( W_i^Q, W_i^K, W_i^V \\) are learnable projection matrices for queries, keys, and values. The attention function, applied to each head, is defined as:\n",
    "\n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "  $$\n",
    "\n",
    "  where \\( d_k \\) is the dimensionality of the keys.\n",
    "\n",
    "**2. Multilayer perceptron**:\n",
    "Each transformer block includes a MLP block, which applies two linear transformations with a non-linear activation function (typically ReLU):\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "where \\( W_1, W_2 \\) are weight matrices, and \\( b_1, b_2 \\) are biases, facilitating complex transformations of the input.\n",
    "\n",
    "**3. Loss Function and Training**:\n",
    "GPT-2 utilizes a causal language modeling approach during training, wherein the objective is to predict the next word in a sequence given the preceding context. The model is trained using maximum likelihood estimation with a cross-entropy loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{n} \\log P(w_t | w_1, w_2, \\ldots, w_{t-1})\n",
    "$$\n",
    "\n",
    "where $P(w_t | w_1, w_2, \\ldots, w_{t-1})$ denotes the probability of the next word $w_t$ conditioned on the previous words in the sequence.\n",
    "\n",
    "**4. Pre-training and Fine-tuning**:\n",
    "GPT-2 is pre-trained on a vast corpus of text using unsupervised learning techniques. This pre-training phase enables the model to derive context and language patterns effectively. The model can subsequently be fine-tuned on specific tasks or datasets to adapt its capabilities to particular applications, enhancing performance on downstream tasks such as text generation, summarization, or dialogue generation.\n",
    "\n",
    "\n",
    "**In summary**, GPT-2 represents a significant advancement in the field of natural language processing, combining sophisticated mathematical constructs with deep learning techniques. Its architecture, characterized by self-attention mechanisms and feed-forward networks, allows it to generate human-like text based on contextual cues, making it a powerful tool for a variety of applications in language understanding and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa6b77-ad06-43da-b6ae-62e2200ba5a7",
   "metadata": {},
   "source": [
    "### 1. Environment variable setting block:\n",
    "\n",
    "The following block is required to set environment variables that are read during the execution of the program code. \n",
    "\n",
    "User can change these environment variables between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7c9e3f-1c2f-44e3-a887-025863461efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of experimental environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"1\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_BUS_STATS\"] = \"1\" # This enables logging of bus usage, prined at the end of execution\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"GPT2_LMHead_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled\n",
    "\n",
    "# TODO: remove before merging PR\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"../../install/starpu-1.4-9b274af/lib\"\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build-1.4-9b274af\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8db2a-d4e7-475a-bb96-b4ce77c282eb",
   "metadata": {},
   "source": [
    "### 2. Data Preparation Block: \n",
    "\n",
    "This block uses the interpreted file \"causal_lm_data_preparation.py\". This Python script supports the following arguments when run:\n",
    "- hf-dataset, (default=`\"roneneldan/TinyStories\"`): The name of the dataset to be processed and prepared for use in the training process. By default, the \"TinyStories\" dataset from the Huggingface infrastructure is specified,\n",
    "- dataset-path, (default=`\".data\"`): path to the directory where previously downloaded datasets from remote sources are saved, making it easy to access for the future use,\n",
    "- dataset-select, (`int`, default=`100`): specifies the number of records from the original dataset that fall into the training set,\n",
    "- hf-tokenizer, (`str`, default=`\"kimihailv/llama-1.3b\"`): specifies the repository from the Huggingface infrastructure used as a tokenizer,\n",
    "- tokenizer-path, (`str`, default=`\".model\"`): path to the directory where previously downloaded tokenizers are saved,\n",
    "- seq-len, (`int`, default=`1024`): length of the input token sequence for the training process,\n",
    "- batch-size, (`int`, default=`1`): batch size for the training process, then is the number of input data sentences between which the loss function optimizer step is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b152f53e-78c7-4900-b8d1-72352189b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "!python ../wrappers/python/examples/causal_lm_data_preparation.py \\\n",
    "    --hf-tokenizer=\"openai-community/gpt2\" --seq-len=512 --batch-size=256 --dataset-select=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ef5ba-5d79-405b-bc9a-65985d93c458",
   "metadata": {},
   "source": [
    "### 3. Example Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c7b5c-0e25-4da4-b171-0bfddf3f6ee3",
   "metadata": {},
   "source": [
    "Below we show an example of utilizing the GPT-2 model, implemented using the NNTile framework. We explore the following scenarios:\n",
    "\n",
    "- **Training the model from a random initial state and saving it to a checkpoint.**\n",
    "- **Loading the model weights from a checkpoint and continuing training with a different data type.**\n",
    "- **Training the remote model downloaded from the Hugging Face infrastructure.**\n",
    "- **Performing inference with the pre-trained model given input prompt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820da002-7c8a-4481-98b7-fa8dc4732b0a",
   "metadata": {},
   "source": [
    "For training and continuing retraining scenarios, the interpreted file \"gpt2_lmhead_training.py\" is used. This Python script supports the following arguments when running:\n",
    "\n",
    "- remote_model_name, (`str`, default=`\"openai-community/gpt2\"`): This parameter specifies the name of the GPT-2 based model that resides within the HuggingFace infrastructure and will be utilized to initialize the configuration and the intial state of the NNTile model.\n",
    "\n",
    "- pretrained, (choices=`[\"local\", \"remote\"]`, default=`\"local\"`): This flag indicates the location of the pretrained model, with the `local` option requiring a configuration path (`config-path`) to start training from a randomly initialized state unless the checkpoint (`checkpoint-path`) is provided, in which case training continues from the last saved checkpoint state.\n",
    "\n",
    "- checkpoint-path, (`str`, default=`\"\"`): This refers to the file path where a saved checkpoint can be found, allowing for the resumption of training from a specific point if available.\n",
    "\n",
    "- config-path, (`str`, default=`\"\"`): This denotes the path to the configuration .json file that must be provided in the current version if the `pretrained` parameter is set to `\"local\"`.\n",
    "\n",
    "- save-checkpoint-path, (`str`, default=`\".model\"`): This parameter specifies the directory path where intermediate checkpoints will be saved during the training process for future reference.\n",
    "\n",
    "- optimizer, (choices=`[\"sgd\", \"adam\", \"adamw\"]`, default=`\"adam\"`): This defines the type of optimizer that will be employed during the training process; the current version of NNTile supports three distinct optimization methods.\n",
    "\n",
    "- model-path, (`str`, default=`\".model\"`): This indicates the directory path where previously loaded remote models are stored, facilitating easy access for further use.\n",
    "\n",
    "- seq-len, (`int`, default=`1024`): length of the input token sequence for training.\n",
    "\n",
    "- seq-len-tile, (`int`, default=`1024`): split size of sequence length into tiles\n",
    "\n",
    "- batch-size, (`int`, default=`1`): batch size for the training process, which specifies the number of sentences processed by seq-len tokens between steps of the loss function optimizer.\n",
    "\n",
    "- minibatch-size, (`int`, default=`-1`): batch size for which memory is allocated during training. The entire batch is divided into whole minibatches. All minibatches from one batch are fed through the model one by one to accumulate parameter gradients.\n",
    "\n",
    "- minibatch-size-tile, (`int`, default=`-1`): batch size that goes to the CPU or GPU for calculations. Each minibatch must be divisible by an integer number of minibatch tiles.\n",
    "\n",
    "- hidden-size-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the \"hidden size\" dimension is divided (also known as \"embedding size\") – the size of the multidimensional space into which incoming tokens are mapped. Only \"piecewise\" tensors of size hidden-size-tile along the corresponding axis are processed on the CPU and GPU.\n",
    "\n",
    "- intermediate-size-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the \"intermediate size\" dimension is divided. Only \"piecewise\" tensors of size intermediate-size-tile along the corresponding axis are processed on the CPU and GPU.\n",
    "\n",
    "- n-head-tile, (`int`, default=`-1`): the size of the pieces (tiles) into which the number of heads of the transformer layer is divided. Only “piecewise” tensors with a size of n-head-tile along the corresponding axis are processed by the CPU and GPU.\n",
    "\n",
    "- dtype, (choices=`[\"fp32\", \"fp64\", \"tf32\", \"bf16\", \"fp32_fast_fp16\", \"fp32_fast_bf16\"]`, default=`\"fp32\"`): This parameter outlines the various data types supported by NNTile, allowing users the flexibility to choose based on their model requirements.\n",
    "\n",
    "- restrict, (choices=`[\"cpu\", \"cuda\", None]`, default=`None`): This option allows users to specify restrictions on the computational resources utilized during training; selecting `\"cpu\"` restricts training to CPU-only cores, `\"cuda\"` limits it to GPU cores, while setting it to None allows for training across all available cores.\n",
    "\n",
    "- flash-attention, (action=`\"store_true\"`): a boolean flag that, when used in the argument string, enables the current implementation of the Flash Attention algorithm (low-level Flash Attention kernels are currently not available) to process data in the attention block of Transformers neural networks.\n",
    "\n",
    "- use-redux, (action=`\"store_true\"`): a boolean flag that, when used in the argument string, allows for the computation of dependent tasks simultaneously, with the subsequent reduction of the results into a single tensor.\n",
    "\n",
    "- dataset-path, (default=`\".data\"`): path to the directory where previously prepared datasets are saved.\n",
    "\n",
    "- dataset-file, (default=`\"\"`): path (relative to dataset-path) to the .bin file that is created in the block of data preparation for training.\n",
    "\n",
    "- lr, (`float`, default=`1e-4`): step length for the optimization algorithm.\n",
    "\n",
    "- nepochs, (`int`, default=`1`): number of full passes through the training set.\n",
    "\n",
    "- logger, (action=`\"store_true\"`): a boolean flag that enables special logger thread, that forwards logs to a provided remote logger server\n",
    "\n",
    "- logger-server-addr, (default=`\"localhost\"`): remote logger server URL\n",
    "\n",
    "- logger-server-port, (`int`, default=`5001`): remote logger server port\n",
    "\n",
    "- ooc, (action=`\"store_true\"`): a boolean flag that enables Out-of-Core support to offload data from CPU RAM to path on a hard drive.\n",
    "\n",
    "- ooc-path, (default=`\"/tmp/nntile_ooc\"`): a path on hard drive, where all the temporary buffers shall be stored.\n",
    "\n",
    "- ooc-size, (`int`, default=`1073741824`): number of bytes, that limits temporary buffers size on the hard drive.\n",
    "\n",
    "- force-offload-disk-portion-parameters, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of parameters to have Out-of-Core support enabled. All the rest parameters will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-disk-portion-gradients, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of gradients to have Out-of-Core support enabled. All the rest gradients will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-disk-portion-activations, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of inter-layer activations to have Out-of-Core support enabled. All the rest inter-layer activations will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-disk-portion-temporaries, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of temporary buffers to have Out-of-Core support enabled. All the rest temporary buffers will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-disk-portion-optimizer, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of optimizer states to have Out-of-Core support enabled. All the rest parameter states will have their Out-of-Core support disabled.\n",
    "\n",
    "- force-offload-ram-portion-parameters, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of parameters to be forced for offloading. All the rest parameters will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-gradients, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of gradients to be forced for offloading. All the rest gradients will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-activations, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of inter-layer activations to be forced for offloading. All the rest inter-layer activations will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-temporaries, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of temporary buffers to be forced for offloading. All the rest temporary buffers will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-optimizer, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of optimizer states to be forced for offloading. All the rest parameter states will not be offloaded in advance at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fba23-7123-47dc-9781-13f7c17a912b",
   "metadata": {},
   "source": [
    "### 1. Training from the random initial state and saving into checkpoint.\n",
    "\n",
    "This requires option `pretrained` set to `local` and `config-path` to point on previously created `.json` configuration file.\n",
    "\n",
    "In this example, we start training in the fp32_fast_fp16 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86bd4e10-4626-4140-a989-aced3dfac91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.955167293548584 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7675855159759521 seconds\n",
      "From PyTorch loader to NNTile batches in 0.007760047912597656 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.003764152526855\n",
      "Batch=2/3 Epoch=1/1 Loss=9.656996726989746\n",
      "Batch=3/3 Epoch=1/1 Loss=9.290155410766602\n",
      "NNTile training time: 18.83450222015381 seconds\n",
      "NNTile training throughput tokens/sec: 41754.85982095564\n",
      "NNTile performance (model flops): 23.85550676690466 Tflops/s\n",
      "NNTile loss on the last batch: 9.290155410766602\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.4414 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t23.5330 MB/s\t(transfers : 200 - avg 3.1097 MB)\n",
      "Total transfers: 1.2382 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26a9db-4d52-439f-8d08-10673510567a",
   "metadata": {},
   "source": [
    "### 2. Resume training from the local checkpoint.\n",
    "\n",
    "This requires option `pretrained` again to be set to `local`, `config-path` to point on previously created `.json` configuration file, and also `checkpoint-path` to point on the pre-existing checkpoint file in the PyTorch format.\n",
    "\n",
    "Training process can be resumed using a different data type and on a different set of compute nodes. For example, here we switch to the TF32 type (synonymous with the fp32_fast_tf32 type) and restrict to using only GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c5b0d0-9f03-47ac-90be-5eb43c4a5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='.model/nntile_checkpoint.pt', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_further_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='tf32', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, ooc_force_portion_parameters=0.0, ooc_force_portion_gradients=0.0, ooc_force_portion_activations=0.0, ooc_force_portion_temporaries=0.0, ooc_force_portion_optimizer=0.0)\n",
      "/home/jovyan/mikhalev/nntile/notebooks/../wrappers/python/examples/gpt2_lmhead_training.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.checkpoint_path)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 1.041334867477417 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='tf32', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7132716178894043 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008783102035522461 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=9.226259231567383\n",
      "Batch=2/3 Epoch=1/1 Loss=9.769364356994629\n",
      "Batch=3/3 Epoch=1/1 Loss=9.306928634643555\n",
      "NNTile training time: 19.239985466003418 seconds\n",
      "NNTile training throughput tokens/sec: 40874.8749519383\n",
      "NNTile performance (model flops): 23.352751277181248 Tflops/s\n",
      "NNTile loss on the last batch: 9.306928634643555\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.2040 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t23.3044 MB/s\t(transfers : 200 - avg 3.1097 MB)\n",
      "Total transfers: 1.2382 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained NNTile gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --checkpoint-path=\".model/nntile_checkpoint.pt\" \\\n",
    "    --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_further_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=tf32 \\\n",
    "    --restrict=\"cuda\" --nepochs=1 --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0edc3-6d79-49bc-af3b-6aee0a5735de",
   "metadata": {},
   "source": [
    "### 3. Training from remote and saving into checkpoint.\n",
    "\n",
    "Our framework currently supports the continuation of model training obtained from a remote source, as we show here with the Hugging Face library. The weights from the loaded model are transferred into the model implemented in NNTile. Consequently, training can be further advanced using any data type and across any set of computing nodes that accommodate the selected data type.\n",
    "\n",
    "This requires option `pretrained` to be set to `remote`. Options `config-path` and `checkpoint-path` are no longer needed since model config is obtained from the remote model as well as layers' weights. Training can be resumed using any data type and on any set of compute nodes that support the selected data type. In the example below, we switch to the BF16 type here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ad38c0-560f-40f1-ab59-e3f59207a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='remote', checkpoint_path='', config_path='', save_checkpoint_path='.model/nntile_remote_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, ooc_force_portion_parameters=0.0, ooc_force_portion_gradients=0.0, ooc_force_portion_activations=0.0, ooc_force_portion_temporaries=0.0, ooc_force_portion_optimizer=0.0)\n",
      "model.safetensors: 100%|█████████████████████| 548M/548M [00:08<00:00, 68.0MB/s]\n",
      "generation_config.json: 100%|██████████████████| 124/124 [00:00<00:00, 1.41MB/s]\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"openai-community/gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 1.1410090923309326 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='bf16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.8270199298858643 seconds\n",
      "From PyTorch loader to NNTile batches in 0.00976252555847168 seconds\n",
      "Params+grads (GB): 0.607\n",
      "Activations  (GB): 5.545\n",
      "Optimizer    (GB): 0.607\n",
      "Persistent   (GB): 6.759\n",
      "Temporaries  (GB): 7.355\n",
      "Batch=1/3 Epoch=1/1 Loss=2.366389036178589\n",
      "Batch=2/3 Epoch=1/1 Loss=2.3126914501190186\n",
      "Batch=3/3 Epoch=1/1 Loss=2.3147048950195312\n",
      "NNTile training time: 14.173014402389526 seconds\n",
      "NNTile training throughput tokens/sec: 55487.98425459936\n",
      "NNTile performance (model flops): 31.70155496973236 Tflops/s\n",
      "NNTile loss on the last batch: 2.3147048950195312\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.3271 GB\t15.5728 MB/s\t(transfers : 402 - avg 0.8333 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.3037 GB\t14.4567 MB/s\t(transfers : 200 - avg 1.5548 MB)\n",
      "Total transfers: 0.6308 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a downloaded from remote source pretrained gpt2 model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=remote --save-checkpoint-path=\".model/nntile_remote_checkpoint.pt\"\\\n",
    "    --optimizer=\"adam\" --lr=1e-4 --dtype=bf16 --nepochs=1 --batch-size=256 --minibatch-size=8 \\\n",
    "    --dataset-file=\"tinystories/train.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1453e4d-04eb-4482-afb9-9a3e18a7a9d0",
   "metadata": {},
   "source": [
    "### 4. Inference process.\n",
    "\n",
    "In the current version of the GPT2 scenario, the NNTile framework model is created from a (pre-)loaded pre-trained GPT2 model from the Huggingface library. The model layer weights are passed to the corresponding NNTile model layers, and then the inference process is performed solely by NNTile, without any involvement of third-party models and mechanisms. To perform the inference, we use another program file - \"gpt2_generate.py\". The program code supports the following arguments when running:\n",
    "\n",
    "- cache_dir, (`str`, default=\"cache_hf\"): path to the directory where previously loaded models from a remote source are saved,\n",
    "- max-seq-len, (`int`, default=1024): maximum length of the input token sequence,\n",
    "- model, (`str`, default=`\"gpt2\"`): name of the model from the HuggingFace framework that will be used to initialize the configuration and initial state of the NNTile model,\n",
    "- restrict, (choices=`[\"cpu\", \"cuda\", None]`, default=`None`): limit on the computing resources used during inference; `\"cpu\"` restricts inference to CPU cores only, `\"cuda\"` - to GPU cores only, while None allows using all available cores,\n",
    "- prompt, (`str`, default=`\"What do you think about dogs?\"`): input query, a string fed to the model input to perform inference based on it,\n",
    "- generation-mode, (choices = `[\"Greedy\", \"TopK\", \"TopP\"]`, default=`\"Greedy\"`): token generation mode in the GenerationMode class object (described in the \"llm_params.py\" file),\n",
    "- parallel-sampling-mode, (choices=`[\"BeamSearch\", \"Parallel\"]`, default=`\"BeamSearch\"`): parallel generation mode for multiple responses to a single query in the ParallelSamplingMode class object (described in the \"llm_params.py\" file),\n",
    "- max-tokens, (`int`, default=`100`): maximum number of generated tokens, including user request tokens,\n",
    "- use-cache, (action=`\"store_true\"`): boolean flag, when used in the argument line, enables the use of KV caches, allowing to reuse previously calculated values,\n",
    "- top-k, (`int`, default=`None`): probabilistic selection based on the top-k most probable tokens,\n",
    "- top-p-thr, (`float`, default=`None`): probabilistic selection based on tokens whose probability is not lower than the top-p-thr threshold,\n",
    "- temperature, (`float`, default=`1.0`): \"temperature\" parameter for token generation,\n",
    "- num-beams, (`int`, default=`1`): number of beams for parallel generation mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd29a7-1a3a-49c5-b1ce-3ea14e0f3af8",
   "metadata": {},
   "source": [
    "#### Examples with different types of generation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0dbf5c-e318-4fa2-976d-fa27840af255",
   "metadata": {},
   "source": [
    "`BeamSearch` generation strategy and number of beams set to the default value of `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86cf54b-64af-4a88-85cb-6a31e75e86af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "tokenizer_config.json: 100%|██████████████████| 26.0/26.0 [00:00<00:00, 232kB/s]\n",
      "vocab.json: 100%|██████████████████████████| 1.04M/1.04M [00:00<00:00, 4.37MB/s]\n",
      "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 3.78MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 2.95MB/s]\n",
      "config.json: 100%|█████████████████████████████| 665/665 [00:00<00:00, 8.83MB/s]\n",
      "model.safetensors: 100%|██████████████████████| 548M/548M [00:05<00:00, 108MB/s]\n",
      "generation_config.json: 100%|██████████████████| 124/124 [00:00<00:00, 1.10MB/s]\n",
      "Why does the Sun shine?\n",
      "\n",
      "The Sun is the most visible star in the sky. It\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t46.9551 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t0.2746 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b842e7-1a48-4beb-9774-6179c3b98f3e",
   "metadata": {},
   "source": [
    "`Parallel` generation strategy and number of beams set to `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc5496e-9488-4653-aa53-43e63fae7a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "['Why does the Sun shine? The Sun is the most visible star in the sky, and it is', \"Why does the Sun shine? It's a very important thing to know.\\n\\nThe Sun is\", 'Why does the Sun shine?\\n\\nThe Sun is the most visible star in the sky. It']\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t190.4654 MB/s\t(transfers : 280 - avg 2.2248 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0084 GB\t2.6379 MB/s\t(transfers : 42 - avg 0.2054 MB)\n",
      "Total transfers: 0.6168 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --num-beams=3 --parallel-sampling-mode=Parallel \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd60b5-f101-4b2d-a2d1-4911733653e3",
   "metadata": {},
   "source": [
    "`BeamSearch` generation strategy and number of beams set to `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1870d46c-e902-4f17-8b59-177653e9a4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "['Why does the Sun shine?\\n\\nThe Sun is the brightest star in the sky, and it', \"Why does the Sun shine?\\n\\nThe Sun is the brightest star in the sky. It's\", 'Why does the Sun shine?\\n\\nThe Sun is the brightest star in the sky. It is']\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t194.7430 MB/s\t(transfers : 280 - avg 2.2248 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0084 GB\t2.6971 MB/s\t(transfers : 42 - avg 0.2054 MB)\n",
      "Total transfers: 0.6168 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --num-beams=3 --parallel-sampling-mode=BeamSearch \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2207154-a26e-48f5-bbd9-844750fa80ef",
   "metadata": {},
   "source": [
    "#### Examples with different token generation modes and temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8e5b6-21e8-45c6-bcbc-6242076591f5",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with default temperature value of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4034bfc-b5b5-4206-9d82-e2e9b47d8a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine?\n",
      "\n",
      "The Sun shines through our solar system every day, so if\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t237.9439 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.3914 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d665b83-f7d3-4024-999b-2e981ed7d6e9",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with the temperature value of `100.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "145b3f56-59ee-4281-a889-8e83aca3f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine? It's like seeing some sun and having it come out of space.\"\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t207.4208 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.2129 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 --temperature=100.0 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14970f7-fb3e-44f7-9488-3bcd54326a05",
   "metadata": {},
   "source": [
    "`TopK` token generation strategy with the temperature value of `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f3912bc-d552-40f2-880c-2fa465705408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine?\n",
      "\n",
      "The Sun is the most visible star in the sky. It\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t189.8001 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.1099 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopK --top-k=10 --temperature=0.01 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa3110-988d-4eea-b567-b662c62764ce",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with default temperature value of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b083a8d8-6f19-4660-91b3-423a505d2e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine?\n",
      "\n",
      "In South Africa, at least half the country's sun is\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t224.9703 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.3155 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbf4ec-96b9-488f-b368-270b96feccf7",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with the temperature value of `100.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dc76f11-3018-4bde-b5f0-611106b5be5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine? Wilji Runrams calculates Team Shinyeness superiority responses 1989 century PS predicting\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t256.3075 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.4988 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 --temperature=100.0 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5130895-3acf-46b6-abbc-047c662401d5",
   "metadata": {},
   "source": [
    "`TopP` token generation strategy with the temperature value of `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f56cd2-387a-4a4a-a94a-b22766b370ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice:\n",
      " None\n",
      "Why does the Sun shine?\n",
      "\n",
      "The Sun is the most visible star in the sky. It\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6083 GB\t221.5020 MB/s\t(transfers : 229 - avg 2.7203 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0036 GB\t1.2953 MB/s\t(transfers : 16 - avg 0.2277 MB)\n",
      "Total transfers: 0.6119 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../wrappers/python/examples/gpt2_generate.py --cache_dir=.model --max-seq-len=512 \\\n",
    "    --model=gpt2 --restrict=cuda --use-cache \\\n",
    "    --generation-mode=TopP --top-p=0.1 --temperature=0.01 \\\n",
    "    --prompt=\"Why does the Sun shine?\" \\\n",
    "    --max-tokens=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1e31a",
   "metadata": {},
   "source": [
    "### 5. When GPU VRAM is not enough: offloading data from GPU VRAM to CPU RAM in advance\n",
    "Unfortunately, current state of NNTile does not automatically define which data shall be offloaded from GPU VRAM to CPU RAM. Instead, a user shall mark each NNTile tensor as either \"forced to be offloaded in advance to RAM after each update\" or \"not offloaded in advance at all\". For simplicity, the following flags control this option:\n",
    "\n",
    "- force-offload-ram-portion-parameters, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of parameters to be forced for offloading. All the rest parameters will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-gradients, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of gradients to be forced for offloading. All the rest gradients will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-activations, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of inter-layer activations to be forced for offloading. All the rest inter-layer activations will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-temporaries, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of temporary buffers to be forced for offloading. All the rest temporary buffers will not be offloaded in advance at all.\n",
    "\n",
    "- force-offload-ram-portion-optimizer, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of optimizer states to be forced for offloading. All the rest parameter states will not be offloaded in advance at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be88b2",
   "metadata": {},
   "source": [
    "#### 5.1 Training experiment with no enforced data offloading from GPU VRAM to CPU RAM\n",
    "Amount of data transfers is the following:\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.7140 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t0.6074 GB\t23.7955 MB/s\t(transfers : 200 - avg 3.1097 MB)\n",
    "Total transfers: 1.2382 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccadb927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 1.0836024284362793 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.662883996963501 seconds\n",
      "From PyTorch loader to NNTile batches in 0.010235786437988281 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.002052307128906\n",
      "Batch=2/3 Epoch=1/1 Loss=9.629499435424805\n",
      "Batch=3/3 Epoch=1/1 Loss=9.322103500366211\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 3 - avg 0.0000 MB)\n",
      "Total transfers: 0.0000 GB\n",
      "#---------------------\n",
      "NNTile training time: 18.604122400283813 seconds\n",
      "NNTile training throughput tokens/sec: 42271.92140963353\n",
      "NNTile performance (model flops): 24.150915882885485 Tflops/s\n",
      "NNTile loss on the last batch: 9.322103500366211\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t95.1205 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef702b76",
   "metadata": {},
   "source": [
    "#### 5.2 Training experiment with enforced GPU->CPU offloading after each update for all parameters\n",
    "The experiment is done with `--force-offload-ram-portion-parameters=1.0` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.7082 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t2.4294 GB\t95.1594 MB/s\t(transfers : 791 - avg 3.1451 MB)\n",
    "Total transfers: 3.0602 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc9cc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=1.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9894649982452393 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.8217501640319824 seconds\n",
      "From PyTorch loader to NNTile batches in 0.010732650756835938 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.906524658203125\n",
      "Batch=2/3 Epoch=1/1 Loss=9.594470977783203\n",
      "Batch=3/3 Epoch=1/1 Loss=9.266134262084961\n",
      "NNTile training time: 18.845014572143555 seconds\n",
      "NNTile training throughput tokens/sec: 41731.56762438874\n",
      "NNTile performance (model flops): 23.842199402080535 Tflops/s\n",
      "NNTile loss on the last batch: 9.266134262084961\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t23.4797 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t2.4294 GB\t90.4279 MB/s\t(transfers : 791 - avg 3.1451 MB)\n",
      "Total transfers: 3.0602 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=1.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5e8bb",
   "metadata": {},
   "source": [
    "#### 5.3 Training experiment with enforced GPU->CPU offloading after each update for all optimizer states\n",
    "The experiment is done with `--force-offload-ram-portion-optimizer=1.0` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.1350 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t4.2515 GB\t162.6655 MB/s\t(transfers : 1382 - avg 3.1502 MB)\n",
    "Total transfers: 4.8823 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc553f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=1.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9605734348297119 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.6457819938659668 seconds\n",
      "From PyTorch loader to NNTile batches in 0.010062456130981445 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.994535446166992\n",
      "Batch=2/3 Epoch=1/1 Loss=9.647612571716309\n",
      "Batch=3/3 Epoch=1/1 Loss=9.31385612487793\n",
      "NNTile training time: 19.136734008789062 seconds\n",
      "NNTile training throughput tokens/sec: 41095.41364993681\n",
      "NNTile performance (model flops): 23.478750081273212 Tflops/s\n",
      "NNTile loss on the last batch: 9.31385612487793\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.1350 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t4.2515 GB\t162.6655 MB/s\t(transfers : 1382 - avg 3.1502 MB)\n",
      "Total transfers: 4.8823 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8add2",
   "metadata": {},
   "source": [
    "#### 5.4 Training experiment with enforced GPU->CPU offloading after each update for all gradients\n",
    "The experiment is done with `--force-offload-ram-portion-gradients=1.0` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.5453 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t60.7361 GB\t2363.2946 MB/s\t(transfers : 19703 - avg 3.1566 MB)\n",
    "Total transfers: 61.3669 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117b0351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=1.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 1.035834550857544 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7969074249267578 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008589982986450195 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.981538772583008\n",
      "Batch=2/3 Epoch=1/1 Loss=9.563827514648438\n",
      "Batch=3/3 Epoch=1/1 Loss=9.213329315185547\n",
      "NNTile training time: 18.863643169403076 seconds\n",
      "NNTile training throughput tokens/sec: 41690.356042972475\n",
      "NNTile performance (model flops): 23.818654282696436 Tflops/s\n",
      "NNTile loss on the last batch: 9.213329315185547\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.5453 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t60.7361 GB\t2363.2946 MB/s\t(transfers : 19703 - avg 3.1566 MB)\n",
      "Total transfers: 61.3669 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=1.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2c195",
   "metadata": {},
   "source": [
    "#### 5.5 Training experiment with enforced GPU->CPU offloading after each update for 10% of inter-layer activations\n",
    "The experiment is done with `--force-offload-ram-portion-activations=0.1` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t21.9020 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t98.7695 GB\t3429.3419 MB/s\t(transfers : 3752 - avg 26.9563 MB)\n",
    "Total transfers: 99.4003 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db3c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.1, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 1.078826665878296 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7700309753417969 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008510112762451172 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.956888198852539\n",
      "Batch=2/3 Epoch=1/1 Loss=9.656490325927734\n",
      "Batch=3/3 Epoch=1/1 Loss=9.33749771118164\n",
      "NNTile training time: 21.75592350959778 seconds\n",
      "NNTile training throughput tokens/sec: 36147.94838072767\n",
      "NNTile performance (model flops): 20.65214997496867 Tflops/s\n",
      "NNTile loss on the last batch: 9.33749771118164\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t21.9020 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t98.7695 GB\t3429.3419 MB/s\t(transfers : 3752 - avg 26.9563 MB)\n",
      "Total transfers: 99.4003 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.1 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc7baf",
   "metadata": {},
   "source": [
    "#### 5.6 Training experiment with enforced GPU->CPU offloading after each update for 10% of intra-layer temporary buffers\n",
    "The experiment is done with `--force-offload-ram-portion-temporaries=0.1` option. Amount of data transfers is the following:\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t15.7608 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t342.8271 GB\t8565.5885 MB/s\t(transfers : 6440 - avg 54.5116 MB)\n",
    "Total transfers: 343.4579 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1aebac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.1, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.9824521541595459 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.6945712566375732 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008262872695922852 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.977161407470703\n",
      "Batch=2/3 Epoch=1/1 Loss=9.625722885131836\n",
      "Batch=3/3 Epoch=1/1 Loss=9.308215141296387\n",
      "NNTile training time: 33.67155838012695 seconds\n",
      "NNTile training throughput tokens/sec: 23355.972750704474\n",
      "NNTile performance (model flops): 13.343801617134002 Tflops/s\n",
      "NNTile loss on the last batch: 9.308215141296387\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t15.7608 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t342.8271 GB\t8565.5885 MB/s\t(transfers : 6440 - avg 54.5116 MB)\n",
      "Total transfers: 343.4579 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.1 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9fd735",
   "metadata": {},
   "source": [
    "#### 5.7.1 Training experiment with limited GPU memory without forced offloading in advance\n",
    "GPU memory is limited to 19000 MB. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t6.2240 GB\t309.4812 MB/s\t(transfers : 1118 - avg 5.7007 MB)\n",
    "\tCUDA 0 -> NUMA 0\t12.3630 GB\t614.7371 MB/s\t(transfers : 1195 - avg 10.5939 MB)\n",
    "Total transfers: 18.5870 GB\n",
    "#---------------------\n",
    "NNTile training time: 20.6288743019104 seconds\n",
    "NNTile training throughput tokens/sec: 38122.87517439427\n",
    "NNTile performance (model flops): 21.780470838515438 Tflops/s\n",
    "NNTile loss on the last batch: 9.298209190368652\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cde3b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=4, hidden_size_tile=-1, intermediate_size_tile=768, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 1.0536348819732666 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=768, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.9552161693572998 seconds\n",
      "From PyTorch loader to NNTile batches in 0.02664923667907715 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.96198844909668\n",
      "Batch=2/3 Epoch=1/1 Loss=9.66005802154541\n",
      "Batch=3/3 Epoch=1/1 Loss=9.338000297546387\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t3.0884 GB\t141.3378 MB/s\t(transfers : 1511 - avg 2.0930 MB)\n",
      "\tCUDA 0 -> NUMA 0\t7.6937 GB\t352.0919 MB/s\t(transfers : 1659 - avg 4.7489 MB)\n",
      "Total transfers: 10.7822 GB\n",
      "#---------------------\n",
      "NNTile training time: 22.451163291931152 seconds\n",
      "NNTile training throughput tokens/sec: 35028.56354363785\n",
      "NNTile performance (model flops): 20.012619806014182 Tflops/s\n",
      "NNTile loss on the last batch: 9.338000297546387\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t87.4673 MB/s\t(transfers : 197 - avg 3.1570 MB)\n",
      "Total transfers: 0.6074 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --minibatch-size-tile=4 --intermediate-size-tile=768 \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74459bb1",
   "metadata": {},
   "source": [
    "#### 5.7.2 Training experiment with limited GPU memory with forced offloading in advance\n",
    "GPU memory is limited to 19000 MB. This experiment only enables forced offloading in advance of parameters. Training time and data transfers are reported below:\n",
    "```sh\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t1.9116 GB\t100.8448 MB/s\t(transfers : 640 - avg 3.0585 MB)\n",
    "\tCUDA 0 -> NUMA 0\t3.4945 GB\t184.3536 MB/s\t(transfers : 1182 - avg 3.0274 MB)\n",
    "Total transfers: 5.4061 GB\n",
    "#---------------------\n",
    "NNTile training time: 19.451268672943115 seconds\n",
    "NNTile training throughput tokens/sec: 40430.8846493871\n",
    "NNTile performance (model flops): 23.09908945883563 Tflops/s\n",
    "NNTile loss on the last batch: 9.346678733825684\n",
    "\n",
    "#---------------------\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t1.8368 GB\t98.1068 MB/s\t(transfers : 598 - avg 3.1452 MB)\n",
    "\tCUDA 0 -> NUMA 0\t3.4945 GB\t186.6519 MB/s\t(transfers : 1182 - avg 3.0274 MB)\n",
    "Total transfers: 5.3313 GB\n",
    "#---------------------\n",
    "NNTile training time: 19.222965478897095 seconds\n",
    "NNTile training throughput tokens/sec: 40911.06550981129\n",
    "NNTile performance (model flops): 23.373427770934054 Tflops/s\n",
    "NNTile loss on the last batch: 9.327847480773926\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6fee699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=4, hidden_size_tile=-1, intermediate_size_tile=768, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=False, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, force_offload_disk_portion_parameters=0.0, force_offload_disk_portion_gradients=0.0, force_offload_disk_portion_activations=0.0, force_offload_disk_portion_temporaries=0.0, force_offload_disk_portion_optimizer=0.0, force_offload_ram_portion_parameters=0.0, force_offload_ram_portion_gradients=0.0, force_offload_ram_portion_activations=0.0, force_offload_ram_portion_temporaries=0.0, force_offload_ram_portion_optimizer=1.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x1031a5)[0x7f54789871a5]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0xa375e)[0x7f547892775e]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0xa5d8c)[0x7f5478929d8c]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x124eb4)[0x7f54789a8eb4]\n",
      "../../install/starpu-1.4-9b274af/lib/libstarpu-1.4.so.6(+0x1255dd)[0x7f54789a95dd]\n",
      "/usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3)[0x7f54a7597ac3]\n",
      "/usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850)[0x7f54a7629850]\n",
      "StarPU + NNTile + MPI init in 1.070171594619751 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=768, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.819854736328125 seconds\n",
      "From PyTorch loader to NNTile batches in 0.02263665199279785 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.007906913757324\n",
      "Batch=2/3 Epoch=1/1 Loss=9.652756690979004\n",
      "Batch=3/3 Epoch=1/1 Loss=9.367780685424805\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t2.8800 GB\t132.5622 MB/s\t(transfers : 1523 - avg 1.9364 MB)\n",
      "\tCUDA 0 -> NUMA 0\t7.7834 GB\t358.2653 MB/s\t(transfers : 2275 - avg 3.5034 MB)\n",
      "Total transfers: 10.6634 GB\n",
      "#---------------------\n",
      "NNTile training time: 22.30956220626831 seconds\n",
      "NNTile training throughput tokens/sec: 35250.89343882492\n",
      "NNTile performance (model flops): 20.139641962042557 Tflops/s\n",
      "NNTile loss on the last batch: 9.367780685424805\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.7570 GB\t110.9258 MB/s\t(transfers : 200 - avg 3.8759 MB)\n",
      "Total transfers: 0.7570 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!STARPU_LIMIT_CUDA_MEM=19000 python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" \\\n",
    "    --minibatch-size-tile=4 --intermediate-size-tile=768 \\\n",
    "    --force-offload-ram-portion-parameters=0.0 --force-offload-ram-portion-gradients=0.0 \\\n",
    "    --force-offload-ram-portion-activations=0.0 --force-offload-ram-portion-temporaries=0.0 \\\n",
    "    --force-offload-ram-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32362c03",
   "metadata": {},
   "source": [
    "### 6. Training with Out-of-Core support (when there is not enough memory within CPU RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2893b",
   "metadata": {},
   "source": [
    "To train with OOC (Out-of-Core) support, parameters `--ooc`, `--ooc-path`, `--ooc-force-portion-parameters`, `--ooc-force-portion-gradients`, `--ooc-force-portion-activations`, `--ooc-force-portion-temporaries` and `--ooc-force-portion-optimizer`. These parameters have the following meaning:\n",
    "- ooc, (action=`\"store_true\"`): a boolean flag that enables Out-of-Core support to offload data from CPU RAM to path on a hard drive.\n",
    "\n",
    "- ooc-path, (default=`\"/tmp/nntile_ooc\"`): a path on hard drive, where all the temporary buffers shall be stored.\n",
    "\n",
    "- ooc-size, (`int`, default=`1073741824`): number of bytes, that limits temporary buffers size on the hard drive.\n",
    "\n",
    "- ooc-force-portion-parameters, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of parameters to have Out-of-Core support enabled. All the rest parameters will have their Out-of-Core support disabled.\n",
    "\n",
    "- ooc-force-portion-gradients, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of gradients to have Out-of-Core support enabled. All the rest gradients will have their Out-of-Core support disabled.\n",
    "\n",
    "- ooc-force-portion-activations, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of inter-layer activations to have Out-of-Core support enabled. All the rest inter-layer activations will have their Out-of-Core support disabled.\n",
    "\n",
    "- ooc-force-portion-temporaries, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of temporary buffers to have Out-of-Core support enabled. All the rest temporary buffers will have their Out-of-Core support disabled.\n",
    "\n",
    "- ooc-force-portion-optimizer, (`float`, default=`0.0`): a float value between 0.0 and 1.0, indicating a portion of optimizer states to have Out-of-Core support enabled. All the rest parameter states will have their Out-of-Core support disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfb221",
   "metadata": {},
   "source": [
    "The first example shows how to train with OOC enabled, but no OOC support for data. Additional output prints bandwidth and latency of the chosen hard drive:\n",
    "```sh\n",
    "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
    "0 -> 2: 1345 MB/s\n",
    "1 -> 2: 1277 MB/s\n",
    "2 -> 0: 1345 MB/s\n",
    "2 -> 1: 1275 MB/s\n",
    "0 -> 2: 32 us\n",
    "1 -> 2: 42 us\n",
    "2 -> 0: 32 us\n",
    "2 -> 1: 42 us\n",
    "```\n",
    "Memory nodes:\n",
    "- `0`: CPU RAM\n",
    "- `1`: GPU VRAM\n",
    "- `2`: Hard drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c86bc112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, ooc_force_portion_parameters=0.0, ooc_force_portion_gradients=0.0, ooc_force_portion_activations=0.0, ooc_force_portion_temporaries=0.0, ooc_force_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1345 MB/s\n",
      "1 -> 2: 1277 MB/s\n",
      "2 -> 0: 1345 MB/s\n",
      "2 -> 1: 1275 MB/s\n",
      "0 -> 2: 32 us\n",
      "1 -> 2: 42 us\n",
      "2 -> 0: 32 us\n",
      "2 -> 1: 42 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.2428526878356934 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.6515815258026123 seconds\n",
      "From PyTorch loader to NNTile batches in 0.008284568786621094 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.001976013183594\n",
      "Batch=2/3 Epoch=1/1 Loss=9.62998104095459\n",
      "Batch=3/3 Epoch=1/1 Loss=9.247664451599121\n",
      "NNTile training time: 18.742900133132935 seconds\n",
      "NNTile training throughput tokens/sec: 41958.928149533145\n",
      "NNTile performance (model flops): 23.97209567210435 Tflops/s\n",
      "NNTile loss on the last batch: 9.247664451599121\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.7550 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t0.6074 GB\t23.8349 MB/s\t(transfers : 200 - avg 3.1097 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "Total transfers: 1.2382 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" --ooc --ooc-path=\"/tmp/nntile_ooc\" \\\n",
    "    --ooc-force-portion-parameters=0.0 --ooc-force-portion-gradients=0.0 --ooc-force-portion-activations=0.0 \\\n",
    "    --ooc-force-portion-temporaries=0.0 --ooc-force-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04532667",
   "metadata": {},
   "source": [
    "The second example enables only parameters (`--ooc-force-portion-parameters=1.0`) to be offloaded from GPU VRAM to Hard Drive after every optimizer step, which is visible at `CUDA 0 -> NUMA 0` and `NUMA 0 -> Disk 0` data transfers:\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.3202 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t3.0368 GB\t117.0815 MB/s\t(transfers : 988 - avg 3.1475 MB)\n",
    "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
    "\tNUMA 0 -> Disk 0\t2.4294 GB\t93.7337 MB/s\t(transfers : 788 - avg 3.1570 MB)\n",
    "Total transfers: 6.0971 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0233f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, ooc_force_portion_parameters=1.0, ooc_force_portion_gradients=0.0, ooc_force_portion_activations=0.0, ooc_force_portion_temporaries=0.0, ooc_force_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1306 MB/s\n",
      "1 -> 2: 1241 MB/s\n",
      "2 -> 0: 1306 MB/s\n",
      "2 -> 1: 1239 MB/s\n",
      "0 -> 2: 31 us\n",
      "1 -> 2: 41 us\n",
      "2 -> 0: 31 us\n",
      "2 -> 1: 41 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.1533846855163574 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.6512961387634277 seconds\n",
      "From PyTorch loader to NNTile batches in 0.01000356674194336 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.985692977905273\n",
      "Batch=2/3 Epoch=1/1 Loss=9.646240234375\n",
      "Batch=3/3 Epoch=1/1 Loss=9.331768035888672\n",
      "NNTile training time: 19.04175090789795 seconds\n",
      "NNTile training throughput tokens/sec: 41300.403718326736\n",
      "NNTile performance (model flops): 23.59586559751714 Tflops/s\n",
      "NNTile loss on the last batch: 9.331768035888672\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t24.3202 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t3.0368 GB\t117.0815 MB/s\t(transfers : 988 - avg 3.1475 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t2.4294 GB\t93.7337 MB/s\t(transfers : 788 - avg 3.1570 MB)\n",
      "Total transfers: 6.0971 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" --ooc --ooc-path=\"/tmp/nntile_ooc\" \\\n",
    "    --ooc-force-portion-parameters=1.0 --ooc-force-portion-gradients=0.0 --ooc-force-portion-activations=0.0 \\\n",
    "    --ooc-force-portion-temporaries=0.0 --ooc-force-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8e254",
   "metadata": {},
   "source": [
    "The third example enables only optimizer states (`--ooc-force-portion-optimizer=1.0`) to be offloaded from GPU VRAM to Hard Drive after every optimizer step.\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t23.8268 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t4.2515 GB\t160.5885 MB/s\t(transfers : 1382 - avg 3.1502 MB)\n",
    "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
    "\tNUMA 0 -> Disk 0\t3.6442 GB\t137.7531 MB/s\t(transfers : 1182 - avg 3.1570 MB)\n",
    "Total transfers: 8.5265 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4735c137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, ooc_force_portion_parameters=0.0, ooc_force_portion_gradients=0.0, ooc_force_portion_activations=0.0, ooc_force_portion_temporaries=0.0, ooc_force_portion_optimizer=1.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1284 MB/s\n",
      "1 -> 2: 1222 MB/s\n",
      "2 -> 0: 1284 MB/s\n",
      "2 -> 1: 1220 MB/s\n",
      "0 -> 2: 31 us\n",
      "1 -> 2: 40 us\n",
      "2 -> 0: 31 us\n",
      "2 -> 1: 41 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.2453739643096924 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7874770164489746 seconds\n",
      "From PyTorch loader to NNTile batches in 0.012087345123291016 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.95427131652832\n",
      "Batch=2/3 Epoch=1/1 Loss=9.603293418884277\n",
      "Batch=3/3 Epoch=1/1 Loss=9.282905578613281\n",
      "NNTile training time: 19.455347061157227 seconds\n",
      "NNTile training throughput tokens/sec: 40422.4091982465\n",
      "NNTile performance (model flops): 23.094247239680683 Tflops/s\n",
      "NNTile loss on the last batch: 9.282905578613281\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t23.8268 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t4.2515 GB\t160.5885 MB/s\t(transfers : 1382 - avg 3.1502 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t3.6442 GB\t137.7531 MB/s\t(transfers : 1182 - avg 3.1570 MB)\n",
      "Total transfers: 8.5265 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" --ooc --ooc-path=\"/tmp/nntile_ooc\" \\\n",
    "    --ooc-force-portion-parameters=0.0 --ooc-force-portion-gradients=0.0 --ooc-force-portion-activations=0.0 \\\n",
    "    --ooc-force-portion-temporaries=0.0 --ooc-force-portion-optimizer=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf83af7",
   "metadata": {},
   "source": [
    "The 4th example enables only gradients (`--ooc-force-portion-gradients=1.0`) to be offloaded from GPU VRAM to Hard Drive after every minibatch.\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t19.9820 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t58.9140 GB\t1866.2146 MB/s\t(transfers : 19112 - avg 3.1565 MB)\n",
    "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
    "\tNUMA 0 -> Disk 0\t58.3066 GB\t1848.1129 MB/s\t(transfers : 18912 - avg 3.1570 MB)\n",
    "Total transfers: 117.8514 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "131c508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, ooc_force_portion_parameters=0.0, ooc_force_portion_gradients=1.0, ooc_force_portion_activations=0.0, ooc_force_portion_temporaries=0.0, ooc_force_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1344 MB/s\n",
      "1 -> 2: 1276 MB/s\n",
      "2 -> 0: 1344 MB/s\n",
      "2 -> 1: 1274 MB/s\n",
      "0 -> 2: 30 us\n",
      "1 -> 2: 39 us\n",
      "2 -> 0: 30 us\n",
      "2 -> 1: 40 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.17750883102417 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7038607597351074 seconds\n",
      "From PyTorch loader to NNTile batches in 0.013973236083984375 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=10.963013648986816\n",
      "Batch=2/3 Epoch=1/1 Loss=9.622785568237305\n",
      "Batch=3/3 Epoch=1/1 Loss=9.291943550109863\n",
      "NNTile training time: 24.129664659500122 seconds\n",
      "NNTile training throughput tokens/sec: 32591.915847051478\n",
      "NNTile performance (model flops): 18.62050722645509 Tflops/s\n",
      "NNTile loss on the last batch: 9.291943550109863\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t19.9820 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t58.9140 GB\t1866.2146 MB/s\t(transfers : 19112 - avg 3.1565 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t58.3066 GB\t1848.1129 MB/s\t(transfers : 18912 - avg 3.1570 MB)\n",
      "Total transfers: 117.8514 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" --ooc --ooc-path=\"/tmp/nntile_ooc\" \\\n",
    "    --ooc-force-portion-parameters=0.0 --ooc-force-portion-gradients=1.0 --ooc-force-portion-activations=0.0 \\\n",
    "    --ooc-force-portion-temporaries=0.0 --ooc-force-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ff160",
   "metadata": {},
   "source": [
    "The 5th example enables only 10% of inter-layer activations (`--ooc-force-portion-activations=0.1`) to be offloaded from GPU VRAM to Hard Drive after every minibatch.\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t20.6690 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t52.9257 GB\t1734.1639 MB/s\t(transfers : 2121 - avg 25.5521 MB)\n",
    "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
    "\tNUMA 0 -> Disk 0\t52.3184 GB\t1715.4471 MB/s\t(transfers : 1921 - avg 27.8886 MB)\n",
    "Total transfers: 105.8749 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a832873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, ooc_force_portion_parameters=0.0, ooc_force_portion_gradients=0.0, ooc_force_portion_activations=0.1, ooc_force_portion_temporaries=0.0, ooc_force_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1249 MB/s\n",
      "1 -> 2: 1190 MB/s\n",
      "2 -> 0: 1249 MB/s\n",
      "2 -> 1: 1188 MB/s\n",
      "0 -> 2: 30 us\n",
      "1 -> 2: 39 us\n",
      "2 -> 0: 30 us\n",
      "2 -> 1: 39 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.2345759868621826 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.7290642261505127 seconds\n",
      "From PyTorch loader to NNTile batches in 0.010792970657348633 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.004066467285156\n",
      "Batch=2/3 Epoch=1/1 Loss=9.642704963684082\n",
      "Batch=3/3 Epoch=1/1 Loss=9.308977127075195\n",
      "NNTile training time: 23.77764582633972 seconds\n",
      "NNTile training throughput tokens/sec: 33074.42653253876\n",
      "NNTile performance (model flops): 18.896176620918457 Tflops/s\n",
      "NNTile loss on the last batch: 9.308977127075195\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t20.6690 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t52.9257 GB\t1734.1639 MB/s\t(transfers : 2121 - avg 25.5521 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t52.3184 GB\t1715.4471 MB/s\t(transfers : 1921 - avg 27.8886 MB)\n",
      "Total transfers: 105.8749 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" --ooc --ooc-path=\"/tmp/nntile_ooc\" \\\n",
    "    --ooc-force-portion-parameters=0.0 --ooc-force-portion-gradients=0.0 --ooc-force-portion-activations=0.1 \\\n",
    "    --ooc-force-portion-temporaries=0.0 --ooc-force-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c79694",
   "metadata": {},
   "source": [
    "The 6th example enables only 10% of temporary buffers (`--ooc-force-portion-temporaries=0.1`) to be offloaded from GPU VRAM to Hard Drive after every minibatch.\n",
    "```sh\n",
    "Data transfer stats:\n",
    "\tNUMA 0 -> CUDA 0\t0.6308 GB\t21.2224 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
    "\tCUDA 0 -> NUMA 0\t54.6259 GB\t1837.7930 MB/s\t(transfers : 1641 - avg 34.0871 MB)\n",
    "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
    "\tNUMA 0 -> Disk 0\t54.0186 GB\t1818.7304 MB/s\t(transfers : 1441 - avg 38.3865 MB)\n",
    "Total transfers: 109.2753 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d16fd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(remote_model_name='openai-community/gpt2', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/gpt2_default_config.json', save_checkpoint_path='.model/nntile_checkpoint.pt', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=256, minibatch_size=8, minibatch_size_tile=-1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='fp32_fast_fp16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001, ooc=True, ooc_path='/tmp/nntile_ooc', ooc_size=1073741824, ooc_force_portion_parameters=0.0, ooc_force_portion_gradients=0.0, ooc_force_portion_activations=0.0, ooc_force_portion_temporaries=0.1, ooc_force_portion_optimizer=0.0)\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "#---------------------\n",
      "Data transfer speed for /tmp/nntile_ooc (node 2):\n",
      "0 -> 2: 1145 MB/s\n",
      "1 -> 2: 1095 MB/s\n",
      "2 -> 0: 1145 MB/s\n",
      "2 -> 1: 1094 MB/s\n",
      "0 -> 2: 39 us\n",
      "1 -> 2: 49 us\n",
      "2 -> 0: 39 us\n",
      "2 -> 1: 49 us\n",
      "\n",
      "#---------------------\n",
      "StarPU + NNTile + MPI init in 1.29323410987854 seconds\n",
      "GPT2ConfigNNTile(vocab_size=50257, vocab_embed_dim_tile=768, hidden_size=768, hidden_size_tile=768, intermediate_size=3072, intermediate_size_tile=3072, n_head=12, n_head_tile=12, activation_function='gelutanh', dtype='fp32_fast_fp16', flash_attention=False, layer_norm_epsilon=1e-05, max_position_embeddings=1024, num_hidden_layers=12, redux=False, eos_token_id=50256, name='gpt2')\n",
      "Converting PyTorch model to NNTile requires 0.5963935852050781 seconds\n",
      "From PyTorch loader to NNTile batches in 0.010392427444458008 seconds\n",
      "Params+grads (GB): 1.215\n",
      "Activations  (GB): 11.089\n",
      "Optimizer    (GB): 1.215\n",
      "Persistent   (GB): 13.518\n",
      "Temporaries  (GB): 14.698\n",
      "Batch=1/3 Epoch=1/1 Loss=11.027915954589844\n",
      "Batch=2/3 Epoch=1/1 Loss=9.564784049987793\n",
      "Batch=3/3 Epoch=1/1 Loss=9.196428298950195\n",
      "NNTile training time: 22.871919631958008 seconds\n",
      "NNTile training throughput tokens/sec: 34384.171186975946\n",
      "NNTile performance (model flops): 19.644463708956117 Tflops/s\n",
      "NNTile loss on the last batch: 9.196428298950195\n",
      "\n",
      "#---------------------\n",
      "Data transfer stats:\n",
      "\tNUMA 0 -> CUDA 0\t0.6308 GB\t21.2224 MB/s\t(transfers : 402 - avg 1.6068 MB)\n",
      "\tCUDA 0 -> NUMA 0\t54.6259 GB\t1837.7930 MB/s\t(transfers : 1641 - avg 34.0871 MB)\n",
      "\tDisk 0 -> NUMA 0\t0.0000 GB\t0.0000 MB/s\t(transfers : 0 - avg -nan MB)\n",
      "\tNUMA 0 -> Disk 0\t54.0186 GB\t1818.7304 MB/s\t(transfers : 1441 - avg 38.3865 MB)\n",
      "Total transfers: 109.2753 GB\n",
      "#---------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to train gpt2_lmhead model on TinyStories\n",
    "!python ../wrappers/python/examples/gpt2_lmhead_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/gpt2_default_config.json\" \\\n",
    "    --save-checkpoint-path=\".model/nntile_checkpoint.pt\" --optimizer=\"adam\" --lr=1e-4 --dtype=fp32_fast_fp16 --nepochs=1 \\\n",
    "    --batch-size=256 --minibatch-size=8 --dataset-file=\"tinystories/train.bin\" --ooc --ooc-path=\"/tmp/nntile_ooc\" \\\n",
    "    --ooc-force-portion-parameters=0.0 --ooc-force-portion-gradients=0.0 --ooc-force-portion-activations=0.0 \\\n",
    "    --ooc-force-portion-temporaries=0.1 --ooc-force-portion-optimizer=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05b33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mikhalev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
