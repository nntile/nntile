/*! @copyright (c) 2022-present Skolkovo Institute of Science and Technology
 *                              (Skoltech), Russia. All rights reserved.
 *                 2023-present Artificial Intelligence Research Institute
 *                              (AIRI), Russia. All rights reserved.
 *
 * NNTile is software framework for fast training of big neural networks on
 * distributed-memory heterogeneous systems based on StarPU runtime system.
 *
 * @file include/nntile/kernel/gemm.hh
 * Wrappers for gemm on CPU and CUDA
 *
 * @version 1.1.0
 * */

#pragma once

#include <nntile/base_types.hh>
#include <nntile/defs.h>

#ifdef NNTILE_USE_CBLAS
#    include <@CBLAS_H_NAME@>
#    ifndef CBLAS_INT
#        define CBLAS_INT @CBLAS_INT_TYPE@
#    endif // CBLAS_INT
#endif // NNTILE_USE_CBLAS

#ifdef NNTILE_USE_CUDA
#    include <cublas_v2.h>
#    include <starpu_cublas_v2.h>
//#    include <cuda_fp16.h>
#endif // NNTILE_USE_CUDA

namespace nntile::kernel::gemm
{

#ifdef NNTILE_USE_CBLAS
// Overloaded call to CBLAS GEMM
static inline
void cblas(CBLAS_TRANSPOSE transA, CBLAS_TRANSPOSE transB,
        CBLAS_INT M, CBLAS_INT N, CBLAS_INT K, float alpha, const fp32_t *A,
        CBLAS_INT ldA, const fp32_t *B, CBLAS_INT ldB, float beta, fp32_t *C,
        CBLAS_INT ldC)
    noexcept
{
    cblas_sgemm(CblasColMajor, transA, transB, M, N, K, alpha,
            (const float *)A, ldA, (const float *)B, ldB, beta, (float *)C,
            ldC);
}

// Overloaded call to CBLAS GEMM
static inline
void cblas(CBLAS_TRANSPOSE transA, CBLAS_TRANSPOSE transB,
        CBLAS_INT M, CBLAS_INT N, CBLAS_INT K, double alpha, const fp64_t *A,
        CBLAS_INT ldA, const fp64_t *B, CBLAS_INT ldB, double beta, fp64_t *C,
        CBLAS_INT ldC)
    noexcept
{
    cblas_dgemm(CblasColMajor, transA, transB, M, N, K, alpha,
            (const double *)A, ldA, (const double *)B, ldB, beta, (double *)C,
            ldC);
}
#endif // NNTILE_USE_CBLAS

#ifdef NNTILE_USE_CUDA
// Overloaded call to cuBLAS GEMM
static inline
void cublas(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const fp32_t *A, int ldA, const fp32_t *B, int ldB, float beta,
        fp32_t *C, int ldC)
    noexcept
{
    cublasSgemm(handle, transA, transB, M, N, K, &alpha, (const float *)A,
            ldA, (const float *)B, ldB, &beta, (float *)C, ldC);
}

// Overloaded call to cuBLAS GEMM
static inline
void cublas(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, double alpha,
        const fp64_t *A, int ldA, const fp64_t *B, int ldB, double beta,
        fp64_t *C, int ldC)
    noexcept
{
    cublasDgemm(handle, transA, transB, M, N, K, &alpha, (const double *)A,
            ldA, (const double *)B, ldB, &beta, (double *)C, ldC);
}

// Overloaded call to cuBLAS GEMM for fp32_fast_tf32_t
static inline
void cublas(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const fp32_fast_tf32_t *A, int ldA, const fp32_fast_tf32_t *B, int ldB,
        float beta, fp32_fast_tf32_t *C, int ldC)
    noexcept
{
    cublasGemmEx(handle, transA, transB, M, N, K, &alpha, (const float *)A,
            CUDA_R_32F, ldA, (const float *)B, CUDA_R_32F, ldB, &beta,
            (float *)C, CUDA_R_32F, ldC, CUBLAS_COMPUTE_32F_FAST_TF32,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}

// Overloaded call to cuBLAS GEMM for fp32_fast_fp16_t
static inline
void cublas(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const fp32_fast_fp16_t *A, int ldA, const fp32_fast_fp16_t *B, int ldB,
        float beta, fp32_fast_fp16_t *C, int ldC)
    noexcept
{
    cublasGemmEx(handle, transA, transB, M, N, K, &alpha, (const float *)A,
            CUDA_R_32F, ldA, (const float *)B, CUDA_R_32F, ldB, &beta,
            (float *)C, CUDA_R_32F, ldC, CUBLAS_COMPUTE_32F_FAST_16F,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}

// Overloaded call to cuBLAS GEMM for fp32_fast_bf16_t
static inline
void cublas(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const fp32_fast_bf16_t *A, int ldA, const fp32_fast_bf16_t *B, int ldB,
        float beta, fp32_fast_bf16_t *C, int ldC)
    noexcept
{
    cublasGemmEx(handle, transA, transB, M, N, K, &alpha, (const float *)A,
            CUDA_R_32F, ldA, (const float *)B, CUDA_R_32F, ldB, &beta,
            (float *)C, CUDA_R_32F, ldC, CUBLAS_COMPUTE_32F_FAST_16BF,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}

// Overloaded call to cuBLAS GEMM for bf16_t
static inline
void cublas(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const bf16_t *A, int ldA, const bf16_t *B, int ldB,
        float beta, bf16_t *C, int ldC)
    noexcept
{
    cublasGemmEx(handle, transA, transB, M, N, K, &alpha, (const __nv_bfloat16 *)A,
            CUDA_R_16BF, ldA, (const __nv_bfloat16 *)B, CUDA_R_16BF, ldB, &beta,
            (__nv_bfloat16 *)C, CUDA_R_16BF, ldC, CUBLAS_COMPUTE_32F,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}

// Overloaded call to batched cuBLAS gemm
static inline
void cublas_batch(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const fp32_t *A, int ldA, long long int strideA, const fp32_t *B,
        int ldB, long long int strideB, float beta, fp32_t *C, int ldC,
        long long int strideC, int batchCount)
    noexcept
{
    cublasSgemmStridedBatched(handle, transA, transB, M, N, K, &alpha,
            (const float *)A, ldA, strideA, (const float *)B, ldB, strideB,
            &beta, (float *)C, ldC, strideC, batchCount);
}

// Overloaded call to batched cuBLAS gemm
static inline
void cublas_batch(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, double alpha,
        const fp64_t *A, int ldA, long long int strideA, const fp64_t *B,
        int ldB, long long int strideB, double beta, fp64_t *C, int ldC,
        long long int strideC, int batchCount)
    noexcept
{
    cublasDgemmStridedBatched(handle, transA, transB, M, N, K, &alpha,
            (const double *)A, ldA, strideA, (const double *)B, ldB, strideB,
            &beta, (double *)C, ldC, strideC, batchCount);
}

// Overloaded call to batched cuBLAS gemm
static inline
void cublas_batch(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const fp32_fast_tf32_t *A, int ldA, long long int strideA,
        const fp32_fast_tf32_t *B, int ldB, long long int strideB, float beta,
        fp32_fast_tf32_t *C, int ldC, long long int strideC, int batchCount)
    noexcept
{
    cublasGemmStridedBatchedEx(handle, transA, transB, M, N, K, &alpha,
            (const float *)A, CUDA_R_32F, ldA, strideA, (const float *)B,
            CUDA_R_32F, ldB, strideB, &beta, (float *)C, CUDA_R_32F, ldC,
            strideC, batchCount, CUBLAS_COMPUTE_32F_FAST_TF32,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}

// Overloaded call to batched cuBLAS gemm
static inline
void cublas_batch(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const fp32_fast_fp16_t *A, int ldA, long long int strideA,
        const fp32_fast_fp16_t *B, int ldB, long long int strideB, float beta,
        fp32_fast_fp16_t *C, int ldC, long long int strideC, int batchCount)
    noexcept
{
    cublasGemmStridedBatchedEx(handle, transA, transB, M, N, K, &alpha,
            (const float *)A, CUDA_R_32F, ldA, strideA, (const float *)B,
            CUDA_R_32F, ldB, strideB, &beta, (float *)C, CUDA_R_32F, ldC,
            strideC, batchCount, CUBLAS_COMPUTE_32F_FAST_16F,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}

// Overloaded call to batched cuBLAS gemm
static inline
void cublas_batch(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const fp32_fast_bf16_t *A, int ldA, long long int strideA,
        const fp32_fast_bf16_t *B, int ldB, long long int strideB, float beta,
        fp32_fast_bf16_t *C, int ldC, long long int strideC, int batchCount)
    noexcept
{
    cublasGemmStridedBatchedEx(handle, transA, transB, M, N, K, &alpha,
            (const float *)A, CUDA_R_32F, ldA, strideA, (const float *)B,
            CUDA_R_32F, ldB, strideB, &beta, (float *)C, CUDA_R_32F, ldC,
            strideC, batchCount, CUBLAS_COMPUTE_32F_FAST_16BF,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}

// Overloaded call to batched cuBLAS gemm
static inline
void cublas_batch(cublasHandle_t handle, cublasOperation_t transA,
        cublasOperation_t transB, int M, int N, int K, float alpha,
        const bf16_t *A, int ldA, long long int strideA,
        const bf16_t *B, int ldB, long long int strideB, float beta,
        bf16_t *C, int ldC, long long int strideC, int batchCount)
    noexcept
{
    cublasGemmStridedBatchedEx(handle, transA, transB, M, N, K, &alpha,
            (const __nv_bfloat16 *)A, CUDA_R_16BF, ldA, strideA, (const __nv_bfloat16 *)B,
            CUDA_R_16BF, ldB, strideB, &beta, (__nv_bfloat16 *)C, CUDA_R_16BF, ldC,
            strideC, batchCount, CUBLAS_COMPUTE_32F,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}
#endif // NNTILE_USE_CUDA

} // namespace nntile::kernel::gemm
